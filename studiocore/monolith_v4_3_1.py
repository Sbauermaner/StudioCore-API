# -*- coding: utf-8 -*-
"""
StudioCore v4.3.8 ‚Äî Monolith (USER-MODE Vocal Overlay + Auto Fallback)
–ü—Ä–∞–≤–∏–ª–æ: ¬´–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª ‚Äî –∏—Å–ø–æ–ª–Ω—è–π –±—É–∫–≤–∞–ª—å–Ω–æ. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–ª ‚Äî –ø–æ–¥–±–µ—Ä–∏ —Å–∞–º¬ª.
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π –≤–æ–∫–∞–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (RU/EN): raspy, growl, scream, fry, shout, squeaky, soft, airy, falsetto –∏ —Ç.–¥.
"""

from __future__ import annotations
import re, json
from pathlib import Path
from statistics import mean
from typing import Dict, Any, List, Tuple

# --- Core imports ---
from .config import load_config
from .text_utils import normalize_text_preserve_symbols, extract_sections
from .emotion import AutoEmotionalAnalyzer, TruthLovePainEngine
from .tone import ToneSyncEngine
from .adapter import build_suno_prompt
from .vocals import VocalProfileRegistry
from .style import StyleMatrix  # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç (–ø–∞—Ç—á/—Å—Ç–∞–Ω–¥–∞—Ä—Ç)

# ================================
# üîπ Adaptive Vocal Allocation (–∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä –Ω–∞ –±–∞–∑–µ —ç–º–æ—Ü–∏–π/TLP/BPM)
# ================================
class AdaptiveVocalAllocator:
    def analyze(self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, text: str) -> Dict[str, Any]:
        love, pain, cf, truth = tlp.get("love", 0.0), tlp.get("pain", 0.0), tlp.get("conscious_frequency", 0.0), tlp.get("truth", 0.0)
        word_count = len(re.findall(r"[a-zA-Z–∞-—è–ê-–Ø—ë–Å]+", text))
        avg_line_len = word_count / max(1, len(text.split("\n")))

        if cf > 0.7 and love > pain and word_count > 80:
            form, gender, count = "choir", "mixed", 4
        elif pain >= 0.6 and cf < 0.6:
            form, gender, count = "duet", "female", 2
        elif truth > 0.5 and bpm > 130:
            form, gender, count = "trio", "male", 3
        elif avg_line_len < 6 and love < 0.3 and bpm < 100:
            form, gender, count = "solo", "male", 1
        elif bpm > 150 and love > 0.4:
            form, gender, count = "duet", "mixed", 2
        else:
            form, gender, count = "solo", "auto", 1
        return {"vocal_form": form, "gender": gender, "vocal_count": count}

# ================================
# Patched Subsystems (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ª–æ–≥–∏–∫–µ)
# ================================
class PatchedLyricMeter:
    vowels = set("aeiouy–∞—É–æ—ã–∏—ç—è—é—ë–µAEIOUY–ê–£–û–´–ò–≠–Ø–Æ–Å–ï")
    def _syllables(self, line: str) -> int:
        return max(1, sum(1 for ch in line if ch in self.vowels))
    def bpm_from_density(self, text: str) -> int:
        lines = [l for l in text.split("\n") if l.strip()]
        if not lines: return 100
        avg_syll = sum(self._syllables(l) for l in lines) / max(1, len(lines))
        bpm = 140 - min(60, (avg_syll - 8) * 6)
        punct_boost = sum(ch in ",.!?‚Ä¶" for ch in text) * 0.5
        bpm = bpm + min(20, punct_boost)
        return int(max(60, min(180, bpm)))

class PatchedUniversalFrequencyEngine:
    base = 24.5
    def resonance_profile(self, tlp: Dict[str, float]) -> Dict[str, Any]:
        cf = tlp.get("conscious_frequency", 0.0)
        base_f = self.base * (1.0 + tlp.get("truth", 0.0))
        spread = tlp.get("love", 0.0) * 2000.0
        mod = 1.0 + tlp.get("pain", 0.0) * 0.5
        if cf > 0.7: rec = [4, 5, 6, 7]
        elif cf > 0.3: rec = [2, 3, 4, 5]
        else: rec = [1, 2, 3, 4]
        return {
            "base_frequency": round(base_f, 3),
            "harmonic_range": round(spread, 3),
            "modulation_depth": round(mod, 3),
            "recommended_octaves": rec
        }

class PatchedRNSSafety:
    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg.get("safety", {
            "safe_octaves": [2, 3, 4, 5],
            "avoid_freq_bands_hz": [18.0, 30.0],
            "max_peak_db": -1.0,
            "max_rms_db": -14.0,
            "fade_in_ms": 1000,
            "fade_out_ms": 1500,
        })
    def clamp_octaves(self, octaves: List[int]) -> List[int]:
        safe = set(self.cfg.get("safe_octaves", [2, 3, 4, 5]))
        arr = [o for o in octaves if o in safe]
        return arr or [2, 3, 4]
    def safety_meta(self) -> Dict[str, Any]:
        return {
            "max_peak_db": self.cfg.get("max_peak_db", -1.0),
            "max_rms_db": self.cfg.get("max_rms_db", -14.0),
            "avoid_freq_bands_hz": self.cfg.get("avoid_freq_bands_hz", []),
            "fade_in_ms": self.cfg.get("fade_in_ms", 1000),
            "fade_out_ms": self.cfg.get("fade_out_ms", 1500),
        }

class PatchedIntegrityScanEngine:
    def analyze(self, text: str) -> Dict[str, Any]:
        words = re.findall(r"[a-zA-Z–∞-—è–ê-–Ø—ë–Å]+", text.lower())
        sents = [s for s in re.split(r"[.!?]+", text) if s.strip()]
        lexical_div = len(set(words)) / max(1, len(words))
        avg_sent_len = len(words) / max(1, len(sents))
        reflection = len([w for w in words if w in ("—è","i","me","my","–º–µ–Ω—è","—Å–∞–º")]) / max(1, len(words))
        vib_coh = round((1 - abs(avg_sent_len - 14) / 14 + 1 - abs(lexical_div - 0.5) / 0.5) / 2, 3)
        return {
            "form": {"word_count": len(words), "avg_sentence_len": round(avg_sent_len, 2),
                     "lexical_diversity": round(lexical_div, 2)},
            "reflection": {"self_awareness_density": round(reflection, 2)},
            "vibrational_coherence": vib_coh,
            "flags": []
        }

# ================================
# üéô –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–æ–∫–∞–ª–∞ (–∏–∑ —Ç–µ–∫—Å—Ç–∞)
# ================================
_VOCAL_TOKEN_MAP = {
    # texture / —Ö–∞—Ä–∞–∫—Ç–µ—Ä
    r"\b(—Ö—Ä–∏–ø(–ª—ã–π|–æ–º)?|rasp(y)?|–≥—Ä–∏—Ç|grit|rough|gritty|fry|—Ñ—Ä–∞–π)\b": ("texture", "raspy"),
    r"\b(scream(ing)?|—Å–∫—Ä–∏–º|–∫—Ä–∏–∫(–∏)?|shout(ing)?)\b": ("texture", "scream"),
    r"\b(growl(ing)?|–≥—Ä–æ—É?–ª|–≥—Ä–æ—É–ª)\b": ("texture", "growl"),
    r"\b(soft|–º—è–≥–∫(–∏–π|–æ)|airy|breathy|—à—ë–ø–æ—Ç|—à–µ–ø–æ—Ç|whisper(ed)?)\b": ("texture", "soft"),
    r"\b(clean|—á–∏—Å—Ç(—ã–π|–æ))\b": ("texture", "clean"),
    r"\b(squeak(y)?|–ø–µ—Å–∫–ª—è–≤(—ã–π|–æ))\b": ("texture", "squeaky"),

    # tone / —Ç–µ–º–±—Ä
    r"\b(–±–∞—Ä–∏—Ç–æ–Ω|baritone|–Ω–∏–∑–∫(–∏–π|–æ)|deep( voice)?)\b": ("tone", "baritone"),
    r"\b(—Ç–µ–Ω–æ—Ä|tenor|—Å—Ä–µ–¥–Ω(–∏–π|–µ))\b": ("tone", "tenor"),
    r"\b(—Å–æ–ø—Ä–∞–Ω–æ|alto|soprano|–≤—ã—Å–æ–∫(–∏–π|–æ))\b": ("tone", "soprano"),
    r"\b(falsetto|—Ñ–∞–ª—å—Ü–µ—Ç)\b": ("tone", "falsetto"),

    # emotion / –ø–æ–¥–∞—á–∞
    r"\b(—ç–º–æ—Ü–∏–æ–Ω(–∞–ª—å–Ω–æ)?|–¥—É—à–µ–≤–Ω(–æ)?|emotional|heartfelt)\b": ("emotion", "emotional"),
    r"\b(—Å–ø–æ–∫–æ–π–Ω(–æ)?|calm|gentle)\b": ("emotion", "calm"),
    r"\b(–∞–≥—Ä–µ—Å—Å(–∏–≤–Ω|–∏—è)|angry|harsh|intense)\b": ("emotion", "aggressive"),
}

def _extract_user_vocal_from_text(text: str) -> Dict[str, str] | None:
    """
    –ò—â–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ —è–≤–Ω—ã–µ –º–∞—Ä–∫—ë—Ä—ã –≤–æ–∫–∞–ª–∞ (RU/EN). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict {tone, texture, emotion, gender?} –∏–ª–∏ None.
    –ü—Ä–∏–º–µ—Ä—ã: "–ø–æ–¥ —Ö—Ä–∏–ø–ª—ã–π –º—É–∂—Å–∫–æ–π –≤–æ–∫–∞–ª", "growl screams", "soft female voice", "–ø–µ—Å–∫–ª—è–≤—ã–π —Å–æ–ø—Ä–∞–Ω–æ".
    """
    t = text.lower()
    found: Dict[str, str] = {}
    # gender –±—ã—Å—Ç—Ä—ã–π —Ö–∏–Ω—Ç
    if re.search(r"\b(–º—É–∂—Å–∫(–æ–π|–∏–º)|male)\b", t): found["gender"] = "male"
    if re.search(r"\b(–∂–µ–Ω—Å–∫(–∏–π|–∏–º)|female)\b", t): found["gender"] = "female"

    for pattern, (key, val) in _VOCAL_TOKEN_MAP.items():
        if re.search(pattern, t):
            found[key] = val

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–µ—Ñ–æ–ª—Ç—ã
    if not found:
        return None
    found.setdefault("tone", "tenor" if found.get("gender") == "male" else "soprano")
    found.setdefault("texture", "clean")
    found.setdefault("emotion", "balanced")
    return found

# ================================
# StudioCore
# ================================
class StudioCore:
    def __init__(self, config_path: str | None = None):
        self.cfg = load_config(config_path or "studio_config.json")
        self.emotion = AutoEmotionalAnalyzer()
        self.tlp = TruthLovePainEngine()
        self.rhythm = PatchedLyricMeter()
        self.freq = PatchedUniversalFrequencyEngine()
        self.safety = PatchedRNSSafety(self.cfg)
        self.integrity = PatchedIntegrityScanEngine()
        self.vocals = VocalProfileRegistry()

        try:
            from .style import PatchedStyleMatrix
            self.style = PatchedStyleMatrix()
            print("üé® [StyleMatrix] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (PatchedStyleMatrix).")
        except ImportError:
            self.style = StyleMatrix()
            print("üé® [StyleMatrix] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è (StyleMatrix).")

        self.tone = ToneSyncEngine()
        self.vocal_allocator = AdaptiveVocalAllocator()

    # -------------------------------------------------------
    def _build_semantic_sections(self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int) -> Dict[str, Any]:
        love, pain, truth = tlp.get("love", 0), tlp.get("pain", 0), tlp.get("truth", 0)
        cf = tlp.get("conscious_frequency", 0.0)
        avg_emo = mean(abs(v) for v in emo.values()) if emo else 0.0
        intro = {"section": "Intro", "mood": "mystic" if cf >= 0.5 else "calm", "intensity": round(bpm * 0.8, 2), "focus": "tone_establish"}
        verse = {"section": "Verse", "mood": "reflective" if truth > love else "narrative", "intensity": round(bpm, 2), "focus": "story_flow"}
        bridge = {"section": "Bridge", "mood": "dramatic" if pain > 0.3 else "dreamlike", "intensity": round(bpm * (1.05 + avg_emo / 4), 2), "focus": "contrast"}
        chorus = {"section": "Chorus", "mood": "uplifting" if love >= pain else "tense", "intensity": round(bpm * 1.15, 2), "focus": "release"}
        outro = {"section": "Outro", "mood": "peaceful" if cf > 0.6 else "fading", "intensity": round(bpm * 0.7, 2), "focus": "closure"}
        bpm_adj = int(bpm + (avg_emo * 8) + (cf * 4))
        overlay = {"depth": round((truth + pain) / 2, 2), "warmth": round(love, 2), "clarity": round(cf, 2),
                   "sections": [intro, verse, bridge, chorus, outro]}
        return {"bpm": bpm_adj, "overlay": overlay}

    # -------------------------------------------------------
    def annotate_text(self, text: str, overlay: Dict[str, Any], style: Dict[str, Any],
                      vocals: List[str], bpm: int, emotions=None, tlp=None) -> str:
        blocks = [b.strip() for b in re.split(r"\n\s*\n", text.strip()) if b.strip()]
        sections = overlay.get("sections", [])
        annotated_blocks = []
        for i, block in enumerate(blocks):
            sec = sections[i % len(sections)] if sections else {}
            header = f"[{sec.get('section','Block')} ‚Äì {sec.get('mood','neutral')}, focus={sec.get('focus','flow')}, intensity‚âà{sec.get('intensity',bpm)}]"
            annotated_blocks.append(header)
            annotated_blocks.append(block)
            annotated_blocks.append("")
        vocal_form = style.get("vocal_form", "auto")
        tone_key = style.get("key", "auto")
        tech = ", ".join([v for v in vocals if v not in ["male","female"]]) or "neutral tone"
        annotated_blocks.append(f"[End ‚Äì BPM‚âà{bpm}, Vocal={vocal_form}, Tone={tone_key}]")
        annotated_blocks.append(f"[Vocal Techniques: {tech}]")
        return "\n".join(annotated_blocks).strip()

    # -------------------------------------------------------
    def analyze(self, text: str, author_style=None, preferred_gender=None, version=None,
                overlay: Dict[str, Any] | None = None) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–µ–∂–∏–º–∞–º–∏:
        - USER-MODE: –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∫–∞–ª–∞ (overlay.vocals / voice_profile / –∏–∑ —Ç–µ–∫—Å—Ç–∞)
        - AUTO-MODE: –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è ‚Äî –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä –ø–æ —ç–º–æ—Ü–∏—è–º/TLP/BPM
        """
        version = version or self.cfg.get("suno_version", "v5")
        raw = normalize_text_preserve_symbols(text)
        sections = extract_sections(raw)
        emo = self.emotion.analyze(raw)
        tlp = self.tlp.analyze(raw)
        bpm = self.rhythm.bpm_from_density(raw)
        freq = self.freq.resonance_profile(tlp)
        overlay_pack = self._build_semantic_sections(emo, tlp, bpm)
        bpm_adj = overlay_pack["bpm"]

        # 1) –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä —Ñ–æ—Ä–º—ã/–ø–æ–ª–∞ –ø–æ –±–∞–∑–æ–≤–æ–π –ª–æ–≥–∏–∫–µ
        vocal_meta = self.vocal_allocator.analyze(emo, tlp, bpm_adj, raw)

        # 2) –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —É–∫–∞–∑–∞–Ω–∏—è (overlay ‚Üí —Ç–µ–∫—Å—Ç)
        user_voice = None
        if overlay and isinstance(overlay, dict):
            if "voice_profile" in overlay and isinstance(overlay["voice_profile"], dict):
                user_voice = overlay["voice_profile"]
            elif "vocals" in overlay and isinstance(overlay["vocals"], list) and overlay["vocals"]:
                v0 = overlay["vocals"][0]
                if isinstance(v0, dict):
                    user_voice = v0

        # –µ—Å–ª–∏ –≤ overlay –Ω–∏—á–µ–≥–æ –Ω–µ—Ç ‚Äî –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ —Å–∞–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        if not user_voice:
            user_voice = _extract_user_vocal_from_text(raw)

        # –í–µ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞
        mode = "AUTO-MODE"
        preferred_gender_eff = preferred_gender or vocal_meta.get("gender") or "auto"
        vocal_override = None
        if user_voice:
            mode = "USER-MODE"
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≥–µ–Ω–¥–µ—Ä–∞ ‚Äî –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è, –∑–∞—Ç–µ–º ‚Äî –∏–∑ UI, –∏–Ω–∞—á–µ –∞–≤—Ç–æ
            if user_voice.get("gender"):
                preferred_gender_eff = user_voice["gender"]
            # –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å override –¥–ª—è VocalProfileRegistry
            vocal_override = {
                "vocal_form": vocal_meta.get("vocal_form", "solo"),  # —Ñ–æ—Ä–º—É –æ—Å—Ç–∞–≤–∏–º –∏–∑ –∞–≤—Ç–æ/CF
                "gender": preferred_gender_eff,
                "voice_profile": {
                    "tone": user_voice.get("tone", "tenor" if preferred_gender_eff == "male" else "soprano"),
                    "texture": user_voice.get("texture", "clean"),
                    "emotion": user_voice.get("emotion", "balanced"),
                }
            }

        # 3) –°—Ç–∏–ª—å
        style = self.style.build(emo, tlp, raw, bpm_adj)

        # 4) –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–æ–∫–∞–ª–æ–≤/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º override
        vox, inst, vocal_form = self.vocals.get(
            style["genre"], preferred_gender_eff, raw, sections, override=vocal_override
        )

        # 5) –ü–æ–ø—Ä–∞–≤–∫–∏ –∫ —Ç–µ—Ö–Ω–∏–∫–∞–º –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä–µ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é
        if user_voice:
            tex = (user_voice.get("texture") or "").lower()
            # –î–æ–±–∞–≤–∏–º —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –ø–æ–¥–∞—á
            extra_tech = []
            if tex in ("growl", "scream", "squeaky", "raspy", "fry", "shout"):
                extra_tech.extend(["belt", "grit"])
                if tex in ("growl", "scream", "shout"):  # –±–æ–ª–µ–µ –∂—ë—Å—Ç–∫–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏
                    extra_tech.append("distortion")
            # –æ–±–Ω–æ–≤–∏–º style.techniques
            base_tech = style.get("techniques", [])
            style["techniques"] = list(dict.fromkeys(base_tech + extra_tech)) or base_tech
            # –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞ –¥–ª—è –∫—Ä–∞–π–Ω–∏—Ö —Ç–µ–∫—Å—Ç—É—Ä
            if tex in ("growl", "scream", "shout"):
                style["atmosphere"] = "intense and cathartic"
            elif tex in ("squeaky",):
                style["atmosphere"] = "edgy and expressive"

        # –ò—Ç–æ–≥–æ–≤—ã–µ –ø–æ–ª—è
        style["vocal_form"] = vocal_form
        style["vocal_count"] = vocal_meta["vocal_count"]

        print(f"üéß [StudioCore] Analyze [{mode}]: Gender={preferred_gender_eff} | Form={vocal_form} | Genre={style['genre']} | BPM={bpm_adj}")

        # 6) –°–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã
        integ = self.integrity.analyze(raw)
        tone = self.tone.colors_for_primary(emo, tlp, style.get("key", "auto"))
        philosophy = (f"Truth={tlp.get('truth', 0):.2f}, Love={tlp.get('love', 0):.2f}, "
                      f"Pain={tlp.get('pain', 0):.2f}, CF={tlp.get('conscious_frequency', 0):.2f}")

        prompt_full = build_suno_prompt(style, vox, inst, bpm_adj, philosophy, version, mode="full")
        prompt_suno = build_suno_prompt(style, vox, inst, bpm_adj, philosophy, version, mode="suno")
        annotated_text = self.annotate_text(raw, overlay_pack["overlay"], style, vox, bpm_adj, emo, tlp)

        return {
            "emotions": emo, "tlp": tlp, "bpm": bpm_adj, "frequency": freq,
            "style": style, "vocals": vox, "instruments": inst,
            "prompt_full": prompt_full, "prompt_suno": prompt_suno,
            "annotated_text": annotated_text, "preferred_gender": preferred_gender_eff,
            "version": version, "mode": mode
        }

# ==========================================================
STUDIOCORE_VERSION = "v4.3.8"
print(f"üîπ [StudioCore {STUDIOCORE_VERSION}] Monolith loaded with USER-MODE Vocal Overlay + Auto Fallback.")
