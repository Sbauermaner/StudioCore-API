studiocore/monolith_v4_3_1.py
# -*- coding: utf-8 -*-
"""
StudioCore v4.3.9 â€” Monolith (USER-MODE Vocal Overlay + Auto Voice Detection)
ÐŸÑ€Ð°Ð²Ð¸Ð»Ð¾: Â«Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑƒÐºÐ°Ð·Ð°Ð» â€” Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐ¹ Ð±ÑƒÐºÐ²Ð°Ð»ÑŒÐ½Ð¾. Ð•ÑÐ»Ð¸ Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð» â€” Ð¿Ð¾Ð´Ð±ÐµÑ€Ð¸ ÑÐ°Ð¼Â».
ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹ Ð²Ð¾ÐºÐ°Ð»Ð° Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° (RU/EN) Ð¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· detect_voice_profile().
"""

from __future__ import annotations
import re, json
from statistics import mean
from typing import Dict, Any, List, Tuple

# --- Core imports ---
from .config import load_config
from .text_utils import normalize_text_preserve_symbols, extract_sections
from .emotion import AutoEmotionalAnalyzer, TruthLovePainEngine
from .tone import ToneSyncEngine
from .adapter import build_suno_prompt
from .vocals import VocalProfileRegistry
from .style import StyleMatrixÂ  # Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚ (Ð¿Ð°Ñ‚Ñ‡ Ð¸Ð»Ð¸ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚)

# ==========================================================
# ðŸ§© ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ð°Ð²Ñ‚Ð¾Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ð²Ð¾ÐºÐ°Ð»Ð°
# ==========================================================
try:
Â  Â  from .style import detect_voice_profile
Â  Â  _AUTO_VOCAL_DETECT = True
Â  Â  print("ðŸŽ™ï¸ [Monolith] Auto voice detection Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½ (detect_voice_profile Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½).")
except Exception:
Â  Â  detect_voice_profile = None
Â  Â  _AUTO_VOCAL_DETECT = False
Â  Â  print("âš ï¸ [Monolith] Auto voice detection Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ (detect_voice_profile Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚).")

# ==========================================================
# ðŸ”¹ Adaptive Vocal Allocation (Ð°Ð²Ñ‚Ð¾Ð¿Ð¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ ÑÐ¼Ð¾Ñ†Ð¸ÑÐ¼/TLP/BPM)
# ==========================================================
class AdaptiveVocalAllocator:
Â  Â  def analyze(self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, text: str) -> Dict[str, Any]:
Â  Â  Â  Â  love, pain, cf, truth = tlp.get("love", 0.0), tlp.get("pain", 0.0), tlp.get("conscious_frequency", 0.0), tlp.get("truth", 0.0)
Â  Â  Â  Â  word_count = len(re.findall(r"[a-zA-ZÐ°-ÑÐ-Ð¯Ñ‘Ð]+", text))
Â  Â  Â  Â  avg_line_len = word_count / max(1, len(text.split("\n")))

Â  Â  Â  Â  if cf > 0.7 and love > pain and word_count > 80:
Â  Â  Â  Â  Â  Â  form, gender, count = "choir", "mixed", 4
Â  Â  Â  Â  elif pain >= 0.6 and cf < 0.6:
Â  Â  Â  Â  Â  Â  form, gender, count = "duet", "female", 2
Â  Â  Â  Â  elif truth > 0.5 and bpm > 130:
Â  Â  Â  Â  Â  Â  form, gender, count = "trio", "male", 3
Â  Â  Â  Â  elif avg_line_len < 6 and love < 0.3 and bpm < 100:
Â  Â  Â  Â  Â  Â  form, gender, count = "solo", "male", 1
Â  Â  Â  Â  elif bpm > 150 and love > 0.4:
Â  Â  Â  Â  Â  Â  form, gender, count = "duet", "mixed", 2
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  form, gender, count = "solo", "auto", 1
Â  Â  Â  Â  return {"vocal_form": form, "gender": gender, "vocal_count": count}


# ==========================================================
# ðŸ”¸ Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ð´ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ (Ð·Ð°Ð¼ÐµÐ½Ð° monolith_subsystems)
# ==========================================================
class PatchedLyricMeter:
Â  Â  vowels = set("aeiouyÐ°ÑƒÐ¾Ñ‹Ð¸ÑÑÑŽÑ‘ÐµAEIOUYÐÐ£ÐžÐ«Ð˜Ð­Ð¯Ð®ÐÐ•")
Â  Â  def _syllables(self, line: str) -> int:
Â  Â  Â  Â  return max(1, sum(1 for ch in line if ch in self.vowels))
Â  Â  def bpm_from_density(self, text: str) -> int:
Â  Â  Â  Â  lines = [l for l in text.split("\n") if l.strip()]
Â  Â  Â  Â  if not lines: return 100
Â  Â  Â  Â  avg_syll = sum(self._syllables(l) for l in lines) / max(1, len(lines))
Â  Â  Â  Â  bpm = 140 - min(60, (avg_syll - 8) * 6)
Â  Â  Â  Â  punct_boost = sum(ch in ",.!?â€¦" for ch in text) * 0.5
Â  Â  Â  Â  bpm = bpm + min(20, punct_boost)
Â  Â  Â  Â  return int(max(60, min(180, bpm)))

class PatchedUniversalFrequencyEngine:
Â  Â  base = 24.5
Â  Â  def resonance_profile(self, tlp: Dict[str, float]) -> Dict[str, Any]:
Â  Â  Â  Â  cf = tlp.get("conscious_frequency", 0.0)
Â  Â  Â  Â  base_f = self.base * (1.0 + tlp.get("truth", 0.0))
Â  Â  Â  Â  spread = tlp.get("love", 0.0) * 2000.0
Â  Â  Â  Â  mod = 1.0 + tlp.get("pain", 0.0) * 0.5
Â  Â  Â  Â  if cf > 0.7: rec = [4, 5, 6, 7]
Â  Â  Â  Â  elif cf > 0.3: rec = [2, 3, 4, 5]
Â  Â  Â  Â  else: rec = [1, 2, 3, 4]
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "base_frequency": round(base_f, 3),
Â  Â  Â  Â  Â  Â  "harmonic_range": round(spread, 3),
Â  Â  Â  Â  Â  Â  "modulation_depth": round(mod, 3),
Â  Â  Â  Â  Â  Â  "recommended_octaves": rec
Â  Â  Â  Â  }

class PatchedRNSSafety:
Â  Â  def __init__(self, cfg: Dict[str, Any]):
Â  Â  Â  Â  self.cfg = cfg.get("safety", {
Â  Â  Â  Â  Â  Â  "safe_octaves": [2, 3, 4, 5],
Â  Â  Â  Â  Â  Â  "avoid_freq_bands_hz": [18.0, 30.0],
Â  Â  Â  Â  Â  Â  "max_peak_db": -1.0,
Â  Â  Â  Â  Â  Â  "max_rms_db": -14.0,
Â  Â  Â  Â  Â  Â  "fade_in_ms": 1000,
Â  Â  Â  Â  Â  Â  "fade_out_ms": 1500,
Â  Â  Â  Â  })
Â  Â  def clamp_octaves(self, octaves: List[int]) -> List[int]:
Â  Â  Â  Â  safe = set(self.cfg.get("safe_octaves", [2, 3, 4, 5]))
Â  Â  Â  Â  arr = [o for o in octaves if o in safe]
Â  Â  Â  Â  return arr or [2, 3, 4]
Â  Â  def safety_meta(self) -> Dict[str, Any]:
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "max_peak_db": self.cfg.get("max_peak_db", -1.0),
Â  Â  Â  Â  Â  Â  "max_rms_db": self.cfg.get("max_rms_db", -14.0),
Â  Â  Â  Â  Â  Â  "avoid_freq_bands_hz": self.cfg.get("avoid_freq_bands_hz", []),
Â  Â  Â  Â  Â  Â  "fade_in_ms": self.cfg.get("fade_in_ms", 1000),
Â  Â  Â  Â  Â  Â  "fade_out_ms": self.cfg.get("fade_out_ms", 1500),
Â  Â  Â  Â  }

class PatchedIntegrityScanEngine:
Â  Â  def analyze(self, text: str) -> Dict[str, Any]:
Â  Â  Â  Â  words = re.findall(r"[a-zA-ZÐ°-ÑÐ-Ð¯Ñ‘Ð]+", text.lower())
Â  Â  Â  Â  sents = [s for s in re.split(r"[.!?]+", text) if s.strip()]
Â  Â  Â  Â  lexical_div = len(set(words)) / max(1, len(words))
Â  Â  Â  Â  avg_sent_len = len(words) / max(1, len(sents))
Â  Â  Â  Â  reflection = len([w for w in words if w in ("Ñ","i","me","my","Ð¼ÐµÐ½Ñ","ÑÐ°Ð¼")]) / max(1, len(words))
Â  Â  Â  Â  vib_coh = round((1 - abs(avg_sent_len - 14) / 14 + 1 - abs(lexical_div - 0.5) / 0.5) / 2, 3)
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "form": {"word_count": len(words), "avg_sentence_len": round(avg_sent_len, 2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â "lexical_diversity": round(lexical_div, 2)},
Â  Â  Â  Â  Â  Â  "reflection": {"self_awareness_density": round(reflection, 2)},
Â  Â  Â  Â  Â  Â  "vibrational_coherence": vib_coh,
Â  Â  Â  Â  Â  Â  "flags": []
Â  Â  Â  Â  }

# ==========================================================
# StudioCore
# ==========================================================
class StudioCore:
Â  Â  def __init__(self, config_path: str | None = None):
Â  Â  Â  Â  self.cfg = load_config(config_path or "studio_config.json")
Â  Â  Â  Â  self.emotion = AutoEmotionalAnalyzer()
Â  Â  Â  Â  self.tlp = TruthLovePainEngine()

Â  Â  Â  Â  # ÐŸÐ¾Ð´ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾
Â  Â  Â  Â  self.rhythm = PatchedLyricMeter()
Â  Â  Â  Â  self.freq = PatchedUniversalFrequencyEngine()
Â  Â  Â  Â  self.safety = PatchedRNSSafety(self.cfg)
Â  Â  Â  Â  self.integrity = PatchedIntegrityScanEngine()
Â  Â  Â  Â  self.vocals = VocalProfileRegistry()

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  from .style import PatchedStyleMatrix
Â  Â  Â  Â  Â  Â  self.style = PatchedStyleMatrix()
Â  Â  Â  Â  Â  Â  print("ðŸŽ¨ [StyleMatrix] Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¿Ð°Ñ‚Ñ‡Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ (PatchedStyleMatrix).")
Â  Â  Â  Â  except ImportError:
Â  Â  Â  Â  Â  Â  self.style = StyleMatrix()
Â  Â  Â  Â  Â  Â  print("ðŸŽ¨ [StyleMatrix] Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ (StyleMatrix).")

Â  Â  Â  Â  self.tone = ToneSyncEngine()
Â  Â  Â  Â  self.vocal_allocator = AdaptiveVocalAllocator()

Â  Â  # -------------------------------------------------------
Â  Â  def _build_semantic_sections(self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int) -> Dict[str, Any]:
Â  Â  Â  Â  love, pain, truth = tlp.get("love", 0), tlp.get("pain", 0), tlp.get("truth", 0)
Â  Â  Â  Â  cf = tlp.get("conscious_frequency", 0.0)
Â  Â  Â  Â  avg_emo = mean(abs(v) for v in emo.values()) if emo else 0.0
Â  Â  Â  Â  intro = {"section": "Intro", "mood": "mystic" if cf >= 0.5 else "calm", "intensity": round(bpm * 0.8, 2), "focus": "tone_establish"}
Â  Â  Â  Â  verse = {"section": "Verse", "mood": "reflective" if truth > love else "narrative", "intensity": round(bpm, 2), "focus": "story_flow"}
Â  Â  Â  Â  bridge = {"section": "Bridge", "mood": "dramatic" if pain > 0.3 else "dreamlike", "intensity": round(bpm * (1.05 + avg_emo / 4), 2), "focus": "contrast"}
Â  Â  Â  Â  chorus = {"section": "Chorus", "mood": "uplifting" if love >= pain else "tense", "intensity": round(bpm * 1.15, 2), "focus": "release"}
Â  Â  Â  Â  outro = {"section": "Outro", "mood": "peaceful" if cf > 0.6 else "fading", "intensity": round(bpm * 0.7, 2), "focus": "closure"}
Â  Â  Â  Â  bpm_adj = int(bpm + (avg_emo * 8) + (cf * 4))
Â  Â  Â  Â  overlay = {"depth": round((truth + pain) / 2, 2), "warmth": round(love, 2), "clarity": round(cf, 2),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â "sections": [intro, verse, bridge, chorus, outro]}
Â  Â  Â  Â  return {"bpm": bpm_adj, "overlay": overlay}

Â  Â  # -------------------------------------------------------
Â  Â  def annotate_text(self, text: str, overlay: Dict[str, Any], style: Dict[str, Any],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  vocals: List[str], bpm: int, emotions=None, tlp=None) -> str:
Â  Â  Â  Â  """
Â  Â  Â  Â  Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÑ‚ Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸ Ðº Ñ‚ÐµÐºÑÑ‚Ñƒ (ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° Ð¿ÐµÑÐ½Ð¸, BPM, Ð²Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸)
Â  Â  Â  Â  """
Â  Â  Â  Â  blocks = [b.strip() for b in re.split(r"\n\s*\n", text.strip()) if b.strip()]
Â  Â  Â  Â  sections = overlay.get("sections", [])
Â  Â  Â  Â  annotated_blocks = []
Â  Â  Â  Â  for i, block in enumerate(blocks):
Â  Â  Â  Â  Â  Â  sec = sections[i % len(sections)] if sections else {}
Â  Â  Â  Â  Â  Â  header = f"[{sec.get('section','Block')} â€“ {sec.get('mood','neutral')}, focus={sec.get('focus','flow')}, intensityâ‰ˆ{sec.get('intensity',bpm)}]"
Â  Â  Â  Â  Â  Â  annotated_blocks.append(header)
Â  Â  Â  Â  Â  Â  annotated_blocks.append(block)
Â  Â  Â  Â  Â  Â  annotated_blocks.append("")
Â  Â  Â  Â  vocal_form = style.get("vocal_form", "auto")
Â  Â  Â  Â  tone_key = style.get("key", "auto")
Â  Â  Â  Â  tech = ", ".join([v for v in vocals if v not in ["male","female"]]) or "neutral tone"
Â  Â  Â  Â  annotated_blocks.append(f"[End â€“ BPMâ‰ˆ{bpm}, Vocal={vocal_form}, Tone={tone_key}]")
Â  Â  Â  Â  annotated_blocks.append(f"[Vocal Techniques: {tech}]")
Â  Â  Â  Â  return "\n".join(annotated_blocks).strip()

Â  Â  # -------------------------------------------------------
Â  Â  def analyze(self, text: str, author_style=None, preferred_gender=None, version=None,
Â  Â  Â  Â  Â  Â  Â  Â  overlay: Dict[str, Any] | None = None) -> Dict[str, Any]:
Â  Â  Â  Â  version = version or self.cfg.get("suno_version", "v5")
Â  Â  Â  Â  raw = normalize_text_preserve_symbols(text)
Â  Â  Â  Â  sections = extract_sections(raw)
Â  Â  Â  Â  emo = self.emotion.analyze(raw)
Â  Â  Â  Â  tlp = self.tlp.analyze(raw)
Â  Â  Â  Â  bpm = self.rhythm.bpm_from_density(raw)
Â  Â  Â  Â  freq = self.freq.resonance_profile(tlp)
Â  Â  Â  Â  overlay_pack = self._build_semantic_sections(emo, tlp, bpm)
Â  Â  Â  Â  bpm_adj = overlay_pack["bpm"]

Â  Â  Â  Â  vocal_meta = self.vocal_allocator.analyze(emo, tlp, bpm_adj, raw)

Â  Â  Â  Â  user_voice, auto_detected_hint = None, None
Â  Â  Â  Â  if overlay and "voice_profile" in overlay:
Â  Â  Â  Â  Â  Â  user_voice = overlay["voice_profile"]
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  from .monolith import _extract_user_vocal_from_text
Â  Â  Â  Â  Â  Â  Â  Â  user_voice = _extract_user_vocal_from_text(raw)
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  pass

Â  Â  Â  Â  if not user_voice and _AUTO_VOCAL_DETECT and detect_voice_profile:
Â  Â  Â  Â  Â  Â  auto_detected_hint = detect_voice_profile(raw)
Â  Â  Â  Â  Â  Â  if auto_detected_hint:
Â  Â  Â  Â  Â  Â  Â  Â  overlay_pack["overlay"]["voice_profile_hint"] = auto_detected_hint

Â  Â  Â  Â  mode = "AUTO-MODE"
Â  Â  Â  Â  if user_voice:
Â  Â  Â  Â  Â  Â  mode = "USER-MODE"
Â  Â  Â  Â  elif auto_detected_hint:
Â  Â  Â  Â  Â  Â  mode = "AUTO-DETECT"

Â  Â  Â  Â  preferred_gender_eff = preferred_gender or vocal_meta.get("gender") or "auto"
Â  Â  Â  Â  style = self.style.build(emo, tlp, raw, bpm_adj, overlay_pack["overlay"])

Â  Â  Â  Â  vox, inst, vocal_form = self.vocals.get(
Â  Â  Â  Â  Â  Â  style["genre"], preferred_gender_eff, raw, sections
Â  Â  Â  Â  )
Â  Â  Â  Â  style["vocal_form"] = vocal_form
Â  Â  Â  Â  style["vocal_count"] = vocal_meta["vocal_count"]

Â  Â  Â  Â  print(f"ðŸŽ§ [StudioCore] Analyze [{mode}]: Gender={preferred_gender_eff} | Form={vocal_form} | Genre={style['genre']} | BPM={bpm_adj}")

Â  Â  Â  Â  integ = self.integrity.analyze(raw)
Â  Â  Â  Â  tone = self.tone.colors_for_primary(emo, tlp, style.get("key", "auto"))
Â  Â  Â  Â  philosophy = (f"Truth={tlp.get('truth', 0):.2f}, Love={tlp.get('love', 0):.2f}, "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Pain={tlp.get('pain', 0):.2f}, CF={tlp.get('conscious_frequency', 0):.2f}")

Â  Â  Â  Â  prompt_full = build_suno_prompt(style, vox, inst, bpm_adj, philosophy, version, mode="full")
Â  Â  Â  Â  prompt_suno = build_suno_prompt(style, vox, inst, bpm_adj, philosophy, version, mode="suno")
Â  Â  Â  Â  annotated_text = self.annotate_text(raw, overlay_pack["overlay"], style, vox, bpm_adj, emo, tlp)

Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "emotions": emo, "tlp": tlp, "bpm": bpm_adj, "frequency": freq,
Â  Â  Â  Â  Â  Â  "style": style, "vocals": vox, "instruments": inst,
Â  Â  Â  Â  Â  Â  "prompt_full": prompt_full, "prompt_suno": prompt_suno,
Â  Â  Â  Â  Â  Â  "annotated_text": annotated_text, "preferred_gender": preferred_gender_eff,
Â  Â  Â  Â  Â  Â  "version": version, "mode": mode
Â  Â  Â  Â  }


# ==========================================================
STUDIOCORE_VERSION = "v4.3.9"
print(f"ðŸ”¹ [StudioCore {STUDIOCORE_VERSION}] Monolith loaded (USER-MODE + Auto Voice Detection).")