# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e
# -*- coding: utf - 8 -*-
"""
StudioCore Monolith (v6 - Suno –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏)
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ f - string (—Å—Ç—Ä–æ–∫–∞ 259)
- –í–Ω–µ–¥—Ä–µ–Ω–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –í–Ω–µ–¥—Ä–µ–Ω–∞ Suno - –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –≤ 'annotate_text'
- Task 6.2: Version now imported from config.py
"""

import re
import time
from typing import Dict, Any, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor
import logging

# === 1. –ò–º–ø–æ—Ä—Ç —è–¥—Ä–∞ ===
# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e

# AI_TRAINING_PROHIBITED: Redistribution or training of AI models on this codebase
# without explicit written permission from the Author is prohibited.

from .config import DEFAULT_CONFIG, load_config

# Task 6.2: Import version from config instead of hardcoding
MONOLITH_VERSION = DEFAULT_CONFIG.MONOLITH_VERSION
STUDIOCORE_VERSION = DEFAULT_CONFIG.STUDIOCORE_VERSION

# v16: –ò–°–ü–†–ê–í–õ–ï–ù ImportError
from .text_utils import normalize_text_preserve_symbols, extract_raw_blocks

# v15: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω ImportError (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞)
from .emotion import AutoEmotionalAnalyzer, TruthLovePainEngine
from .tone import ToneSyncEngine
from .vocals import VocalProfileRegistry
from .integrity import (
    IntegrityScanEngine as FullIntegrityScanEngine,
)  # –ò–º–ø–æ—Ä—Ç –¥–≤–∏–∂–∫–∞ V6
from .rhythm import LyricMeter

# v11: 'PatchedStyleMatrix' - —ç—Ç–æ –Ω–∞—à 'StyleMatrix'
from .style import PatchedStyleMatrix
from .color_engine_adapter import ColorEngineAdapter
from .rde_engine import RhythmDynamicsEmotionEngine
# Task 18.1: Import conflict resolution classes
from .consistency_v8 import ConsistencyLayerV8
from .genre_conflict_resolver import GenreConflictResolver

# === 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ ===
log = logging.getLogger(__name__)

# ==========================================================
# üó£Ô∏è –£—Ç–∏–ª–∏—Ç—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∫–∞–ª–∞ (v2 - –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ style.py)
# ==========================================================


def detect_voice_profile(text: str) -> Optional[str]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–æ–∫–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    """
    log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: detect_voice_profile")
    text_low = text.lower()
    patterns = [
        r"–ø–æ–¥\s+[–∞ - —èa - z\s,]+–≤–æ–∫–∞–ª",  # –ø–æ–¥ —Ö—Ä–∏–ø–ª—ã–π –º—É–∂—Å–∫–æ–π –≤–æ–∫–∞–ª
        # (soft female growl)
        r"\([\s\S]*?(–≤–æ–∫–∞–ª|voice|growl|scream|—à[–µ—ë]–ø–æ—Ç|–∫—Ä–∏–∫)[\s\S]*?\)",
        r"(–º—É–∂—Å–∫\w+|–∂–µ–Ω—Å–∫\w+)\s + –≤–æ–∫–∞–ª",
        r"(soft|airy|raspy|grit|growl|scream|whisper)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text_low)
        if match:
            hint = match.group(0).strip("() ")
            log.debug(f"–û–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∫–∞–ª–∞ –Ω–∞–π–¥–µ–Ω–æ: {hint}")
            return hint

    log.debug("–û–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∫–∞–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    return None


def detect_gender_from_grammar(text: str) -> Optional[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–ª –ø–æ –≥—Ä–∞–º–º–∞—Ç–∏–∫–µ ("—è —à–µ–ª" / "—è —à–ª–∞")
    """
    log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: detect_gender_from_grammar")
    male_verbs = len(re.findall(r"\b(—è\s+\w + –ª)\b", text, re.I))
    female_verbs = len(re.findall(r"\b(—è\s+\w + –ª–∞)\b", text, re.I))
    log.debug(
        f"–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: Male —Ö–∏—Ç—ã={male_verbs}, Female —Ö–∏—Ç—ã={female_verbs}"
    )

    if male_verbs > female_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: male")
        return "male"
    if female_verbs > male_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: female")
        return "female"
    if male_verbs > 0 and male_verbs == female_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: mixed")
        return "mixed"

    log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ (auto)")
    return "auto"


# ==========================================================
# üîπ –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã
# (–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö, —á—Ç–æ–±—ã –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å rhythm.py –∏ —Ç.–¥. –≤ —Ç–µ—Å—Ç–∞—Ö)
# ==========================================================


class PatchedLyricMeter:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ –Ω–æ–≤—ã–º LyricMeter –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""

    def __init__(self) -> None:
        self._engine = LyricMeter()

    def analyze(
        self,
        text: str,
        *,
        emotions: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        tlp: Optional[Dict[str, float]] = None,
        header_bpm: Optional[float] = None,
    ):
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: PatchedLyricMeter.analyze")
        tlp = tlp or {}
        if cf is None:
            cf = tlp.get("conscious_frequency")
        return self._engine.analyze(
            text,
            emotions=emotions,
            cf=cf,
            tlp=tlp,
            header_bpm=header_bpm,
        )

    def bpm_from_density(
        self,
        text: str,
        emo: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        tlp: Optional[Dict[str, float]] = None,
    ) -> int:
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: PatchedLyricMeter.bpm_from_density")
        analysis = self.analyze(text, emotions=emo, cf=cf, tlp=tlp)
        bpm = int(round(analysis["global_bpm"]))
        log.debug(
            "–†–∞—Å—á–µ—Ç BPM (patched): resolved=%s, header=%s, estimated=%s",
            bpm,
            analysis.get("header_bpm"),
            analysis.get("estimated_bpm"),
        )
        return bpm


class PatchedUniversalFrequencyEngine:
    def resonance_profile(self, tlp: Dict[str, float]) -> Dict[str, Any]:
        cf = tlp.get("conscious_frequency", 0.0)
        if cf > 0.7:
            rec = [4, 5, 6, 7]
        elif cf > 0.3:
            rec = [2, 3, 4, 5]
        else:
            rec = [1, 2, 3, 4]
        return {"recommended_octaves": rec}


class PatchedRNSSafety:
    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg.get("safety", {"safe_octaves": [2, 3, 4, 5]})

    def clamp_octaves(self, octaves: List[int]) -> List[int]:
        safe = set(self.cfg.get("safe_octaves", [2, 3, 4, 5]))
        arr = [o for o in octaves if o in safe]
        return arr or [2, 3, 4]


class PatchedIntegrityScanEngine:
    def __init__(self):
        self._engine = FullIntegrityScanEngine()

    def analyze(
        self, 
        text: str,
        # Task 2.3: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        emotions: Optional[Dict[str, float]] = None,
        tlp: Optional[Dict[str, float]] = None,
    ) -> Dict[str, Any]:
        """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–≥–ª—É—à–∫—É –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (V6 Logic)."""
        return self._engine.analyze(text, emotions=emotions, tlp=tlp)


class AdaptiveVocalAllocator:
    def __init__(self):
        self._vocal_registry = VocalProfileRegistry()

    def analyze(
        self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, text: str
    ) -> Dict[str, Any]:
        """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–≥–ª—É—à–∫—É –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–ª–ª–æ–∫–∞—Ç–æ—Ä –≤–æ–∫–∞–ª–∞ (V6 Logic)."""
        # Task 2.3: –ü–µ—Ä–µ–¥–∞–µ–º emotions –∏ tlp –≤ get() –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º V6 –ª–æ–≥–∏–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–π / TLP
        vox, _, vocal_form = self._vocal_registry.get("default", "auto", text, [], [], emotions=emo, tlp=tlp)
        vocal_count = len(
            [
                v
                for v in vox
                if v in ["solo", "duet", "trio", "quartet", "quintet", "choir"]
            ]
        )

        return {
            "vocal_form": vocal_form,
            "gender": "auto",
            "vocal_count": vocal_count or 1,
        }


# ==========================================================
# üöÄ StudioCore Monolith (v4.3.11)
# ==========================================================


class StudioCore:
    def __init__(self, config_path: Optional[str] = None):
        log.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StudioCore...")
        self.cfg = load_config(config_path or "studio_config.json")

        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: AutoEmotionalAnalyzer")
        self.emotion = AutoEmotionalAnalyzer()
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: TruthLovePainEngine")
        self.tlp = TruthLovePainEngine()

        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: PatchedLyricMeter")
        self.rhythm = PatchedLyricMeter()
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: PatchedUniversalFrequencyEngine")
        self.freq = PatchedUniversalFrequencyEngine()
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: PatchedRNSSafety")
        self.safety = PatchedRNSSafety(self.cfg)
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: PatchedIntegrityScanEngine")
        self.integrity = PatchedIntegrityScanEngine()
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: VocalProfileRegistry")
        self.vocals = VocalProfileRegistry()

        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: PatchedStyleMatrix")
        try:
            # (PatchedStyleMatrix - —ç—Ç–æ –Ω–∞—à StyleMatrix v11)
            self.style = PatchedStyleMatrix()
            log.info(
                "üé® [StyleMatrix] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (PatchedStyleMatrix)."
            )
        except ImportError as e:
            log.error(f"–ù–ï –£–î–ê–õ–û–°–¨ –∑–∞–≥—Ä—É–∑–∏—Ç—å PatchedStyleMatrix: {e}")
            self.style = None  # type: ignore

        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: ToneSyncEngine")
        self.tone = ToneSyncEngine()
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: AdaptiveVocalAllocator")
        self.vocal_allocator = AdaptiveVocalAllocator()
        
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: ColorEngineAdapter")
        self.color_engine = ColorEngineAdapter()
        
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: RhythmDynamicsEmotionEngine")
        self.rde_engine = RhythmDynamicsEmotionEngine()

        log.info(
            f"üîπ [StudioCore {STUDIOCORE_VERSION}] Monolith loaded (Section - Aware Duet Mode v2)."
        )

    # -------------------------------------------------------
    # üé§ (v4) –ê–Ω–∞–ª–∏–∑ –≤–æ–∫–∞–ª–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º
    # -------------------------------------------------------

    def _analyze_sections(
        self, text_blocks: List[str], ui_gender: str
    ) -> Dict[str, Any]:
        """
        v4: –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –∫–∞–∂–¥—ã–π –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ —Ö–∏–Ω—Ç—ã,
        –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å –≤–æ–∫–∞–ª–∞.
        """
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: _analyze_sections")

        section_profiles: List[Dict[str, Optional[str]]] = []
        final_gender = "auto"
        user_voice_hint = None  # –•–∏–Ω—Ç, –∑–∞–¥–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º

        # 1. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
        for block_text in text_blocks:
            # 1a. –ò—â–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É ("—è —à–µ–ª" / "—è –∂–¥–∞–ª–∞")
            g_gender = detect_gender_from_grammar(block_text)

            # 1b. –ò—â–µ–º —Ö–∏–Ω—Ç—ã ("(—à–µ–ø–æ—Ç–æ–º)", "(–º—É–∂—Å–∫–æ–π –≤–æ–∫–∞–ª)")
            v_hint = detect_voice_profile(block_text)

            if v_hint and not user_voice_hint:
                user_voice_hint = v_hint  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ö–∏–Ω—Ç

            section_profiles.append({"gender": g_gender, "hint": v_hint})
            # v5: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ f - string (—É–±—Ä–∞–Ω–æ —Ç—Ä–æ–µ—Ç–æ—á–∏–µ)
            log.debug(f"–ë–ª–æ–∫ [{block_text[:20]}...] -> –ü–æ–ª: {g_gender}, –•–∏–Ω—Ç: {v_hint}")

        # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª–∞ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã)
        genders_found = {p["gender"] for p in section_profiles if p["gender"]}

        if ui_gender != "auto":
            final_gender = ui_gender  # 1. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç UI
            log.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª –∏–∑ UI")
        elif "male" in genders_found and "female" in genders_found:
            final_gender = "mixed"  # 2. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥—É—ç—Ç
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'mixed' (M / F)")
        elif "male" in genders_found:
            final_gender = "male"  # 3. –¢–æ–ª—å–∫–æ –º—É–∂—Å–∫–æ–π
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'male'")
        elif "female" in genders_found:
            final_gender = "female"  # 4. –¢–æ–ª—å–∫–æ –∂–µ–Ω—Å–∫–∏–π
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'female'")
        # (else: –æ—Å—Ç–∞–µ—Ç—Å—è 'auto')

        log.debug(f"–ò—Ç–æ–≥ –ø–æ –≤–æ–∫–∞–ª—É (–≤—Å–µ –±–ª–æ–∫–∏): {genders_found}")
        return {
            "final_gender_preference": final_gender,
            "user_voice_hint": user_voice_hint,
            "section_profiles": section_profiles,
        }

    # -------------------------------------------------------
    # üéº (v6) –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π
    # -------------------------------------------------------

    def _build_semantic_layers(
        self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, style_key: str
    ) -> Dict[str, Any]:
        """v6: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 'energy', 'arrangement' –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'key'"""
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: _build_semantic_layers")

        love, pain, truth = tlp.get("love", 0), tlp.get("pain", 0), tlp.get("truth", 0)
        cf = tlp.get("conscious_frequency", 0)
        # avg_emo reserved for future use

        key = style_key  # –ë–µ—Ä–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑ style.py

        def get_focus(mood: str, energy: str) -> str:
            if energy == "high":
                return "climax"
            if energy == "low":
                return "minimal"
            if mood == "dramatic":
                return "contrast"
            if mood == "narrative":
                return "story_flow"
            return "flow"

        # (v6) –õ–æ–≥–∏–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —à–∞–±–ª–æ–Ω–∞—Ö Suno
        intro = {
            "tag": "Intro",
            "mood": "mystic" if cf >= 0.5 else "calm",
            "energy": "low",
            "arrangement": "minimal",
            "bpm": int(bpm * 0.8),
            "key": key,
        }
        verse = {
            "tag": "Verse",
            "mood": "narrative" if love >= truth else "reflective",
            "energy": "mid",
            "arrangement": "standard",
            "bpm": bpm,
            "key": key,
        }
        bridge = {
            "tag": "Bridge",
            "mood": "dramatic" if pain > 0.3 else "dreamlike",
            "energy": "mid - high",
            "arrangement": "building",
            "bpm": int(bpm * 1.05),
            "key": key,
        }
        chorus = {
            "tag": "Chorus",
            "mood": "uplifting" if love >= pain else "tense",
            "energy": "high",
            "arrangement": "full arrangement",
            "bpm": int(bpm * 1.1),
            "key": key,
        }
        outro = {
            "tag": "Outro",
            "mood": "peaceful" if cf > 0.6 else "fading",
            "energy": "low",
            "arrangement": "minimal",
            "bpm": int(bpm * 0.7),
            "key": key,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º 'focus' –Ω–∞ –æ—Å–Ω–æ–≤–µ mood / energy
        for s in [intro, verse, bridge, chorus, outro]:
            s["focus"] = get_focus(s["mood"], s["energy"])

        # v6: BPM —Ç–µ–ø–µ—Ä—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç TLP (–±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç)
        bpm_adj = int(bpm + (love * 10) - (pain * 15) + (truth * 5))
        bpm_adj = max(60, min(180, bpm_adj))
        log.debug(f"BPM —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –¥–æ {bpm_adj}")

        return {
            "bpm_suggested": bpm_adj,
            "layers": {
                "depth": round((truth + pain) / 2, 2),
                "warmth": round(love, 2),
                "clarity": round(cf, 2),
                "sections": [intro, verse, bridge, chorus, outro],
            },
        }

    # -------------------------------------------------------
    # ‚úçÔ∏è (v6) –ê–Ω–Ω–æ—Ç–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
    # -------------------------------------------------------

    def annotate_text(
        self,
        text_blocks: List[str],
        section_profiles: List[Dict[str, Optional[str]]],
        semantic_sections: List[Dict[str, Any]],
    ) -> Tuple[str, str]:
        """
        v6: –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–Ω. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 2 –≤–µ—Ä—Å–∏–∏:
        1.  `annotated_text_ui`: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π (–¥–ª—è Gradio)
        2.  `annotated_text_suno`: –ß–∏—Å—Ç—ã–π (–¥–ª—è Suno Lyrics)
        """
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: annotate_text (v6)")

        ui_blocks = []
        suno_blocks = []

        num_blocks = len(text_blocks)

        # --- (v6) –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –º—ç–ø–ø–∏–Ω–≥–∞ —Å–µ–∫—Ü–∏–π ---
        # intro, verse1, chorus1, verse2, chorus2, bridge, chorus3, outro
        semantic_map = []
        if num_blocks <= 5:
            semantic_map = ["Intro", "Verse", "Bridge", "Chorus", "Outro"]
        elif num_blocks == 6:
            semantic_map = ["Intro", "Verse", "Chorus", "Verse", "Chorus", "Outro"]
        elif num_blocks == 7:
            semantic_map = [
                "Intro",
                "Verse",
                "Pre - Chorus",
                "Chorus",
                "Verse",
                "Bridge",
                "Outro",
            ]
        else:  # 8+
            semantic_map = [
                "Intro",
                "Verse",
                "Pre - Chorus",
                "Chorus",
                "Verse",
                "Bridge",
                "Chorus",
                "Outro",
            ]

        # –î–æ–ø–æ–ª–Ω—è–µ–º –∫–∞—Ä—Ç—É, –µ—Å–ª–∏ –±–ª–æ–∫–æ–≤ –±–æ–ª—å—à–µ 8
        if num_blocks > len(semantic_map):
            semantic_map.extend(["Verse", "Chorus"] * (num_blocks - len(semantic_map)))

        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–∫—Ü–∏–π
        sec_defs = {s["tag"].lower(): s for s in semantic_sections}
        # –ó–∞–ø–∞—Å–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        sec_defs.setdefault("pre - chorus", sec_defs["bridge"])
        sec_defs.setdefault("verse 2", sec_defs["verse"])
        # --- –ö–æ–Ω–µ—Ü –ª–æ–≥–∏–∫–∏ –º—ç–ø–ø–∏–Ω–≥–∞ ---

        # FIX: Use the calculated Chorus BPM as the final tag BPM for
        # consistency (semantic_sections[3] = Chorus)
        final_bpm = semantic_sections[3].get("bpm", 120)  # (BPM –ø—Ä–∏–ø–µ–≤–∞)
        final_key = semantic_sections[3].get("key", "auto")
        # final_vocal_form reserved for future use

        for i, block_text in enumerate(text_blocks):
            if not block_text.strip():
                continue

            # 1. –ë–µ—Ä–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫—É (Intro, Verse...)
            tag_name = semantic_map[i].lower()
            # Fallback –Ω–∞ 'verse'
            sem = sec_defs.get(tag_name, sec_defs["verse"])

            # 2. –ë–µ—Ä–µ–º –≤–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å (Male, Female...)
            profile = section_profiles[i]
            # MALE, FEMALE, MIXED, AUTO
            gender_tag = profile.get("gender", "auto").upper()

            # 3. –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏
            suno_tag_parts = [
                tag_name.upper(),
                f"vocal: {gender_tag}" if gender_tag != "AUTO" else None,
                f"mood: {sem['mood']}",
                f"energy: {sem['energy']}",
                f"arrangement: {sem['arrangement']}",
            ]
            suno_tag = f"[{' - '.join(filter(None, suno_tag_parts))}]"

            # UI - —Ç–µ–≥ (–±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π)
            ui_tag = f"[{tag_name.upper()} - {gender_tag} - {sem['mood']}, {sem['energy']}, {sem['arrangement']}, BPM‚âà{sem['bpm']}]"

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π BPM / Key (–±–µ—Ä–µ–º –∏–∑ –ø—Ä–∏–ø–µ–≤–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
            if "chorus" in tag_name:
                final_bpm = sem["bpm"]
                final_key = sem["key"]

            ui_blocks.append(ui_tag)
            ui_blocks.append(block_text)
            ui_blocks.append("")

            suno_blocks.append(suno_tag)
            suno_blocks.append(block_text)
            suno_blocks.append("")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–≥
        # (vocal_form –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ 'analyze')
        end_tag_ui = f"[End ‚Äì BPM‚âà{final_bpm}, Tone={final_key}]"
        end_tag_suno = f"[{final_bpm} BPM, {final_key}]"

        ui_blocks.append(end_tag_ui)
        suno_blocks.append(end_tag_suno)

        return "\n".join(ui_blocks).strip(), "\n".join(suno_blocks).strip()

    # -------------------------------------------------------
    # üöÄ –ì–ª–∞–≤–Ω—ã–π –ü–∞–π–ø–ª–∞–π–Ω
    # -------------------------------------------------------

    def _check_safety(self, text: str) -> str:
        """
        Task 1.1: Safety check method that validates input length and checks for aggression keywords.
        Returns the text (possibly replaced with neutral text if aggression detected).
        """
        # Input type validation
        if not text or not isinstance(text, str):
            raise ValueError("Text input is required and must be a string")
        
        # Length validation
        if len(text) > DEFAULT_CONFIG.MAX_INPUT_LENGTH:
            raise ValueError(
                f"Text length ({len(text)}) exceeds maximum allowed length ({DEFAULT_CONFIG.MAX_INPUT_LENGTH})"
            )
        
        # Aggression filter
        aggression_keywords = DEFAULT_CONFIG.AGGRESSION_KEYWORDS
        text_lower = text.lower()
        found_keywords = [kw for kw in aggression_keywords if kw.lower() in text_lower]
        if found_keywords:
            log.warning(
                f"Aggression keywords detected: {found_keywords}. Replacing with neutral text."
            )
            text = DEFAULT_CONFIG.FALLBACK_NEUTRAL_TEXT
        
        return text

    def _infer_gender_from_text(self, text: str) -> Optional[str]:
        """
        Infer gender from Russian text grammar (past tense verb endings).
        Returns 'male', 'female', or None if unclear.
        """
        # Russian past tense patterns: "–ª" (male) vs "–ª–∞" (female)
        male_patterns = [
            r'\b\w+–ª\b',  # Words ending with "–ª" (past tense, male)
            r'\b\w+–ª—Å—è\b',  # Reflexive verbs ending with "–ª—Å—è" (male)
        ]
        female_patterns = [
            r'\b\w+–ª–∞\b',  # Words ending with "–ª–∞" (past tense, female)
            r'\b\w+–ª–∞—Å—å\b',  # Reflexive verbs ending with "–ª–∞—Å—å" (female)
        ]
        
        male_matches = []
        female_matches = []
        for pattern in male_patterns:
            male_matches.extend(re.findall(pattern, text, re.IGNORECASE))
        for pattern in female_patterns:
            female_matches.extend(re.findall(pattern, text, re.IGNORECASE))
        
        male_count = len(male_matches)
        female_count = len(female_matches)
        
        if male_count > female_count and male_count > 0:
            return "male"
        elif female_count > male_count and female_count > 0:
            return "female"
        elif male_count == female_count and male_count > 0:
            # If equal, check which appears first in text
            first_male_pos = min((text.lower().find(m) for m in male_matches), default=len(text))
            first_female_pos = min((text.lower().find(m) for m in female_matches), default=len(text))
            if first_male_pos < first_female_pos:
                return "male"
            elif first_female_pos < first_male_pos:
                return "female"
        return None

    def _infer_texture_from_mood(self, mood: str, emotions: Dict[str, float]) -> str:
        """
        Infer vocal texture based on mood and emotions.
        """
        mood_lower = (mood or "").lower()
        emotion_keys = [k.lower() for k in emotions.keys()] if emotions else []
        
        # Reflective/introspective moods -> breathy/intimate
        if any(word in mood_lower for word in ["reflective", "introspective", "melancholic", "nostalgic", "peaceful"]):
            return "breathy/intimate"
        if any(word in emotion_keys for word in ["peace", "nostalgia", "melancholy"]):
            return "breathy/intimate"
        
        # Uplifting/energetic moods -> resonant
        if any(word in mood_lower for word in ["uplifting", "energetic", "joyful", "triumphant"]):
            return "resonant"
        if any(word in emotion_keys for word in ["joy", "triumph", "energy"]):
            return "resonant"
        
        # Default
        return "dynamic"

    def _generate_color_signature_from_emotions(self, emotions: Dict[str, float]) -> str:
        """
        Generate color signature from primary emotion.
        """
        if not emotions:
            return "neutral"
        
        dominant = max(emotions, key=emotions.get)
        
        # Emotion to color mapping
        emotion_color_map = {
            "nostalgia": "sepia/orange",
            "pain": "grey/blue",
            "joy": "yellow/gold",
            "peace": "green/teal",
            "love": "pink/rose",
            "melancholy": "blue/grey",
            "triumph": "red/orange",
            "anger": "red/dark",
            "fear": "purple/dark",
            "sadness": "blue/indigo",
        }
        
        return emotion_color_map.get(dominant.lower(), "neutral")

    def _generate_resonance_hz_from_key(self, key: str) -> float:
        """
        Generate mock resonance_hz based on key.
        """
        # Base frequencies for common keys (approximate)
        key_freq_map = {
            "c": 130.81,
            "c#": 138.59,
            "d": 146.83,
            "d#": 155.56,
            "e": 164.81,
            "f": 174.61,
            "f#": 185.00,
            "g": 196.00,
            "g#": 207.65,
            "a": 220.00,
            "a#": 233.08,
            "b": 246.94,
        }
        
        # Extract key root (first letter, case-insensitive)
        key_lower = (key or "").lower().strip()
        key_root = key_lower.split()[0] if key_lower else "c"
        
        # Remove # and b (sharp/flat) for matching
        key_root = key_root.replace("#", "").replace("b", "")
        
        base_freq = key_freq_map.get(key_root, 130.81)
        
        # Minor keys typically ~10Hz lower
        if "minor" in key_lower:
            base_freq -= 10.0
        
        return round(base_freq, 2)

    def _generate_breathing_map_from_punctuation(self, text: str) -> Dict[str, Any]:
        """
        Generate simple breathing map based on punctuation.
        Commas = short breath, periods = long breath.
        """
        breathing_points = []
        text_length = len(text)
        
        for i, char in enumerate(text):
            if char == ',':
                breathing_points.append({
                    "position": i,
                    "type": "short",
                    "duration_ms": 200,
                })
            elif char in '.!?':
                breathing_points.append({
                    "position": i,
                    "type": "long",
                    "duration_ms": 500,
                })
        
        return {
            "breathing_points": breathing_points,
            "total_points": len(breathing_points),
            "inhale_points": [p["position"] for p in breathing_points if p["type"] == "long"],
            "exhale_points": [p["position"] for p in breathing_points if p["type"] == "short"],
        }

    def _enrich_result_with_smart_defaults(
        self, result: Dict[str, Any], text: str, preferred_gender: str
    ) -> Dict[str, Any]:
        """
        Enrich result dictionary with smart defaults for missing fields.
        """
        # 1. Vocal Inference
        vocal = result.get("vocal", {})
        if isinstance(vocal, dict):
            # Infer gender if auto and not set
            if preferred_gender == "auto" and vocal.get("gender") == "auto":
                inferred_gender = self._infer_gender_from_text(text)
                if inferred_gender:
                    vocal["gender"] = inferred_gender
                    log.debug(f"Inferred gender from text: {inferred_gender}")
            
            # Set texture based on mood if missing
            if not vocal.get("texture"):
                style = result.get("style", {})
                mood = style.get("mood") or style.get("atmosphere") or "neutral"
                emotions = result.get("emotions", {})
                texture = self._infer_texture_from_mood(mood, emotions)
                vocal["texture"] = texture
                log.debug(f"Inferred texture from mood: {texture}")
            
            result["vocal"] = vocal
        
        # 2. Color & Resonance
        style = result.get("style", {})
        if isinstance(style, dict):
            # Add color_signature if missing
            if not style.get("color_signature"):
                emotions = result.get("emotions", {})
                color_sig = self._generate_color_signature_from_emotions(emotions)
                style["color_signature"] = color_sig
                log.debug(f"Generated color_signature: {color_sig}")
            
            result["style"] = style
        
        # Add resonance_hz to RDE if missing
        rde = result.get("rde", {})
        if isinstance(rde, dict) and not rde.get("resonance_hz"):
            key = result.get("key", "C major")
            resonance_hz = self._generate_resonance_hz_from_key(key)
            rde["resonance_hz"] = resonance_hz
            log.debug(f"Generated resonance_hz from key: {resonance_hz}")
            result["rde"] = rde
        
        # 3. ZeroPulse / Breathing
        if not result.get("breathing") and not result.get("zeropulse"):
            breathing_map = self._generate_breathing_map_from_punctuation(text)
            result["breathing"] = breathing_map
            # ZeroPulse is typically derived from breathing
            result["zeropulse"] = {
                "breathing_sync": breathing_map.get("total_points", 0) > 0,
                "points": breathing_map.get("breathing_points", []),
            }
            log.debug(f"Generated breathing map with {breathing_map.get('total_points', 0)} points")
        
        # 4. Genre - Ensure secondary is populated
        style = result.get("style", {})
        if isinstance(style, dict) and not style.get("secondary"):
            genre = style.get("genre", "")
            # If genre contains "hybrid", try to extract secondary from it
            if genre and "hybrid" in str(genre).lower():
                # Split hybrid genre (e.g., "folk rock hybrid" -> ["folk", "rock"])
                genre_parts = str(genre).lower().replace(" hybrid", "").split()
                if len(genre_parts) >= 2:
                    # Use the second part as secondary
                    style["secondary"] = genre_parts[1]
                    log.debug(f"Extracted secondary genre from hybrid: {style['secondary']}")
            # If genre_source indicates hybrid_genre_engine was used, we can infer secondary
            elif style.get("genre_source") == "hybrid_genre_engine" and genre:
                # Try to extract from genre name if it has multiple words
                genre_words = str(genre).split()
                if len(genre_words) >= 2:
                    style["secondary"] = genre_words[1]
                    log.debug(f"Inferred secondary genre: {style['secondary']}")
            
            result["style"] = style
        
        return result

    def analyze(
        self,
        text: str,
        preferred_gender: str = "auto",
        version: Optional[str] = None,
        semantic_hints: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        # Task 10.2: Start timer for runtime metrics
        start_time = time.time()
        
        log.debug(f"--- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê (v{STUDIOCORE_VERSION}) ---")
        log.debug(f"Preferred Gender: {preferred_gender}, Text: {text[:40]}...")

        # Task 1.1: Safety check at the start of analyze
        text = self._check_safety(text)

        raw = normalize_text_preserve_symbols(text)
        text_blocks = extract_raw_blocks(raw)

        # Task 1.2: Section Analysis
        section_result = self._analyze_sections(text_blocks, preferred_gender)
        section_profiles = section_result.get("section_profiles", [])
        voice_hint = section_result.get("user_voice_hint")

        # Task 10.1: Run independent engines in parallel using ThreadPoolExecutor
        # emotion and tone are independent, so they can run in parallel
        emotions = None
        tone_hint = None
        
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Submit independent tasks (emotion and tone don't depend on each other)
            future_emotion = executor.submit(self.emotion.analyze, raw)
            future_tone = executor.submit(self.tone.detect_key, raw)
            
            # Wait for results
            emotions = future_emotion.result()
            tone_hint = future_tone.result()
        
        # Initialize integrity_result to None (will be set later after tlp is available)
        integrity_result = None
        
        log.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç EMO (–¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏): {emotions}")
        
        # Task 3.1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ EMOTION_HIGH_SIGNAL –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–ª–∞–±—ã—Ö —ç–º–æ—Ü–∏–π
        emotion_high_signal = DEFAULT_CONFIG.EMOTION_HIGH_SIGNAL
        if emotions:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —ç–º–æ—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ EMOTION_HIGH_SIGNAL
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            filtered_emotions = {
                k: v for k, v in emotions.items() 
                if v >= emotion_high_signal or k == max(emotions, key=emotions.get)
            }
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —ç–º–æ—Ü–∏—è, 
            # –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            if len(filtered_emotions) < len(emotions) and len(filtered_emotions) > 0:
                total_filtered = sum(filtered_emotions.values())
                if total_filtered > 0:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–æ—Ü–∏–∏
                    emotions = {k: v / total_filtered for k, v in filtered_emotions.items()}
                    log.debug(f"–≠–º–æ—Ü–∏–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –ø–æ EMOTION_HIGH_SIGNAL ({emotion_high_signal}): {emotions}")
                else:
                    # –ï—Å–ª–∏ –≤—Å–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é
                    dominant = max(emotions, key=emotions.get)
                    emotions = {dominant: 1.0}
                    log.debug(f"–í—Å–µ —ç–º–æ—Ü–∏–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞, –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è: {dominant}")
            else:
                log.debug(f"–≠–º–æ—Ü–∏–∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–≤—Å–µ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ {emotion_high_signal})")
        
        log.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç EMO (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏): {emotions}")

        # Task 1.1: TLP Analysis
        tlp = self.tlp.analyze(raw)
        cf = tlp.get("conscious_frequency")
        log.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç TLP: {tlp}, CF: {cf}")

        rhythm_analysis = self.rhythm.analyze(raw, emotions=emotions, tlp=tlp, cf=cf)
        bpm = int(round(rhythm_analysis.get("global_bpm", DEFAULT_CONFIG.FALLBACK_BPM)))
        log.debug(
            "–ë–∞–∑–æ–≤—ã–π BPM: %s (header=%s, estimated=%s)",
            bpm,
            rhythm_analysis.get("header_bpm"),
            rhythm_analysis.get("estimated_bpm"),
        )
        
        # Task 18.1: Auto-resolve BPM-TLP conflicts
        consistency = ConsistencyLayerV8({"bpm": bpm, "tlp": tlp})
        suggested_bpm, was_resolved = consistency.resolve_bpm_tlp_conflict(bpm, tlp)
        if was_resolved:
            log.debug(f"BPM-TLP Konflikt aufgel√∂st: {bpm} ‚Üí {suggested_bpm}")
            bpm = int(round(suggested_bpm))

        # Task 9.1: tone_hint already obtained from parallel execution
        key = tone_hint.get("key") if tone_hint else DEFAULT_CONFIG.FALLBACK_KEY
        if not key or key == "auto":
            key = DEFAULT_CONFIG.FALLBACK_KEY

        # Task 1.3: Style.build() –≤–º–µ—Å—Ç–æ FALLBACK –∑–Ω–∞—á–µ–Ω–∏–π
        if self.style:
            style_result = self.style.build(
                emotions, tlp, raw, bpm, semantic_hints, voice_hint
            )
            style = style_result
        else:
            # Fallback –µ—Å–ª–∏ style engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            style = {
                "genre": DEFAULT_CONFIG.FALLBACK_STYLE,
                "style": DEFAULT_CONFIG.FALLBACK_STYLE,
                "bpm": bpm,
                "key": key,
                "visual": DEFAULT_CONFIG.FALLBACK_VISUAL,
                "narrative": DEFAULT_CONFIG.FALLBACK_NARRATIVE,
                "structure": DEFAULT_CONFIG.FALLBACK_STRUCTURE,
                "emotion": emotions.get("dominant") or DEFAULT_CONFIG.FALLBACK_EMOTION,
            }
            log.warning("Style engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è FALLBACK –∑–Ω–∞—á–µ–Ω–∏—è")

        # Task 1.4: Semantic Layers
        semantic_layers = self._build_semantic_layers(emotions, tlp, bpm, key)
        semantic_sections = semantic_layers.get("layers", {}).get("sections", [])

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º layout –∏–∑ semantic_sections –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        layout = DEFAULT_CONFIG.FALLBACK_STRUCTURE
        if semantic_sections and len(semantic_sections) > 0:
            layout = semantic_sections[0].get("tag", DEFAULT_CONFIG.FALLBACK_STRUCTURE)

        structure = {
            "sections": text_blocks,
            "section_count": len(text_blocks),
            "layout": layout,
        }

        # Task 1.6: Vocal Allocator
        vocal_result = self.vocal_allocator.analyze(emotions, tlp, bpm, raw)
        log.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç Vocal: {vocal_result}")

        # Task 1.6: Integrity Scan
        # Task 2.3: –ü–µ—Ä–µ–¥–∞–µ–º emotions –∏ tlp –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        # Note: integrity requires emotions and tlp, so it runs after they are available (not in parallel)
        integrity_result = self.integrity.analyze(raw, emotions=emotions, tlp=tlp)
        log.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç Integrity: {integrity_result}")

        # Task 1.5: Text Annotation
        annotated_text_ui, annotated_text_suno = self.annotate_text(
            text_blocks, section_profiles, semantic_sections
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: Color Resolution
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è Color Engine
        intermediate_result = {
            "emotions": emotions,
            "tlp": tlp,
            "style": style,
        }
        color_resolution = self.color_engine.resolve_color_wave(intermediate_result)
        color_wave = color_resolution.colors if color_resolution else []
        
        # Task 18.2: Auto-resolve Color-Key conflicts
        resolver = GenreConflictResolver()
        suggested_key, was_resolved = resolver.resolve_color_key_conflict(
            key, color_wave, style
        )
        if was_resolved:
            log.debug(f"Color-Key Konflikt aufgel√∂st: {key} ‚Üí {suggested_key}")
            key = suggested_key
            # Update style dict with new key
            style["key"] = key

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: RDE Analysis
        # RDE —Ç—Ä–µ–±—É–µ—Ç bpm_payload, breathing_profile, emotion_profile, instrumentation_payload
        rde_result = {
            "resonance": self.rde_engine.calc_resonance(raw),
            "fracture": self.rde_engine.calc_fracture(raw),
            "entropy": self.rde_engine.calc_entropy(raw),
        }
        # –ï—Å–ª–∏ –µ—Å—Ç—å TLP, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ emotion vector
        if tlp:
            try:
                rde_emotion_vector = self.rde_engine.export_emotion_vector(raw)
                rde_result["emotion_vector"] = {
                    "valence": rde_emotion_vector.valence,
                    "arousal": rde_emotion_vector.arousal,
                }
            except Exception as e:
                log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å RDE emotion vector: {e}")
        
        # Task 18.1: Auto-resolve Genre-RDE conflicts
        genre = style.get("genre", "")
        adjusted_rde, was_resolved = consistency.resolve_genre_rde_conflict(genre, rde_result)
        if was_resolved:
            log.debug(f"Genre-RDE Konflikt aufgel√∂st: {rde_result} ‚Üí {adjusted_rde}")
            rde_result = adjusted_rde

        # Task 10.2: Calculate runtime and add to result
        runtime_ms = int((time.time() - start_time) * 1000)
        log.debug(f"--- –ê–ù–ê–õ–ò–ó –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù (runtime: {runtime_ms}ms) ---")

        # Task 1.7: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π return —Å–ª–æ–≤–∞—Ä—å —Å –≤—Å–µ–º–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        result = {
            "emotions": emotions,
            "tlp": tlp,
            "bpm": bpm,
            "key": key,
            "structure": structure,
            "style": style,
            "vocal": vocal_result,
            "semantic_layers": semantic_layers,
            "integrity": integrity_result,
            "annotated_text_ui": annotated_text_ui,
            "annotated_text_suno": annotated_text_suno,
            "color_wave": color_wave,
            "rde": rde_result,
            # Task 10.2: Add runtime metrics for diagnostics
            "runtime_ms": runtime_ms,
        }
        
        # Enrich result with smart defaults for missing fields
        result = self._enrich_result_with_smart_defaults(result, text, preferred_gender)
        
        return result


class StudioCoreV5:
    """Compatibility wrapper exposing the v5 API expected by legacy tools."""

    def __init__(self, *args, **kwargs):
        self._core = StudioCore(*args, **kwargs)

    def analyze(self, *args, **kwargs):
        return self._core.analyze(*args, **kwargs)

    def emotion(self, text: str):
        return self._core.emotion.analyze(text)

    def style(
        self,
        emo: Dict[str, float],
        tlp: Dict[str, float],
        text: str,
        bpm: int,
        semantic_hints: Optional[Dict[str, Any]] = None,
        voice_hint: Optional[str] = None,
    ) -> Dict[str, Any]:
        if not self._core.style:
            raise RuntimeError("Style subsystem is unavailable.")
        return self._core.style.build(emo, tlp, text, bpm, semantic_hints, voice_hint)

    def tone(
        self,
        emo: Dict[str, float],
        tlp: Dict[str, float],
        key_hint: Optional[str] = None,
    ):
        return self._core.tone.colors_for_primary(emo, tlp, key_hint or "auto")

    def rhythm(
        self,
        text: str,
        *,
        emotions: Optional[Dict[str, float]] = None,
        tlp: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        header_bpm: Optional[float] = None,
    ):
        return self._core.rhythm.analyze(
            text,
            emotions=emotions,
            tlp=tlp,
            cf=cf,
            header_bpm=header_bpm,
        )

    def __getattr__(self, item: str):
        return getattr(self._core, item)


# ==========================================================
# Task 6.2: Version is now imported from config.py
log.info(
    f"üîπ [StudioCore {MONOLITH_VERSION}] Monolith loaded (Section - Aware Duet Mode v2)."
)

# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e
