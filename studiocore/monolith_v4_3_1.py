# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e
# -*- coding: utf - 8 -*-
"""
StudioCore Monolith (v6 - Suno –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏)
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ f - string (—Å—Ç—Ä–æ–∫–∞ 259)
- –í–Ω–µ–¥—Ä–µ–Ω–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –í–Ω–µ–¥—Ä–µ–Ω–∞ Suno - –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –≤ 'annotate_text'
- Task 6.2: Version now imported from config.py
"""

import re
import random
import time
from typing import Dict, Any, List, Tuple, Optional
import logging

# === 1. –ò–º–ø–æ—Ä—Ç —è–¥—Ä–∞ ===
# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e

# AI_TRAINING_PROHIBITED: Redistribution or training of AI models on this codebase
# without explicit written permission from the Author is prohibited.

from .config import DEFAULT_CONFIG, load_config

# Task 6.2: Import version from config instead of hardcoding
MONOLITH_VERSION = DEFAULT_CONFIG.MONOLITH_VERSION
STUDIOCORE_VERSION = DEFAULT_CONFIG.STUDIOCORE_VERSION

# v16: –ò–°–ü–†–ê–í–õ–ï–ù ImportError
from .text_utils import normalize_text_preserve_symbols, extract_raw_blocks

# v15: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω ImportError (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞)
from .emotion import AutoEmotionalAnalyzer, TruthLovePainEngine
from .tone import ToneSyncEngine
from .vocals import VocalProfileRegistry
from .integrity import (
    IntegrityScanEngine as FullIntegrityScanEngine,
)  # –ò–º–ø–æ—Ä—Ç –¥–≤–∏–∂–∫–∞ V6
from .rhythm import LyricMeter

# v11: 'PatchedStyleMatrix' - —ç—Ç–æ –Ω–∞—à 'StyleMatrix'
from .style import PatchedStyleMatrix
from .color_engine_adapter import ColorEngineAdapter
from .rde_engine import RhythmDynamicsEmotionEngine
# Task 18.1: Import conflict resolution classes
from .consistency_v8 import ConsistencyLayerV8
from .genre_conflict_resolver import GenreConflictResolver

# === 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ ===
log = logging.getLogger(__name__)

# Legacy Bridge: Import Suno prompt builder
try:
    from .adapter import build_suno_prompt
    LEGACY_SUNO_AVAILABLE = True
except ImportError:
    LEGACY_SUNO_AVAILABLE = False
    log.warning("[Legacy Bridge] build_suno_prompt not available, will use fallback")

# Fusion Engine: Import FusionEngineV64 and GenreRoutingEngineV64
try:
    from .fusion_engine_v64 import FusionEngineV64
    from .genre_routing_engine import GenreRoutingEngineV64
    FUSION_ENGINE_AVAILABLE = True
except ImportError:
    FUSION_ENGINE_AVAILABLE = False
    log.warning("[Fusion Engine] FusionEngineV64 not available, will skip fusion")

# Hybrid Genre Engine: Import HybridGenreEngine
try:
    from .hybrid_genre_engine import HybridGenreEngine
    HYBRID_GENRE_ENGINE_AVAILABLE = True
except ImportError:
    HYBRID_GENRE_ENGINE_AVAILABLE = False
    log.warning("[Hybrid Genre Engine] HybridGenreEngine not available, will skip hybrid genre resolution")

# Suno Prompt Engine: Import SunoPromptEngine for advanced tags
try:
    from .suno_advanced_prompts import SunoPromptEngine
    SUNO_PROMPT_ENGINE_AVAILABLE = True
except ImportError:
    SUNO_PROMPT_ENGINE_AVAILABLE = False
    log.warning("[Suno Prompt Engine] SunoPromptEngine not available, will skip advanced tags")

# Emotion-Driven Suno Adapter: Import for emotion-based annotations
try:
    from .suno_annotations import EmotionDrivenSunoAdapter, build_suno_annotations
    EMOTION_SUNO_ADAPTER_AVAILABLE = True
except ImportError:
    EMOTION_SUNO_ADAPTER_AVAILABLE = False
    log.warning("[Emotion Suno Adapter] EmotionDrivenSunoAdapter not available, will skip emotion-driven annotations")

# Suno Annotation Engine: Import for safe annotations
try:
    from .suno_annotations import SunoAnnotationEngine
    SUNO_ANNOTATION_ENGINE_AVAILABLE = True
except ImportError:
    SUNO_ANNOTATION_ENGINE_AVAILABLE = False
    log.warning("[Suno Annotation Engine] SunoAnnotationEngine not available, will skip safe annotations")

# Dynamic Emotion Engine: Import for normalized emotion profile
try:
    from .dynamic_emotion_engine import DynamicEmotionEngine
    DYNAMIC_EMOTION_ENGINE_AVAILABLE = True
except ImportError:
    DYNAMIC_EMOTION_ENGINE_AVAILABLE = False
    log.warning("[Dynamic Emotion Engine] DynamicEmotionEngine not available, will skip normalized emotion profile")

# Genre Database Loader: Import for expanded genre database
try:
    from .genre_database_loader import GenreDatabaseLoader
    GENRE_DATABASE_LOADER_AVAILABLE = True
except ImportError:
    GENRE_DATABASE_LOADER_AVAILABLE = False
    log.warning("[Genre Database Loader] GenreDatabaseLoader not available, will skip expanded genre database")

# ==========================================================
# üó£Ô∏è –£—Ç–∏–ª–∏—Ç—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∫–∞–ª–∞ (v2 - –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –∏–∑ style.py)
# ==========================================================


def detect_voice_profile(text: str) -> Optional[str]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–æ–∫–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    """
    log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: detect_voice_profile")
    text_low = text.lower()
    patterns = [
        r"–ø–æ–¥\s+[–∞ - —èa - z\s,]+–≤–æ–∫–∞–ª",  # –ø–æ–¥ —Ö—Ä–∏–ø–ª—ã–π –º—É–∂—Å–∫–æ–π –≤–æ–∫–∞–ª
        # (soft female growl)
        r"\([\s\S]*?(–≤–æ–∫–∞–ª|voice|growl|scream|—à[–µ—ë]–ø–æ—Ç|–∫—Ä–∏–∫)[\s\S]*?\)",
        r"(–º—É–∂—Å–∫\w+|–∂–µ–Ω—Å–∫\w+)\s + –≤–æ–∫–∞–ª",
        r"(soft|airy|raspy|grit|growl|scream|whisper)",
    ]
    for pattern in patterns:
        match = re.search(pattern, text_low)
        if match:
            hint = match.group(0).strip("() ")
            log.debug(f"–û–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∫–∞–ª–∞ –Ω–∞–π–¥–µ–Ω–æ: {hint}")
            return hint

    log.debug("–û–ø–∏—Å–∞–Ω–∏–µ –≤–æ–∫–∞–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    return None


def detect_gender_from_grammar(text: str) -> Optional[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–ª –ø–æ –≥—Ä–∞–º–º–∞—Ç–∏–∫–µ ("—è —à–µ–ª" / "—è —à–ª–∞")
    """
    log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: detect_gender_from_grammar")
    male_verbs = len(re.findall(r"\b(—è\s+\w + –ª)\b", text, re.I))
    female_verbs = len(re.findall(r"\b(—è\s+\w + –ª–∞)\b", text, re.I))
    log.debug(
        f"–ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: Male —Ö–∏—Ç—ã={male_verbs}, Female —Ö–∏—Ç—ã={female_verbs}"
    )

    if male_verbs > female_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: male")
        return "male"
    if female_verbs > male_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: female")
        return "female"
    if male_verbs > 0 and male_verbs == female_verbs:
        log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞: mixed")
        return "mixed"

    log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ (auto)")
    return "auto"


# ==========================================================
# üîπ –õ–æ–∫–∞–ª—å–Ω—ã–µ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã
# (–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö, —á—Ç–æ–±—ã –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å rhythm.py –∏ —Ç.–¥. –≤ —Ç–µ—Å—Ç–∞—Ö)
# ==========================================================


class PatchedLyricMeter:
    """–û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ –Ω–æ–≤—ã–º LyricMeter –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""

    def __init__(self) -> None:
        self._engine = LyricMeter()

    def analyze(
        self,
        text: str,
        *,
        emotions: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        tlp: Optional[Dict[str, float]] = None,
        header_bpm: Optional[float] = None,
    ):
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: PatchedLyricMeter.analyze")
        tlp = tlp or {}
        if cf is None:
            cf = tlp.get("conscious_frequency")
        return self._engine.analyze(
            text,
            emotions=emotions,
            cf=cf,
            tlp=tlp,
            header_bpm=header_bpm,
        )

    def bpm_from_density(
        self,
        text: str,
        emo: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        tlp: Optional[Dict[str, float]] = None,
    ) -> int:
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: PatchedLyricMeter.bpm_from_density")
        analysis = self.analyze(text, emotions=emo, cf=cf, tlp=tlp)
        bpm = int(round(analysis["global_bpm"]))
        log.debug(
            "–†–∞—Å—á–µ—Ç BPM (patched): resolved=%s, header=%s, estimated=%s",
            bpm,
            analysis.get("header_bpm"),
            analysis.get("estimated_bpm"),
        )
        return bpm


class PatchedUniversalFrequencyEngine:
    def resonance_profile(self, tlp: Dict[str, float]) -> Dict[str, Any]:
        cf = tlp.get("conscious_frequency", 0.0)
        if cf > 0.7:
            rec = [4, 5, 6, 7]
        elif cf > 0.3:
            rec = [2, 3, 4, 5]
        else:
            rec = [1, 2, 3, 4]
        return {"recommended_octaves": rec}


class PatchedRNSSafety:
    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg.get("safety", {"safe_octaves": [2, 3, 4, 5]})

    def clamp_octaves(self, octaves: List[int]) -> List[int]:
        safe = set(self.cfg.get("safe_octaves", [2, 3, 4, 5]))
        arr = [o for o in octaves if o in safe]
        return arr or [2, 3, 4]


class PatchedIntegrityScanEngine:
    def __init__(self):
        self._engine = FullIntegrityScanEngine()

    def analyze(
        self, 
        text: str,
        # Task 2.3: –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        emotions: Optional[Dict[str, float]] = None,
        tlp: Optional[Dict[str, float]] = None,
    ) -> Dict[str, Any]:
        """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–≥–ª—É—à–∫—É –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ (V6 Logic)."""
        return self._engine.analyze(text, emotions=emotions, tlp=tlp)


class AdaptiveVocalAllocator:
    def __init__(self):
        self._vocal_registry = VocalProfileRegistry()

    def analyze(
        self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, text: str
    ) -> Dict[str, Any]:
        """–ó–∞–º–µ–Ω—è–µ—Ç –∑–∞–≥–ª—É—à–∫—É –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–ª–ª–æ–∫–∞—Ç–æ—Ä –≤–æ–∫–∞–ª–∞ (V6 Logic)."""
        # Task 2.3: –ü–µ—Ä–µ–¥–∞–µ–º emotions –∏ tlp –≤ get() –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º V6 –ª–æ–≥–∏–∫—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–π / TLP
        vox, _, vocal_form = self._vocal_registry.get("default", "auto", text, [], [], emotions=emo, tlp=tlp)
        vocal_count = len(
            [
                v
                for v in vox
                if v in ["solo", "duet", "trio", "quartet", "quintet", "choir"]
            ]
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º gender –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–π –∏ TLP
        gender = "auto"
        if emo and isinstance(emo, dict) and len(emo) > 0:
            try:
                from .vocal_techniques import get_vocal_for_emotion
                
                # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —ç–º–æ—Ü–∏—é
                dominant_emotion = max(emo, key=emo.get)
                intensity = emo[dominant_emotion]
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–æ–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —ç–º–æ—Ü–∏–∏
                vocal_techniques = get_vocal_for_emotion(dominant_emotion, intensity)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≥–æ–ª–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏–∫
                female_keywords = ["soprano", "mezzo", "contralto", "female", "head_voice", "falsetto", "whistle", "coloratura", "lyric_soprano", "soft_female", "airy", "ethereal", "angelic"]
                male_keywords = ["tenor", "baritone", "bass", "male", "chest_voice", "guttural", "dramatic", "warm_baritone", "lyric_tenor", "gentle_male"]
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∂–µ–Ω—Å–∫–∏–µ –∏ –º—É–∂—Å–∫–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏
                female_count = sum(1 for tech in vocal_techniques if any(kw in tech.lower() for kw in female_keywords))
                male_count = sum(1 for tech in vocal_techniques if any(kw in tech.lower() for kw in male_keywords))
                
                # –í—ã–±–∏—Ä–∞–µ–º gender –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Ö–Ω–∏–∫
                if female_count > male_count:
                    gender = "female"
                elif male_count > female_count:
                    gender = "male"
                else:
                    # –ï—Å–ª–∏ —Ä–∞–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º TLP –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                    if tlp and isinstance(tlp, dict):
                        love = tlp.get("love", 0.0)
                        pain = tlp.get("pain", 0.0)
                        truth = tlp.get("truth", 0.0)
                        
                        # Love -> female, Pain/Truth -> male
                        if love > pain and love > truth:
                            gender = "female"
                        elif pain > love or truth > love:
                            gender = "male"
                        else:
                            # Fallback –Ω–∞ —ç–º–æ—Ü–∏–∏
                            joy_peace = emo.get("joy", 0) + emo.get("peace", 0) + emo.get("love", 0) + emo.get("awe", 0)
                            anger_epic = emo.get("anger", 0) + emo.get("epic", 0) + emo.get("rage", 0) + emo.get("fear", 0)
                            gender = "female" if joy_peace > anger_epic else "male"
            except (ImportError, AttributeError, Exception) as e:
                log.debug(f"[Vocal Allocator] Could not determine gender from emotions/TLP: {e}, using auto")
                # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç—É—é –ª–æ–≥–∏–∫—É
                if emo and isinstance(emo, dict):
                    joy_peace = emo.get("joy", 0) + emo.get("peace", 0) + emo.get("love", 0)
                    anger_epic = emo.get("anger", 0) + emo.get("epic", 0) + emo.get("rage", 0)
                    gender = "female" if joy_peace > anger_epic else "male"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º vocal style –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–π –∏ TLP
        vocal_style = "standard"
        if emo and isinstance(emo, dict) and len(emo) > 0:
            dominant_emotion = max(emo, key=emo.get) if emo else "neutral"
            
            # –ú–∞–ø–ø–∏–Ω–≥ —ç–º–æ—Ü–∏–π –∫ —Å—Ç–∏–ª—è–º
            emotion_to_style = {
                "joy": "bright",
                "happiness": "bright",
                "love": "soft",
                "peace": "gentle",
                "sadness": "melancholic",
                "melancholy": "melancholic",
                "anger": "aggressive",
                "rage": "harsh",
                "fear": "tense",
                "awe": "epic",
                "epic": "epic",
            }
            vocal_style = emotion_to_style.get(dominant_emotion, "standard")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º vocal tone –Ω–∞ –æ—Å–Ω–æ–≤–µ TLP
        vocal_tone = "neutral"
        if tlp and isinstance(tlp, dict):
            love = tlp.get("love", 0.0)
            pain = tlp.get("pain", 0.0)
            truth = tlp.get("truth", 0.0)
            
            if love > 0.6:
                vocal_tone = "warm"
            elif pain > 0.6:
                vocal_tone = "dark"
            elif truth > 0.6:
                vocal_tone = "clear"
            else:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π –æ—Å–∏
                dominant_axis = max(("love", love), ("pain", pain), ("truth", truth), key=lambda x: x[1])
                if dominant_axis[1] > 0.3:
                    vocal_tone = {"love": "warm", "pain": "dark", "truth": "clear"}.get(dominant_axis[0], "neutral")

        return {
            "vocal_form": vocal_form,
            "gender": gender,
            "vocal_count": vocal_count or 1,
            "style": vocal_style,  # –î–æ–±–∞–≤–ª—è–µ–º style
            "tone": vocal_tone,  # –î–æ–±–∞–≤–ª—è–µ–º tone
        }


# ==========================================================
# üöÄ StudioCore Monolith (v4.3.11)
# ==========================================================


class StudioCore:
    def __init__(self, config_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StudioCore —Å–æ–≥–ª–∞—Å–Ω–æ ACTIVATION_BLUEPRINT.
        –ú–æ–¥—É–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ —Å—Ç—Ä–æ–≥–æ–º –ø–æ—Ä—è–¥–∫–µ —Å–æ–≥–ª–∞—Å–Ω–æ init_sequence.
        """
        log.debug("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StudioCore...")
        
        # === PHASE 0: ConfigLoader ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: ConfigLoader")
        self.cfg = load_config(config_path or "studio_config.json")

        # === PHASE 1: EmotionEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: EmotionEngine (AutoEmotionalAnalyzer)")
        self.emotion = AutoEmotionalAnalyzer()
        
        # === PHASE 2: TLPEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: TLPEngine (TruthLovePainEngine)")
        self.tlp = TruthLovePainEngine()

        # === PHASE 3: RhythmEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: RhythmEngine (PatchedLyricMeter)")
        self.rhythm = PatchedLyricMeter()
        
        # === PHASE 4: FrequencyEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: FrequencyEngine (PatchedUniversalFrequencyEngine)")
        self.freq = PatchedUniversalFrequencyEngine()
        
        # === PHASE 5: SafetyEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: SafetyEngine (PatchedRNSSafety)")
        self.safety = PatchedRNSSafety(self.cfg)
        
        # === PHASE 6: IntegrityScan ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: IntegrityScan (PatchedIntegrityScanEngine)")
        self.integrity = PatchedIntegrityScanEngine()
        
        # === PHASE 7: VocalRegistry ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: VocalRegistry (VocalProfileRegistry)")
        self.vocals = VocalProfileRegistry()

        # === PHASE 8: StyleMatrix ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: StyleMatrix (PatchedStyleMatrix)")
        try:
            # (PatchedStyleMatrix - —ç—Ç–æ –Ω–∞—à StyleMatrix v11)
            self.style = PatchedStyleMatrix()
            log.info(
                "üé® [StyleMatrix] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (PatchedStyleMatrix)."
            )
        except ImportError as e:
            log.error(f"–ù–ï –£–î–ê–õ–û–°–¨ –∑–∞–≥—Ä—É–∑–∏—Ç—å PatchedStyleMatrix: {e}")
            self.style = None  # type: ignore

        # === PHASE 9: ToneEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: ToneEngine (ToneSyncEngine)")
        self.tone = ToneSyncEngine()
        
        # === PHASE 10: VocalAllocator ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: VocalAllocator (AdaptiveVocalAllocator)")
        self.vocal_allocator = AdaptiveVocalAllocator()
        
        # === PHASE 11: ColorEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: ColorEngine (ColorEngineAdapter)")
        self.color_engine = ColorEngineAdapter()
        
        # === PHASE 12: RDEEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: RDEEngine (RhythmDynamicsEmotionEngine)")
        self.rde_engine = RhythmDynamicsEmotionEngine()
        
        # === PHASE 13: GenreDatabase ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: GenreDatabase (GenreDatabaseLoader)")
        self.genre_database = None
        if GENRE_DATABASE_LOADER_AVAILABLE:
            try:
                self.genre_database = GenreDatabaseLoader()
                log.info("‚úÖ [Genre Database Loader] GenreDatabaseLoader loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Genre Database Loader] Failed to initialize: {e}")
                self.genre_database = None
            except Exception as e:
                log.error(f"[Genre Database Loader] Unexpected error during initialization: {e}", exc_info=True)
                self.genre_database = None

        # === PHASE 14: FusionEngine ===
        log.debug("–ó–∞–≥—Ä—É–∑–∫–∞: FusionEngine (FusionEngineV64, GenreRoutingEngineV64)")
        self.fusion_engine = None
        self.genre_routing_engine = None
        if FUSION_ENGINE_AVAILABLE:
            try:
                self.fusion_engine = FusionEngineV64()
                self.genre_routing_engine = GenreRoutingEngineV64()
                log.info("‚úÖ [Fusion Engine] FusionEngineV64 and GenreRoutingEngineV64 loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Fusion Engine] Failed to initialize: {e}")
                self.fusion_engine = None
                self.genre_routing_engine = None
            except Exception as e:
                log.error(f"[Fusion Engine] Unexpected error during initialization: {e}", exc_info=True)
                self.fusion_engine = None
                self.genre_routing_engine = None

        # --- HYBRID GENRE ENGINE SUPPORT (Optional) ---
        self.hybrid_genre_engine = None
        if HYBRID_GENRE_ENGINE_AVAILABLE:
            try:
                self.hybrid_genre_engine = HybridGenreEngine()
                log.info("‚úÖ [Hybrid Genre Engine] HybridGenreEngine loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Hybrid Genre Engine] Failed to initialize: {e}")
                self.hybrid_genre_engine = None
            except Exception as e:
                log.error(f"[Hybrid Genre Engine] Unexpected error during initialization: {e}", exc_info=True)
                self.hybrid_genre_engine = None

        # --- SUNO PROMPT ENGINE SUPPORT (Optional) ---
        self.suno_prompt_engine = None
        if SUNO_PROMPT_ENGINE_AVAILABLE:
            try:
                self.suno_prompt_engine = SunoPromptEngine()
                log.info("‚úÖ [Suno Prompt Engine] SunoPromptEngine loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Suno Prompt Engine] Failed to initialize: {e}")
                self.suno_prompt_engine = None
            except Exception as e:
                log.error(f"[Suno Prompt Engine] Unexpected error during initialization: {e}", exc_info=True)
                self.suno_prompt_engine = None

        # --- EMOTION-DRIVEN SUNO ADAPTER SUPPORT (Optional) ---
        self.emotion_suno_adapter_available = EMOTION_SUNO_ADAPTER_AVAILABLE
        if EMOTION_SUNO_ADAPTER_AVAILABLE:
            log.info("‚úÖ [Emotion Suno Adapter] EmotionDrivenSunoAdapter available")

        # --- SUNO ANNOTATION ENGINE SUPPORT (Optional) ---
        self.suno_annotation_engine = None
        if SUNO_ANNOTATION_ENGINE_AVAILABLE:
            try:
                self.suno_annotation_engine = SunoAnnotationEngine()
                log.info("‚úÖ [Suno Annotation Engine] SunoAnnotationEngine loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Suno Annotation Engine] Failed to initialize: {e}")
                self.suno_annotation_engine = None
            except Exception as e:
                log.error(f"[Suno Annotation Engine] Unexpected error during initialization: {e}", exc_info=True)
                self.suno_annotation_engine = None

        # --- DYNAMIC EMOTION ENGINE SUPPORT (Optional) ---
        self.dynamic_emotion_engine = None
        if DYNAMIC_EMOTION_ENGINE_AVAILABLE:
            try:
                self.dynamic_emotion_engine = DynamicEmotionEngine()
                log.info("‚úÖ [Dynamic Emotion Engine] DynamicEmotionEngine loaded")
            except (ImportError, AttributeError, TypeError) as e:
                log.warning(f"[Dynamic Emotion Engine] Failed to initialize: {e}")
                self.dynamic_emotion_engine = None
            except Exception as e:
                log.error(f"[Dynamic Emotion Engine] Unexpected error during initialization: {e}", exc_info=True)
                self.dynamic_emotion_engine = None

        # --- MATRIX ARCHITECTURE SUPPORT (Optional) ---
        self.matrix_enabled = False
        self.matrix_genre_engine = None
        self.matrix_instrument_engine = None
        self.matrix_serendipity = None
        self.matrix_breathing_engine = None
        
        try:
            from .engines.universal_matrix import UniversalMatrixGenreEngine
            from .engines.instrument_engine import InstrumentEngine
            from .engines.serendipity_engine import SerendipityEngine
            from .engines.rhythm_breathing import RhythmBreathingEngine
            
            self.matrix_genre_engine = UniversalMatrixGenreEngine()
            self.matrix_instrument_engine = InstrumentEngine()
            self.matrix_serendipity = SerendipityEngine()
            self.matrix_breathing_engine = RhythmBreathingEngine()
            self.matrix_enabled = True
            log.info("‚úÖ [Matrix Architecture] New engines loaded: UniversalMatrix, InstrumentEngine, SerendipityEngine, RhythmBreathingEngine")
        except ImportError as e:
            log.debug(f"[Matrix Architecture] Not available (fallback to legacy): {e}")
            self.matrix_enabled = False

        log.info(
            f"üîπ [StudioCore {STUDIOCORE_VERSION}] Monolith loaded (Section - Aware Duet Mode v2)."
            + (f" [Matrix: {'ENABLED' if self.matrix_enabled else 'LEGACY'}]" if hasattr(self, 'matrix_enabled') else "")
        )

    # -------------------------------------------------------
    # üé§ (v4) –ê–Ω–∞–ª–∏–∑ –≤–æ–∫–∞–ª–∞ –ø–æ —Å–µ–∫—Ü–∏—è–º
    # -------------------------------------------------------

    def _analyze_sections(
        self, text_blocks: List[str], ui_gender: str
    ) -> Dict[str, Any]:
        """
        v4: –ü—Ä–æ–≥–æ–Ω—è–µ—Ç –∫–∞–∂–¥—ã–π –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ —Ö–∏–Ω—Ç—ã,
        –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–∏–π –ø—Ä–æ—Ñ–∏–ª—å –≤–æ–∫–∞–ª–∞.
        """
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: _analyze_sections")

        section_profiles: List[Dict[str, Optional[str]]] = []
        final_gender = "auto"
        user_voice_hint = None  # –•–∏–Ω—Ç, –∑–∞–¥–∞–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º

        # 1. –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
        for block_text in text_blocks:
            # 1a. –ò—â–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É ("—è —à–µ–ª" / "—è –∂–¥–∞–ª–∞")
            g_gender = detect_gender_from_grammar(block_text)

            # 1b. –ò—â–µ–º —Ö–∏–Ω—Ç—ã ("(—à–µ–ø–æ—Ç–æ–º)", "(–º—É–∂—Å–∫–æ–π –≤–æ–∫–∞–ª)")
            v_hint = detect_voice_profile(block_text)

            if v_hint and not user_voice_hint:
                user_voice_hint = v_hint  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ö–∏–Ω—Ç

            section_profiles.append({"gender": g_gender, "hint": v_hint})
            # v5: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –æ—à–∏–±–∫–∞ f - string (—É–±—Ä–∞–Ω–æ —Ç—Ä–æ–µ—Ç–æ—á–∏–µ)
            log.debug(f"–ë–ª–æ–∫ [{block_text[:20]}...] -> –ü–æ–ª: {g_gender}, –•–∏–Ω—Ç: {v_hint}")

        # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–ª–∞ (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã)
        genders_found = {p["gender"] for p in section_profiles if p["gender"]}

        if ui_gender != "auto":
            final_gender = ui_gender  # 1. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç UI
            log.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª –∏–∑ UI")
        elif "male" in genders_found and "female" in genders_found:
            final_gender = "mixed"  # 2. –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –¥—É—ç—Ç
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'mixed' (M / F)")
        elif "male" in genders_found:
            final_gender = "male"  # 3. –¢–æ–ª—å–∫–æ –º—É–∂—Å–∫–æ–π
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'male'")
        elif "female" in genders_found:
            final_gender = "female"  # 4. –¢–æ–ª—å–∫–æ –∂–µ–Ω—Å–∫–∏–π
            log.debug("–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞ 'female'")
        # (else: –æ—Å—Ç–∞–µ—Ç—Å—è 'auto')

        log.debug(f"–ò—Ç–æ–≥ –ø–æ –≤–æ–∫–∞–ª—É (–≤—Å–µ –±–ª–æ–∫–∏): {genders_found}")
        return {
            "final_gender_preference": final_gender,
            "user_voice_hint": user_voice_hint,
            "section_profiles": section_profiles,
        }

    # -------------------------------------------------------
    # üéº (v6) –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π
    # -------------------------------------------------------

    def _build_semantic_layers(
        self, emo: Dict[str, float], tlp: Dict[str, float], bpm: int, style_key: str
    ) -> Dict[str, Any]:
        """v6: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 'energy', 'arrangement' –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'key'"""
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: _build_semantic_layers")

        love, pain, truth = tlp.get("love", 0), tlp.get("pain", 0), tlp.get("truth", 0)
        cf = tlp.get("conscious_frequency", 0)
        # avg_emo reserved for future use

        key = style_key  # –ë–µ—Ä–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–∑ style.py

        def get_focus(mood: str, energy: str) -> str:
            if energy == "high":
                return "climax"
            if energy == "low":
                return "minimal"
            if mood == "dramatic":
                return "contrast"
            if mood == "narrative":
                return "story_flow"
            return "flow"

        # (v6) –õ–æ–≥–∏–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —à–∞–±–ª–æ–Ω–∞—Ö Suno
        intro = {
            "tag": "Intro",
            "mood": "mystic" if cf >= 0.5 else "calm",
            "energy": "low",
            "arrangement": "minimal",
            "bpm": int(bpm * 0.8),
            "key": key,
        }
        verse = {
            "tag": "Verse",
            "mood": "narrative" if love >= truth else "reflective",
            "energy": "mid",
            "arrangement": "standard",
            "bpm": bpm,
            "key": key,
        }
        bridge = {
            "tag": "Bridge",
            "mood": "dramatic" if pain > 0.3 else "dreamlike",
            "energy": "mid - high",
            "arrangement": "building",
            "bpm": int(bpm * 1.05),
            "key": key,
        }
        chorus = {
            "tag": "Chorus",
            "mood": "uplifting" if love >= pain else "tense",
            "energy": "high",
            "arrangement": "full arrangement",
            "bpm": int(bpm * 1.1),
            "key": key,
        }
        outro = {
            "tag": "Outro",
            "mood": "peaceful" if cf > 0.6 else "fading",
            "energy": "low",
            "arrangement": "minimal",
            "bpm": int(bpm * 0.7),
            "key": key,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º 'focus' –Ω–∞ –æ—Å–Ω–æ–≤–µ mood / energy
        for s in [intro, verse, bridge, chorus, outro]:
            s["focus"] = get_focus(s["mood"], s["energy"])

        # v6: BPM —Ç–µ–ø–µ—Ä—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç TLP (–±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç)
        bpm_adj = int(bpm + (love * 10) - (pain * 15) + (truth * 5))
        bpm_adj = max(60, min(180, bpm_adj))
        log.debug(f"BPM —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω –¥–æ {bpm_adj}")

        return {
            "bpm_suggested": bpm_adj,
            "layers": {
                "depth": round((truth + pain) / 2, 2),
                "warmth": round(love, 2),
                "clarity": round(cf, 2),
                "sections": [intro, verse, bridge, chorus, outro],
            },
        }

    # -------------------------------------------------------
    # ‚úçÔ∏è (v6) –ê–Ω–Ω–æ—Ç–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
    # -------------------------------------------------------

    def annotate_text(
        self,
        text_blocks: List[str],
        section_profiles: List[Dict[str, Optional[str]]],
        semantic_sections: List[Dict[str, Any]],
    ) -> Tuple[str, str]:
        """
        v6: –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–ø–∏—Å–∞–Ω. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 2 –≤–µ—Ä—Å–∏–∏:
        1.  `annotated_text_ui`: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π (–¥–ª—è Gradio)
        2.  `annotated_text_suno`: –ß–∏—Å—Ç—ã–π (–¥–ª—è Suno Lyrics)
        """
        log.debug("–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏: annotate_text (v6)")

        ui_blocks = []
        suno_blocks = []

        num_blocks = len(text_blocks)

        # --- (v6) –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –º—ç–ø–ø–∏–Ω–≥–∞ —Å–µ–∫—Ü–∏–π ---
        # intro, verse1, chorus1, verse2, chorus2, bridge, chorus3, outro
        semantic_map = []
        if num_blocks <= 5:
            semantic_map = ["Intro", "Verse", "Bridge", "Chorus", "Outro"]
        elif num_blocks == 6:
            semantic_map = ["Intro", "Verse", "Chorus", "Verse", "Chorus", "Outro"]
        elif num_blocks == 7:
            semantic_map = [
                "Intro",
                "Verse",
                "Pre - Chorus",
                "Chorus",
                "Verse",
                "Bridge",
                "Outro",
            ]
        else:  # 8+
            semantic_map = [
                "Intro",
                "Verse",
                "Pre - Chorus",
                "Chorus",
                "Verse",
                "Bridge",
                "Chorus",
                "Outro",
            ]

        # –î–æ–ø–æ–ª–Ω—è–µ–º –∫–∞—Ä—Ç—É, –µ—Å–ª–∏ –±–ª–æ–∫–æ–≤ –±–æ–ª—å—à–µ 8
        if num_blocks > len(semantic_map):
            semantic_map.extend(["Verse", "Chorus"] * (num_blocks - len(semantic_map)))

        # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–∫—Ü–∏–π
        sec_defs = {s["tag"].lower(): s for s in semantic_sections}
        # –ó–∞–ø–∞—Å–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        sec_defs.setdefault("pre - chorus", sec_defs["bridge"])
        sec_defs.setdefault("verse 2", sec_defs["verse"])
        # --- –ö–æ–Ω–µ—Ü –ª–æ–≥–∏–∫–∏ –º—ç–ø–ø–∏–Ω–≥–∞ ---

        # FIX: Use the calculated Chorus BPM as the final tag BPM for
        # consistency (semantic_sections[3] = Chorus)
        final_bpm = semantic_sections[3].get("bpm", 120)  # (BPM –ø—Ä–∏–ø–µ–≤–∞)
        final_key = semantic_sections[3].get("key", "auto")
        # final_vocal_form reserved for future use

        for i, block_text in enumerate(text_blocks):
            if not block_text.strip():
                continue

            # 1. –ë–µ—Ä–µ–º —Å–µ–º–∞–Ω—Ç–∏–∫—É (Intro, Verse...)
            tag_name = semantic_map[i].lower()
            # Fallback –Ω–∞ 'verse'
            sem = sec_defs.get(tag_name, sec_defs["verse"])

            # 2. –ë–µ—Ä–µ–º –≤–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å (Male, Female...)
            profile = section_profiles[i]
            # MALE, FEMALE, MIXED, AUTO
            gender_tag = profile.get("gender", "auto").upper()

            # 3. –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏
            suno_tag_parts = [
                tag_name.upper(),
                f"vocal: {gender_tag}" if gender_tag != "AUTO" else None,
                f"mood: {sem['mood']}",
                f"energy: {sem['energy']}",
                f"arrangement: {sem['arrangement']}",
            ]
            suno_tag = f"[{' - '.join(filter(None, suno_tag_parts))}]"

            # UI - —Ç–µ–≥ (–±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π)
            ui_tag = f"[{tag_name.upper()} - {gender_tag} - {sem['mood']}, {sem['energy']}, {sem['arrangement']}, BPM‚âà{sem['bpm']}]"

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π BPM / Key (–±–µ—Ä–µ–º –∏–∑ –ø—Ä–∏–ø–µ–≤–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å)
            if "chorus" in tag_name:
                final_bpm = sem["bpm"]
                final_key = sem["key"]

            ui_blocks.append(ui_tag)
            ui_blocks.append(block_text)
            ui_blocks.append("")

            suno_blocks.append(suno_tag)
            suno_blocks.append(block_text)
            suno_blocks.append("")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–≥
        # (vocal_form –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ 'analyze')
        end_tag_ui = f"[End ‚Äì BPM‚âà{final_bpm}, Tone={final_key}]"
        end_tag_suno = f"[{final_bpm} BPM, {final_key}]"

        ui_blocks.append(end_tag_ui)
        suno_blocks.append(end_tag_suno)

        return "\n".join(ui_blocks).strip(), "\n".join(suno_blocks).strip()

    # -------------------------------------------------------
    # üöÄ –ì–ª–∞–≤–Ω—ã–π –ü–∞–π–ø–ª–∞–π–Ω
    # -------------------------------------------------------

    def _check_safety(self, text: str) -> str:
        """
        Task 1.1: Safety check method that validates input length and checks for aggression keywords.
        Returns the text (possibly replaced with neutral text if aggression detected).
        """
        # Input type validation
        if not text or not isinstance(text, str):
            raise ValueError("Text input is required and must be a string")
        
        # Length validation
        if len(text) > DEFAULT_CONFIG.MAX_INPUT_LENGTH:
            raise ValueError(
                f"Text length ({len(text)}) exceeds maximum allowed length ({DEFAULT_CONFIG.MAX_INPUT_LENGTH})"
            )
        
        # Aggression filter
        aggression_keywords = DEFAULT_CONFIG.AGGRESSION_KEYWORDS
        text_lower = text.lower()
        found_keywords = [kw for kw in aggression_keywords if kw.lower() in text_lower]
        if found_keywords:
            log.warning(
                f"Aggression keywords detected: {found_keywords}. Replacing with neutral text."
            )
            text = DEFAULT_CONFIG.FALLBACK_NEUTRAL_TEXT
        
        return text

    def _infer_gender_from_text(self, text: str) -> Optional[str]:
        """
        Infer gender from Russian text grammar (past tense verb endings).
        Returns 'male', 'female', or None if unclear.
        """
        # Russian past tense patterns: "–ª" (male) vs "–ª–∞" (female)
        male_patterns = [
            r'\b\w+–ª\b',  # Words ending with "–ª" (past tense, male)
            r'\b\w+–ª—Å—è\b',  # Reflexive verbs ending with "–ª—Å—è" (male)
        ]
        female_patterns = [
            r'\b\w+–ª–∞\b',  # Words ending with "–ª–∞" (past tense, female)
            r'\b\w+–ª–∞—Å—å\b',  # Reflexive verbs ending with "–ª–∞—Å—å" (female)
        ]
        
        male_matches = []
        female_matches = []
        for pattern in male_patterns:
            male_matches.extend(re.findall(pattern, text, re.IGNORECASE))
        for pattern in female_patterns:
            female_matches.extend(re.findall(pattern, text, re.IGNORECASE))
        
        male_count = len(male_matches)
        female_count = len(female_matches)
        
        if male_count > female_count and male_count > 0:
            return "male"
        elif female_count > male_count and female_count > 0:
            return "female"
        elif male_count == female_count and male_count > 0:
            # If equal, check which appears first in text
            first_male_pos = min((text.lower().find(m) for m in male_matches), default=len(text))
            first_female_pos = min((text.lower().find(m) for m in female_matches), default=len(text))
            if first_male_pos < first_female_pos:
                return "male"
            elif first_female_pos < first_male_pos:
                return "female"
        return None

    def _infer_texture_from_mood(self, mood: str, emotions: Dict[str, float]) -> str:
        """
        Infer vocal texture based on mood and emotions.
        """
        mood_lower = (mood or "").lower()
        emotion_keys = [k.lower() for k in emotions.keys()] if emotions else []
        
        # Reflective/introspective moods -> breathy/intimate
        if any(word in mood_lower for word in ["reflective", "introspective", "melancholic", "nostalgic", "peaceful"]):
            return "breathy/intimate"
        if any(word in emotion_keys for word in ["peace", "nostalgia", "melancholy"]):
            return "breathy/intimate"
        
        # Uplifting/energetic moods -> resonant
        if any(word in mood_lower for word in ["uplifting", "energetic", "joyful", "triumphant"]):
            return "resonant"
        if any(word in emotion_keys for word in ["joy", "triumph", "energy"]):
            return "resonant"
        
        # Default
        return "dynamic"

    def _generate_color_signature_from_emotions(self, emotions: Dict[str, float]) -> str:
        """
        Generate color signature from primary emotion.
        """
        if not emotions:
            return "neutral"
        
        dominant = max(emotions, key=emotions.get)
        
        # Emotion to color mapping
        emotion_color_map = {
            "nostalgia": "sepia/orange",
            "pain": "grey/blue",
            "joy": "yellow/gold",
            "peace": "green/teal",
            "love": "pink/rose",
            "melancholy": "blue/grey",
            "triumph": "red/orange",
            "anger": "red/dark",
            "fear": "purple/dark",
            "sadness": "blue/indigo",
        }
        
        return emotion_color_map.get(dominant.lower(), "neutral")

    def _generate_resonance_hz_from_key(self, key: str) -> float:
        """
        Generate mock resonance_hz based on key.
        """
        # Base frequencies for common keys (approximate)
        key_freq_map = {
            "c": 130.81,
            "c#": 138.59,
            "d": 146.83,
            "d#": 155.56,
            "e": 164.81,
            "f": 174.61,
            "f#": 185.00,
            "g": 196.00,
            "g#": 207.65,
            "a": 220.00,
            "a#": 233.08,
            "b": 246.94,
        }
        
        # Extract key root (first letter, case-insensitive)
        key_lower = (key or "").lower().strip()
        key_root = key_lower.split()[0] if key_lower else "c"
        
        # Remove # and b (sharp/flat) for matching
        key_root = key_root.replace("#", "").replace("b", "")
        
        base_freq = key_freq_map.get(key_root, 130.81)
        
        # Minor keys typically ~10Hz lower
        if "minor" in key_lower:
            base_freq -= 10.0
        
        return round(base_freq, 2)

    def _generate_breathing_map_from_punctuation(self, text: str) -> Dict[str, Any]:
        """
        Generate simple breathing map based on punctuation.
        Commas = short breath, periods = long breath.
        """
        breathing_points = []
        text_length = len(text)
        
        for i, char in enumerate(text):
            if char == ',':
                breathing_points.append({
                    "position": i,
                    "type": "short",
                    "duration_ms": 200,
                })
            elif char in '.!?':
                breathing_points.append({
                    "position": i,
                    "type": "long",
                    "duration_ms": 500,
                })
        
        return {
            "breathing_points": breathing_points,
            "total_points": len(breathing_points),
            "inhale_points": [p["position"] for p in breathing_points if p["type"] == "long"],
            "exhale_points": [p["position"] for p in breathing_points if p["type"] == "short"],
        }

    def _enhance_suno_annotations(
        self,
        annotated_text: str,
        emotions: Dict[str, float],
        vocal_result: Dict[str, Any],
        style: Dict[str, Any]
    ) -> str:
        """
        Enhance Suno annotations with advanced tags from SunoPromptEngine.
        Adds voice tags, emotion tags, and FX tags where appropriate.
        """
        if not self.suno_prompt_engine or not annotated_text:
            return annotated_text
        
        try:
            lines = annotated_text.split('\n')
            enhanced_lines = []
            
            # Get dominant emotion for voice tags
            dominant_emotion = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
            intensity = emotions.get(dominant_emotion, 1.0) if emotions and isinstance(emotions, dict) else 1.0
            
            # Map emotions to voice tags using detailed mapping
            try:
                from .vocal_techniques import get_vocal_for_emotion
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–æ–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —ç–º–æ—Ü–∏–∏
                vocal_techniques = get_vocal_for_emotion(dominant_emotion, intensity)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ –≤ —Ç–µ–≥–∏
                if vocal_techniques:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ç–µ—Ö–Ω–∏–∫—É –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–≥
                    primary_technique = vocal_techniques[0]
                    
                    # –ú–∞–ø–ø–∏–Ω–≥ —Ç–µ—Ö–Ω–∏–∫ –∫ —Ç–µ–≥–∞–º
                    technique_to_tag = {
                        "harsh": "[Aggressive]",
                        "scream": "[Gritty]",
                        "guttural": "[Gritty]",
                        "rasp": "[Gritty]",
                        "melancholy": "[Melancholic]",
                        "soft": "[Melancholic]",
                        "vibrato": "[Emotional]",
                        "emotional": "[Emotional]",
                        "ethereal": "[Angelic]",
                        "angelic": "[Angelic]",
                        "breathy": "[Soulful]",
                        "warm": "[Soulful]",
                        "belting": "[Powerful]",
                        "powerful": "[Powerful]",
                        "dramatic": "[Dramatic]",
                    }
                    
                    # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–µ–≥
                    voice_tag = ""
                    for tech_key, tag in technique_to_tag.items():
                        if tech_key in primary_technique.lower():
                            voice_tag = tag
                            break
                    
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π —Ç–µ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–∏
                    if not voice_tag:
                        emotion_to_voice_fallback = {
                            "anger": "[Aggressive]",
                            "rage": "[Gritty]",
                            "sadness": "[Melancholic]",
                            "joy": "[Emotional]",
                            "peace": "[Angelic]",
                            "love": "[Soulful]",
                            "fear": "[Tense]",
                            "awe": "[Epic]",
                            "epic": "[Epic]",
                        }
                        voice_tag = emotion_to_voice_fallback.get(dominant_emotion, "")
            except (ImportError, AttributeError, Exception) as e:
                log.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è —Ç–µ–≥–æ–≤: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥")
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–∞–ø–ø–∏–Ω–≥
            emotion_to_voice = {
                "anger": "[Aggressive]",
                "rage": "[Gritty]",
                "sadness": "[Melancholic]",
                "joy": "[Emotional]",
                "peace": "[Angelic]",
                "love": "[Soulful]",
            }
            voice_tag = emotion_to_voice.get(dominant_emotion, "")
            
            # Add voice tag at the beginning if not present
            if voice_tag and voice_tag not in annotated_text:
                enhanced_lines.append(voice_tag)
            
            # Process each line and enhance sections using construct_section
            current_section_type = None
            for line in lines:
                # Check if line is a section tag
                if line.strip().startswith('[') and line.strip().endswith(']'):
                    section_tag = line.strip()[1:-1].upper()
                    # Try to extract section type (Intro, Verse, Chorus, etc.)
                    for section_type in ["INTRO", "VERSE", "CHORUS", "BRIDGE", "OUTRO", "PRE-CHORUS"]:
                        if section_type in section_tag:
                            current_section_type = section_type
                            # Use construct_section with modifiers based on emotions
                            modifiers = []
                            if dominant_emotion in ["anger", "rage"]:
                                modifiers.append("Aggressive")
                            elif dominant_emotion in ["sadness", "melancholy"]:
                                modifiers.append("Melancholic")
                            elif dominant_emotion in ["joy", "love"]:
                                modifiers.append("Emotional")
                            
                            # Use construct_section for better structure
                            if modifiers:
                                enhanced_section = self.suno_prompt_engine.construct_section(
                                    current_section_type, modifiers=modifiers, lyrics=""
                                )
                                enhanced_lines.append(enhanced_section.strip())
                            else:
                                enhanced_lines.append(line)
                            break
                    else:
                        enhanced_lines.append(line)
                else:
                    enhanced_lines.append(line)
                
                # Add FX tags for dramatic pauses using experimental_stack
                if line.strip() and not line.strip().startswith('['):
                    # Check if line ends with punctuation that suggests pause
                    if line.strip().endswith(('.', '!', '?')):
                        # Add pause tag occasionally for dramatic effect
                        if len(enhanced_lines) % 3 == 0:  # Every 3rd section
                            pause_tag = self.suno_prompt_engine.experimental_stack("Pause", "Dramatic")
                            enhanced_lines.append(pause_tag)
            
            return '\n'.join(enhanced_lines)
        except Exception as e:
            log.warning(f"[Suno Prompt Engine] Error enhancing annotations: {e}")
            return annotated_text

    def _enrich_result_with_smart_defaults(
        self, result: Dict[str, Any], text: str, preferred_gender: str
    ) -> Dict[str, Any]:
        """
        Enrich result dictionary with smart defaults for missing fields.
        """
        # 1. Vocal Inference
        vocal = result.get("vocal", {})
        if isinstance(vocal, dict):
            # Infer gender if auto and not set
            if preferred_gender == "auto" and vocal.get("gender") == "auto":
                inferred_gender = self._infer_gender_from_text(text)
                if inferred_gender:
                    vocal["gender"] = inferred_gender
                    log.debug(f"Inferred gender from text: {inferred_gender}")
            
            # Set texture based on mood if missing
            if not vocal.get("texture"):
                style = result.get("style", {})
                mood = style.get("mood") or style.get("atmosphere") or "neutral"
                emotions = result.get("emotions", {})
                texture = self._infer_texture_from_mood(mood, emotions)
                vocal["texture"] = texture
                log.debug(f"Inferred texture from mood: {texture}")
            
            result["vocal"] = vocal
        
        # 2. Color & Resonance
        style = result.get("style", {})
        if isinstance(style, dict):
            # Add color_signature if missing
            if not style.get("color_signature"):
                emotions = result.get("emotions", {})
                color_sig = self._generate_color_signature_from_emotions(emotions)
                style["color_signature"] = color_sig
                log.debug(f"Generated color_signature: {color_sig}")
            
            result["style"] = style
        
        # Add resonance_hz to RDE if missing
        rde = result.get("rde", {})
        if isinstance(rde, dict) and not rde.get("resonance_hz"):
            key = result.get("key", "C major")
            resonance_hz = self._generate_resonance_hz_from_key(key)
            rde["resonance_hz"] = resonance_hz
            log.debug(f"Generated resonance_hz from key: {resonance_hz}")
            result["rde"] = rde
        
        # 3. ZeroPulse / Breathing
        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º breathing_map –∏–∑ result, –µ—Å–ª–∏ –æ–Ω —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        breathing_map = result.get("breathing", {})
        if not isinstance(breathing_map, dict):
            breathing_map = {}
        
        if not result.get("breathing") and not result.get("zeropulse"):
            # Use RhythmBreathingEngine if available (Matrix Architecture), otherwise fallback to legacy method
            if self.matrix_breathing_engine:
                try:
                    breathing_map = self.matrix_breathing_engine.create_map(text)
                    log.debug(f"[Matrix Breathing] Generated breathing map with {breathing_map.get('total_points', 0)} points")
                except (AttributeError, TypeError, ValueError) as e:
                    log.warning(f"[Matrix Breathing] Failed to use RhythmBreathingEngine, falling back to legacy: {e}")
                    breathing_map = self._generate_breathing_map_from_punctuation(text)
                except Exception as e:
                    log.error(f"[Matrix Breathing] Unexpected error: {e}", exc_info=True)
                    breathing_map = self._generate_breathing_map_from_punctuation(text)
            else:
                breathing_map = self._generate_breathing_map_from_punctuation(text)
            
            result["breathing"] = breathing_map
            # ZeroPulse is typically derived from breathing
            result["zeropulse"] = {
                "breathing_sync": breathing_map.get("total_points", 0) > 0,
                "points": breathing_map.get("breathing_points", []),
            }
            log.debug(f"Generated breathing map with {breathing_map.get('total_points', 0)} points")
        
        # 4. Genre - Ensure secondary is populated
        style = result.get("style", {})
        if isinstance(style, dict) and not style.get("secondary"):
            genre = style.get("genre", "")
            # If genre contains "hybrid", try to extract secondary from it
            if genre and "hybrid" in str(genre).lower():
                # Split hybrid genre (e.g., "folk rock hybrid" -> ["folk", "rock"])
                genre_parts = str(genre).lower().replace(" hybrid", "").split()
                if len(genre_parts) >= 2:
                    # Use the second part as secondary
                    style["secondary"] = genre_parts[1]
                    log.debug(f"Extracted secondary genre from hybrid: {style['secondary']}")
            # If genre_source indicates hybrid_genre_engine was used, we can infer secondary
            elif style.get("genre_source") == "hybrid_genre_engine" and genre:
                # Try to extract from genre name if it has multiple words
                genre_words = str(genre).split()
                if len(genre_words) >= 2:
                    style["secondary"] = genre_words[1]
                    log.debug(f"Inferred secondary genre: {style['secondary']}")
            
            result["style"] = style
        
        return result

    def _apply_quantum_jitter(self, value: float, intensity: float = 0.08) -> float:
        """
        Adds random variation to break static analysis loops.
        This introduces 'Creative Noise' to prevent deterministic results.
        """
        jitter = random.uniform(-intensity, intensity)
        return max(0.0, min(1.0, value + jitter))

    def analyze(
        self,
        text: str,
        preferred_gender: str = "auto",
        version: Optional[str] = None,
        semantic_hints: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ ACTIVATION_BLUEPRINT.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤ —Å—Ç—Ä–æ–≥–æ–º –ø–æ—Ä—è–¥–∫–µ —Ñ–∞–∑:
        - Phase 0: PREPARE (–≤–∞–ª–∏–¥–∞—Ü–∏—è, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
        - Phase 1: PARALLEL_BATCH_A (–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–æ–¥—É–ª–∏)
        - Phase 2: SEQUENTIAL_DEPENDENT (rhythm –ø–æ—Å–ª–µ emotions/tlp)
        - Phase 3: PARALLEL_BATCH_B (–∑–∞–≤–∏—Å–∏–º—ã–µ –º–æ–¥—É–ª–∏)
        - Phase 4: CORE_LOGIC (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
        - Phase 5: FUSION_AND_FINALIZE (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞)
        """
        # ============================================================
        # PHASE 0: PREPARE
        # ============================================================
        log.debug(f"--- –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê (v{STUDIOCORE_VERSION}) ---")
        log.debug(f"Preferred Gender: {preferred_gender}, Text: {text[:40]}...")
        
        # Task 10.2: Start timer for runtime metrics
        start_time = time.time()
        
        # 0.1: validate_input_length
        import sys
        import os
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))
        from security_patches import validate_text_input
        
        try:
            text = validate_text_input(text)
        except ValueError as e:
            log.error(f"[Security] Invalid text input: {e}")
            return {
                "ok": False,
                "error": str(e),
                "result": {}
            }
        
        # 0.2: check_safety
        text = self._check_safety(text)

        # 0.3: normalize_text
        raw = normalize_text_preserve_symbols(text)
        
        # 0.4: extract_blocks
        text_blocks = extract_raw_blocks(raw)

        # Section Analysis (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ extract_blocks)
        section_result = self._analyze_sections(text_blocks, preferred_gender)
        section_profiles = section_result.get("section_profiles", [])
        voice_hint = section_result.get("user_voice_hint")

        # ============================================================
        # PHASE 1: PARALLEL_BATCH_A
        # –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –¥–≤–∏–∂–∫–∏. –ó–∞–ø—É—Å–∫–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
        # ============================================================
        from .parallel_module_executor import ParallelModuleExecutor
        from .result_deduplicator import ResultDeduplicator
        
        executor = ParallelModuleExecutor(max_workers=8, timeout=30.0)
        deduplicator = ResultDeduplicator(similarity_threshold=0.85)
        
        log.debug("[Phase 1] –ó–∞–ø—É—Å–∫ PARALLEL_BATCH_A: emotion, tone, tlp, rde_resonance, rde_fracture, rde_entropy")
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–æ–¥—É–ª–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        parallel_batch_a_modules = []
        parallel_batch_a_modules.append(("emotion", self.emotion.analyze, (raw,), {}))
        parallel_batch_a_modules.append(("tone", self.tone.detect_key, (raw,), {}))
        parallel_batch_a_modules.append(("tlp", self.tlp.analyze, (raw,), {}))
        parallel_batch_a_modules.append(("rde_resonance", self.rde_engine.calc_resonance, (raw,), {}))
        parallel_batch_a_modules.append(("rde_fracture", self.rde_engine.calc_fracture, (raw,), {}))
        parallel_batch_a_modules.append(("rde_entropy", self.rde_engine.calc_entropy, (raw,), {}))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        batch_a_results = executor.execute_independent_modules(parallel_batch_a_modules)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        emotions = batch_a_results.get("emotion", {"neutral": 1.0})
        tone_hint = batch_a_results.get("tone", None)
        tlp = batch_a_results.get("tlp", {})
        
        # –°–æ–±–∏—Ä–∞–µ–º RDE —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        rde_result = {
            "resonance": batch_a_results.get("rde_resonance", 0.5),
            "fracture": batch_a_results.get("rde_fracture", 0.5),
            "entropy": batch_a_results.get("rde_entropy", 0.5),
        }
        
        log.debug(f"[Phase 1] PARALLEL_BATCH_A –∑–∞–≤–µ—Ä—à–µ–Ω: emotions={bool(emotions)}, tlp={bool(tlp)}, rde={bool(rde_result)}")
        
        # ============================================================
        # PHASE 2: SEQUENTIAL_DEPENDENT
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å. –ñ–¥–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Batch A.
        # Rhythm –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Emotion –∏ TLP.
        # ============================================================
        log.debug("[Phase 2] –ó–∞–ø—É—Å–∫ SEQUENTIAL_DEPENDENT: rhythm (—Ç—Ä–µ–±—É–µ—Ç emotions –∏ tlp)")
        
        cf = tlp.get("conscious_frequency") if tlp else None
        
        # Rhythm —Ç—Ä–µ–±—É–µ—Ç emotions –∏ tlp –∏–∑ Phase 1
        rhythm_analysis = self.rhythm.analyze(
            raw,
            emotions=emotions,
            tlp=tlp,
            cf=cf
        )
        
        bpm = int(round(rhythm_analysis.get("global_bpm", DEFAULT_CONFIG.FALLBACK_BPM)))
        log.debug(f"[Phase 2] SEQUENTIAL_DEPENDENT –∑–∞–≤–µ—Ä—à–µ–Ω: bpm={bpm}")
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ key –∏–∑ tone_hint (–Ω—É–∂–Ω–æ –¥–ª—è Phase 3)
        if tone_hint and isinstance(tone_hint, dict):
            key = tone_hint.get("key") or DEFAULT_CONFIG.FALLBACK_KEY
        else:
            key = DEFAULT_CONFIG.FALLBACK_KEY
        
        if not key or key == "auto":
            key = DEFAULT_CONFIG.FALLBACK_KEY
        
        # ============================================================
        # PHASE 3: PARALLEL_BATCH_B
        # –ó–∞–≤–∏—Å–∏–º—ã–µ –æ—Ç —Ä–∏—Ç–º–∞ –∏ —ç–º–æ—Ü–∏–π –¥–≤–∏–∂–∫–∏. –ó–∞–ø—É—Å–∫–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
        # ============================================================
        log.debug("[Phase 3] –ó–∞–ø—É—Å–∫ PARALLEL_BATCH_B: vocal, integrity, annotation, color, dynamic_emotion")
        
        # –°–æ–±–∏—Ä–∞–µ–º –º–æ–¥—É–ª–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        parallel_batch_b_modules = []
        
        # Vocal Allocator
        parallel_batch_b_modules.append(("vocal", self.vocal_allocator.analyze, (emotions, tlp, bpm, raw), {}))
        
        # Integrity Scan
        parallel_batch_b_modules.append(("integrity", self.integrity.analyze, (raw,), {"emotions": emotions, "tlp": tlp}))
        
        # Text Annotation: –£–î–ê–õ–ï–ù–û –∏–∑ Phase 3 - –±—É–¥–µ—Ç –≤—ã–∑–≤–∞–Ω –≤ Phase 4 –ø–æ—Å–ª–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è semantic_sections
        # –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –¥–≤–æ–π–Ω—É—é —Ä–∞–±–æ—Ç—É –∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        
        # Color Resolution
        intermediate_result = {"emotions": emotions, "tlp": tlp, "style": {}}
        parallel_batch_b_modules.append(("color", self.color_engine.resolve_color_wave, (intermediate_result,), {}))
        
        # Dynamic Emotion Engine
        if self.dynamic_emotion_engine:
            parallel_batch_b_modules.append(("dynamic_emotion", self.dynamic_emotion_engine.emotion_profile, (raw,), {}))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        batch_b_results = executor.execute_independent_modules(parallel_batch_b_modules)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        vocal_result = batch_b_results.get("vocal", {})
        if not isinstance(vocal_result, dict):
            vocal_result = {}
        
        # –û–±–æ–≥–∞—â–∞–µ–º vocal_result –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        if emotions and isinstance(emotions, dict) and len(emotions) > 0:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â—É—é —ç–º–æ—Ü–∏—é –¥–ª—è texture
            dominant_emotion = max(emotions, key=emotions.get)
            intensity = emotions[dominant_emotion]
            
            # –î–æ–±–∞–≤–ª—è–µ–º texture –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            if not vocal_result.get("texture"):
                mood = dominant_emotion
                texture = self._infer_texture_from_mood(mood, emotions)
                vocal_result["texture"] = texture
            
            # –î–æ–±–∞–≤–ª—è–µ–º section_techniques –µ—Å–ª–∏ –µ—Å—Ç—å semantic_sections
            # (–±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ –≤ Phase 4, –∫–æ–≥–¥–∞ semantic_sections –±—É–¥—É—Ç –≥–æ—Ç–æ–≤—ã)
        
        integrity_result = batch_b_results.get("integrity", {})
        # Annotation –±—É–¥–µ—Ç –≤—ã–∑–≤–∞–Ω –≤ Phase 4 –ø–æ—Å–ª–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è semantic_sections
        annotated_text_ui, annotated_text_suno = "", ""
        
        color_resolution = batch_b_results.get("color", None)
        emotion_profile_7axis = batch_b_results.get("dynamic_emotion", None)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ color_wave
        if color_resolution and hasattr(color_resolution, 'colors') and color_resolution.colors:
            color_wave = color_resolution.colors
        else:
            color_wave = ["#FFFFFF", "#B0BEC5"]
        
        log.debug(f"[Phase 3] PARALLEL_BATCH_B –∑–∞–≤–µ—Ä—à–µ–Ω: vocal={bool(vocal_result)}, integrity={bool(integrity_result)}, color={bool(color_wave)}")
        
        # ============================================================
        # PHASE 4: CORE_LOGIC
        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç–∏–ª—è
        # ============================================================
        log.debug("[Phase 4] –ó–∞–ø—É—Å–∫ CORE_LOGIC: filter_emotions, resolve_conflicts, determine_matrix_mode, hybrid_genre_refinement, build_semantic_layers")
        
        # 4.1: filter_emotions
        emotion_high_signal = DEFAULT_CONFIG.EMOTION_HIGH_SIGNAL
        if emotions:
            filtered_emotions = {
                k: v for k, v in emotions.items() 
                if v >= emotion_high_signal or k == max(emotions, key=emotions.get)
            }
            if len(filtered_emotions) < len(emotions) and len(filtered_emotions) > 0:
                total_filtered = sum(filtered_emotions.values())
                if total_filtered > 0:
                    emotions = {k: v / total_filtered for k, v in filtered_emotions.items()}
                    log.debug(f"[Phase 4.1] –≠–º–æ—Ü–∏–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã: {emotions}")
                else:
                    dominant = max(emotions, key=emotions.get)
                    emotions = {dominant: 1.0}
                    log.debug(f"[Phase 4.1] –û—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —ç–º–æ—Ü–∏—è: {dominant}")

        # 4.2: resolve_bpm_conflict
        consistency = ConsistencyLayerV8({"bpm": bpm, "tlp": tlp})
        suggested_bpm, was_resolved = consistency.resolve_bpm_tlp_conflict(bpm, tlp)
        if was_resolved:
            log.debug(f"[Phase 4.2] BPM-TLP –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω: {bpm} ‚Üí {suggested_bpm}")
            bpm = int(round(suggested_bpm))

        # 4.3: resolve_key_conflict
        # Key —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ tone_hint –≤ Phase 2, –Ω–æ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã
        resolver = GenreConflictResolver()
        suggested_key, was_key_resolved = resolver.resolve_color_key_conflict(key, color_wave, {})
        if was_key_resolved:
            log.debug(f"[Phase 4.3] Color-Key –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω: {key} ‚Üí {suggested_key}")
            key = suggested_key
        
        # 4.4: build_semantic_layers (–Ω—É–∂–Ω–æ –¥–ª—è annotation –∏ structure)
        semantic_layers = self._build_semantic_layers(emotions, tlp, bpm, key)
        semantic_sections = semantic_layers.get("layers", {}).get("sections", [])
        
        layout = DEFAULT_CONFIG.FALLBACK_STRUCTURE
        if semantic_sections and len(semantic_sections) > 0:
            layout = semantic_sections[0].get("tag", DEFAULT_CONFIG.FALLBACK_STRUCTURE)
        
        structure = {
            "sections": text_blocks,
            "section_count": len(text_blocks),
            "layout": layout,
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º annotation —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ semantic_sections
        if semantic_sections:
            annotation_result = self.annotate_text(text_blocks, section_profiles, semantic_sections)
        if isinstance(annotation_result, tuple) and len(annotation_result) == 2:
            annotated_text_ui, annotated_text_suno = annotation_result
        
        # 4.5: determine_matrix_mode_or_legacy
        # Task 1.3: Style.build() —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Matrix Architecture
        style = None
        # Variables for Matrix Mode data (for GUI)
        quantum_jitter_data = None
        serendipity_data = None
        fibonacci_data = None
        
        if self.matrix_enabled and self.matrix_genre_engine:
            # === MATRIX MODE: Use UniversalMatrixGenreEngine ===
            try:
                # Extract pain, energy, density from TLP and text analysis
                pain_score = tlp.get("pain", 0.0)
                # Calculate energy from text structure
                text_lines = [l for l in raw.split('\n') if l.strip()]
                avg_line_len = sum(len(l) for l in text_lines) / len(text_lines) if text_lines else 50
                energy_score = min(1.0, (avg_line_len / 60.0) + (bpm / 200.0))
                density_score = min(1.0, len(text_lines) / 10.0)
                
                # Apply Quantum Jitter (Creative Noise) to prevent deterministic results
                j_pain = self._apply_quantum_jitter(pain_score)
                j_energy = self._apply_quantum_jitter(energy_score)
                j_density = self._apply_quantum_jitter(density_score)
                
                # Save Quantum Jitter data for GUI
                quantum_jitter_data = {
                    "pain": {"original": pain_score, "jittered": j_pain},
                    "energy": {"original": energy_score, "jittered": j_energy},
                    "density": {"original": density_score, "jittered": j_density},
                }
                
                # Force Fibonacci Rotation (increment counter to rotate sunflower)
                fibonacci_counter = None
                if self.matrix_serendipity:
                    # Explicitly increment counter to ensure rotation on every run
                    self.matrix_serendipity.counter += 1
                    fibonacci_counter = self.matrix_serendipity.counter
                    # Also call fibonacci_select to maintain internal state
                    self.matrix_serendipity.fibonacci_select(["dummy"])
                    # Save Fibonacci Rotation data for GUI
                    fibonacci_data = {"counter": fibonacci_counter}
                
                # Resolve genre using Matrix Engine with jittered values
                matrix_genre, confidence = self.matrix_genre_engine.resolve_genre(
                    pain=j_pain, energy=j_energy, density=j_density
                )
                
                # Apply Serendipity (luck factor)
                serendipity_applied = False
                final_genre = matrix_genre
                if self.matrix_serendipity:
                    final_genre = self.matrix_serendipity.roll_for_serendipity(matrix_genre)
                    serendipity_applied = (final_genre != matrix_genre)
                
                # Save Serendipity data for GUI
                serendipity_data = {
                    "applied": serendipity_applied,
                    "original_genre": matrix_genre,
                    "final_genre": final_genre,
                }
                
                # Select instruments
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: EMOTIONS ‚Üí STYLE (–≤ Matrix Mode)
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ primary_emotion
                if emotions and isinstance(emotions, dict) and len(emotions) > 0:
                    try:
                        primary_emotion = max(emotions, key=emotions.get)
                    except (ValueError, TypeError):
                        primary_emotion = list(emotions.keys())[0] if emotions else "neutral"
                else:
                    primary_emotion = "neutral"
                    log.debug("[Chain Fix] emotions invalid in Matrix Mode, using neutral")
                
                instruments = self.matrix_instrument_engine.select_instruments(
                    genre_profile=final_genre,
                    energy=energy_score,
                    mood=primary_emotion
                ) if self.matrix_instrument_engine else []
                
                # Get top genres for GUI (alternative genres)
                top_genres_list = []
                try:
                    # Try to get alternative genres from matrix engine if available
                    if hasattr(self.matrix_genre_engine, 'get_top_genres'):
                        top_genres_list = self.matrix_genre_engine.get_top_genres(
                            pain=j_pain, energy=j_energy, density=j_density, top_n=5
                        )
                    else:
                        # Fallback: create list with current genre
                        top_genres_list = [(final_genre, confidence)]
                except Exception as e:
                    log.debug(f"[Matrix] Could not get top genres: {e}")
                    top_genres_list = [(final_genre, confidence)]
                
                # Build style result with Matrix data
                style = {
                    "genre": final_genre,
                    "style": final_genre,
                    "bpm": bpm,
                    "key": key,
                    "confidence": confidence,
                    "instruments": instruments,
                    "genre_source": "universal_matrix_fibonacci",
                    "matrix_mode": True,
                    "visual": DEFAULT_CONFIG.FALLBACK_VISUAL,
                    "narrative": DEFAULT_CONFIG.FALLBACK_NARRATIVE,
                    "structure": DEFAULT_CONFIG.FALLBACK_STRUCTURE,
                    "emotion": primary_emotion,
                    "top_genres": top_genres_list,  # Add top genres for GUI
                }
                log.debug("[Matrix] Genre resolved: %s (confidence: %.2f)", final_genre, confidence)
            except (AttributeError, TypeError, ValueError, KeyError) as e:
                log.warning("[Matrix] Error in Matrix mode, falling back to legacy: %s", e)
                # Fall through to legacy mode
                style = None
            except Exception as e:
                # Catch-all –¥–ª—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
                log.error("[Matrix] Unexpected error in Matrix mode: %s", e, exc_info=True)
                style = None
        
        # === LEGACY MODE: Use PatchedStyleMatrix (if Matrix didn't work) ===
        if style is None:
            if self.style:
                style_result = self.style.build(
                    emotions, tlp, raw, bpm, semantic_hints, voice_hint
                )
                style = style_result
                
                # üéØ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–û–ô –ë–ê–ó–´ –î–ê–ù–ù–´–•: –û–±–æ–≥–∞—â–∞–µ–º –∂–∞–Ω—Ä –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ GENRE_DATABASE_EXPANDED.json
                if style and self.genre_database:
                    try:
                        genre_from_style = style.get("genre", "")
                        if genre_from_style:
                            genre_key_normalized = genre_from_style.lower().replace(" ", "_").replace("-", "_")
                            expanded_genre_data = self.genre_database.get_genre(genre_key_normalized)
                            
                            if expanded_genre_data:
                                log.debug(f"[Genre Database] –ù–∞–π–¥–µ–Ω –∂–∞–Ω—Ä –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑–µ (Legacy Mode): {genre_key_normalized}")
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º BPM –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
                                db_bpm = self.genre_database.get_bpm(genre_key_normalized)
                                if db_bpm and isinstance(db_bpm, dict):
                                    style["bpm"] = db_bpm.get("default", style.get("bpm", bpm))
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º Key –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
                                db_keys = self.genre_database.get_key(genre_key_normalized)
                                if db_keys and isinstance(db_keys, list) and len(db_keys) > 0:
                                    style["key"] = db_keys[0]
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–∞ –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã
                                db_colors = self.genre_database.get_colors(genre_key_normalized)
                                if db_colors and isinstance(db_colors, list):
                                    style["genre_colors"] = db_colors
                                    style["color_wave"] = db_colors
                                
                                # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                                original_source = style.get("genre_source", "legacy")
                                style["genre_source"] = f"{original_source}_expanded_db"
                    except (AttributeError, TypeError, KeyError) as e:
                        log.warning(f"[Genre Database] –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∂–∞–Ω—Ä–∞ (Legacy Mode): {e}")
                    except Exception as e:
                        log.error(f"[Genre Database] Unexpected error enriching genre (Legacy Mode): {e}", exc_info=True)
            else:
                # Fallback –µ—Å–ª–∏ style engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                style = {
                    "genre": DEFAULT_CONFIG.FALLBACK_STYLE,
                    "style": DEFAULT_CONFIG.FALLBACK_STYLE,
                    "bpm": bpm,
                    "key": key,
                    "visual": DEFAULT_CONFIG.FALLBACK_VISUAL,
                    "narrative": DEFAULT_CONFIG.FALLBACK_NARRATIVE,
                    "structure": DEFAULT_CONFIG.FALLBACK_STRUCTURE,
                    "emotion": (
                        max(emotions, key=emotions.get) 
                        if emotions and isinstance(emotions, dict) and len(emotions) > 0
                        else DEFAULT_CONFIG.FALLBACK_EMOTION
                    ),
                }
                log.warning("Style engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è FALLBACK –∑–Ω–∞—á–µ–Ω–∏—è")

        # üõ°Ô∏è –ó–ê–©–ò–¢–ê: –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ style –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ None –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤ Fusion Engine
        if style is None:
            log.error("[CRITICAL] Style –æ—Å—Ç–∞–ª—Å—è None –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback")
            style = {
                "genre": DEFAULT_CONFIG.FALLBACK_STYLE,
                "style": DEFAULT_CONFIG.FALLBACK_STYLE,
                "bpm": bpm if isinstance(bpm, (int, float)) else DEFAULT_CONFIG.FALLBACK_BPM,
                "key": key if key else DEFAULT_CONFIG.FALLBACK_KEY,
                "visual": DEFAULT_CONFIG.FALLBACK_VISUAL,
                "narrative": DEFAULT_CONFIG.FALLBACK_NARRATIVE,
                "structure": DEFAULT_CONFIG.FALLBACK_STRUCTURE,
                "emotion": DEFAULT_CONFIG.FALLBACK_EMOTION,
            }
        
        # 4.6: hybrid_genre_refinement
        if self.hybrid_genre_engine and style:
            try:
                genre = style.get("genre")
                if genre and genre not in ("auto", "unknown", ""):
                    context = {
                        "emotions": emotions or {},
                        "tlp": tlp or {},
                        "bpm": bpm,
                        "key": key,
                    }
                    resolved_genre = self.hybrid_genre_engine.resolve(genre=genre, context=context)
                    if resolved_genre and isinstance(resolved_genre, str) and resolved_genre != genre:
                        style["genre"] = resolved_genre
                        original_source = style.get("genre_source", "unknown")
                        style["genre_source"] = f"{original_source}_hybrid_refined"
                        log.debug(f"[Phase 4.6] Hybrid genre refined: {genre} ‚Üí {resolved_genre}")
                        if "hybrid" in resolved_genre.lower():
                            genre_parts = resolved_genre.lower().replace(" hybrid", "").split()
                            if len(genre_parts) >= 2:
                                style["secondary_genre"] = genre_parts[1]
                                style["is_hybrid"] = True
            except (AttributeError, TypeError, ValueError) as e:
                log.warning(f"[Phase 4.6] Hybrid genre refinement failed: {e}")
            except Exception as e:
                log.error(f"[Phase 4.6] Unexpected error in hybrid genre refinement: {e}", exc_info=True)
        
        log.debug(f"[Phase 4] CORE_LOGIC –∑–∞–≤–µ—Ä—à–µ–Ω: style={bool(style)}, semantic_layers={bool(semantic_layers)}")

        # Semantic Layers, Vocal, Integrity, Annotation —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã—à–µ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ style_dependent_results
        
        # –û–±–æ–≥–∞—â–∞–µ–º vocal_result section_techniques –Ω–∞ –æ—Å–Ω–æ–≤–µ semantic_sections
        if semantic_sections and isinstance(semantic_sections, list) and len(semantic_sections) > 0:
            try:
                from .vocal_techniques import get_vocal_for_section
                
                section_techniques_list = []
                for section in semantic_sections:
                    if isinstance(section, dict):
                        section_emotion = section.get("emotion") or dominant_emotion if emotions else "neutral"
                        section_intensity = section.get("intensity") or (emotions.get(section_emotion, 0.5) if emotions and isinstance(emotions, dict) else 0.5)
                        section_name = section.get("tag") or section.get("name") or "Verse"
                        genre_name = style.get("genre") if style else None
                        
                        # –ü–æ–ª—É—á–∞–µ–º –≤–æ–∫–∞–ª—å–Ω—É—é —Ç–µ—Ö–Ω–∏–∫—É –¥–ª—è —Å–µ–∫—Ü–∏–∏
                        section_tech = get_vocal_for_section(
                            section_emotion=section_emotion,
                            section_intensity=section_intensity,
                            global_emotion=dominant_emotion if emotions else None,
                            genre=genre_name,
                            section_name=section_name
                        )
                        section_techniques_list.append(section_tech)
                
                if section_techniques_list:
                    vocal_result["section_techniques"] = section_techniques_list
                    log.debug(f"[Vocal] Added {len(section_techniques_list)} section techniques to vocal_result")
            except (ImportError, AttributeError, Exception) as e:
                log.debug(f"[Vocal] Could not add section techniques: {e}")
        
        # --- SUNO PROMPT ENGINE: Enhance annotations with advanced tags ---
        if self.suno_prompt_engine and annotated_text_suno:
            try:
                # Enhance annotated_text_suno with advanced Suno tags
                # This adds structure tags, voice tags, and FX tags where appropriate
                enhanced_suno = self._enhance_suno_annotations(
                    annotated_text_suno, 
                    emotions, 
                    vocal_result,
                    style
                )
                if enhanced_suno:
                    annotated_text_suno = enhanced_suno
                    log.debug("[Suno Prompt Engine] Annotations enhanced with advanced tags")
                
                # üìä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô: Also use get_style_prompt as alternative style description
                # –≠—Ç–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç SunoPromptEngine, –Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
                if style and style.get("genre"):
                    genre = style.get("genre", "")
                    
                    # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: EMOTIONS ‚Üí STYLE (–≤ SunoPromptEngine)
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ vibe –∏–∑ emotions
                    if emotions and isinstance(emotions, dict) and len(emotions) > 0:
                        try:
                            vibe_from_emotions = max(emotions, key=emotions.get)
                        except (ValueError, TypeError):
                            vibe_from_emotions = list(emotions.keys())[0] if emotions else "neutral"
                    else:
                        vibe_from_emotions = "neutral"
                    
                    vibe = style.get("mood") or style.get("atmosphere") or vibe_from_emotions
                    
                    # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: STYLE ‚Üí INSTRUMENTS
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ instruments –∏–∑ style
                    instruments_list = style.get("instruments", [])
                    if isinstance(instruments_list, list) and len(instruments_list) > 0:
                        instruments_str = instruments_list
                    else:
                        instruments_str = []
                        log.debug("[Chain Fix] instruments invalid in style, using empty list")
                    
                    style_prompt_alt = self.suno_prompt_engine.get_style_prompt(genre, vibe, instruments_str)
                    style["suno_style_prompt_alt"] = style_prompt_alt
                    log.debug(f"[Suno Prompt Engine] Alternative style prompt: {style_prompt_alt}")
            except (AttributeError, TypeError, KeyError) as e:
                log.warning(f"[Suno Prompt Engine] Failed to enhance annotations: {e}")
            except Exception as e:
                log.error(f"[Suno Prompt Engine] Unexpected error enhancing annotations: {e}", exc_info=True)

        # --- DYNAMIC EMOTION ENGINE: Get normalized emotion profile ---
        # ‚úÖ –£–ñ–ï –í–´–ü–û–õ–ù–ï–ù–û –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û –í–´–®–ï (—Å—Ç—Ä–æ–∫–∞ 1293, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ emotion_profile_7axis)
        # emotion_profile_7axis —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ style_dependent_results (—Å—Ç—Ä–æ–∫–∞ 1308)
        if emotion_profile_7axis:
            log.debug(f"[Dynamic Emotion Engine] 7-axis profile already generated (parallel)")

        # --- EMOTION-DRIVEN SUNO ADAPTER: Build emotion-based annotations ---
        emotion_driven_annotations = None
        if self.emotion_suno_adapter_available and structure:
            try:
                # Prepare emotion curve from emotions and TLP
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: EMOTIONS ‚Üí STYLE (–≤ EmotionDrivenSunoAdapter)
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ dominant_cluster –∏–∑ emotions
                if emotions and isinstance(emotions, dict) and len(emotions) > 0:
                    try:
                        dominant_cluster = max(emotions, key=emotions.get)
                    except (ValueError, TypeError):
                        dominant_cluster = list(emotions.keys())[0] if emotions else "narrative"
                else:
                    dominant_cluster = "narrative"
                    log.debug("[Chain Fix] emotions invalid in EmotionDrivenSunoAdapter, using narrative")
                
                emotion_curve = {
                    "dominant_cluster": dominant_cluster,
                    "global_tlp": tlp or {},
                }
                
                # Prepare sections from structure
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: STRUCTURE ‚Üí SECTIONS
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ sections –∏–∑ structure
                sections_data = []
                if structure and isinstance(structure, dict):
                    structure_sections = structure.get("sections", [])
                    if isinstance(structure_sections, list) and len(structure_sections) > 0:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º sections –∏–∑ structure
                        pass
                    else:
                        # Fallback –Ω–∞ text_blocks –µ—Å–ª–∏ sections –ø—É—Å—Ç–æ–π
                        structure_sections = text_blocks if text_blocks else []
                        log.debug("[Chain Fix] structure.sections invalid, using text_blocks fallback")
                else:
                    # Fallback –µ—Å–ª–∏ structure –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
                    structure_sections = text_blocks if text_blocks else []
                    log.debug("[Chain Fix] structure invalid, using text_blocks fallback")
                
                for idx, section_text in enumerate(structure_sections):
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ intensity –∏–∑ emotions
                    intensity = 0.5
                    if emotions and isinstance(emotions, dict):
                        intensity = emotions.get("joy", emotions.get("happiness", 0.5))
                    
                    sections_data.append({
                        "section": f"section_{idx+1}",
                        "name": f"Section {idx+1}",
                        "intensity": intensity,
                        "hot_phrases": [],
                    })
                
                # Build emotion-driven annotations
                emotion_driven_annotations = build_suno_annotations(
                    raw, sections_data, emotion_curve
                )
                log.debug(f"[Emotion Suno Adapter] Built annotations: {emotion_driven_annotations.get('style', 'N/A')}")
            except (AttributeError, TypeError, KeyError, ValueError) as e:
                log.warning(f"[Emotion Suno Adapter] Failed to build annotations: {e}")
            except Exception as e:
                log.error(f"[Emotion Suno Adapter] Unexpected error building annotations: {e}", exc_info=True)

        # --- SUNO ANNOTATION ENGINE: Build safe annotations ---
        suno_safe_annotations = None
        if self.suno_annotation_engine and structure:
            try:
                # Get section texts
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: STRUCTURE ‚Üí SECTIONS (–≤ SunoAnnotationEngine)
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ sections –∏–∑ structure
                if structure and isinstance(structure, dict):
                    section_texts = structure.get("sections", [])
                    if not isinstance(section_texts, list) or len(section_texts) == 0:
                        # Fallback –Ω–∞ text_blocks –µ—Å–ª–∏ sections –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
                        section_texts = text_blocks if text_blocks else []
                        log.debug("[Chain Fix] structure.sections invalid in SunoAnnotationEngine, using text_blocks")
                else:
                    # Fallback –µ—Å–ª–∏ structure –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
                    section_texts = text_blocks if text_blocks else []
                    log.debug("[Chain Fix] structure invalid in SunoAnnotationEngine, using text_blocks")
                
                # Prepare diagnostics for annotation engine
                diagnostics_for_annotations = {
                    "legacy": {
                        "style": style,
                        "bpm": bpm,
                    },
                    "out": {
                        "emotions": emotions,
                        "tlp": tlp,
                        "bpm": {"estimate": bpm} if isinstance(bpm, (int, float)) else bpm,
                        "tone": {"key": key} if key else {},
                        "vocal": vocal_result,
                    },
                }
                
                # Build safe annotations
                suno_safe_annotations = self.suno_annotation_engine.build_suno_safe_annotations(
                    section_texts, diagnostics_for_annotations
                )
                log.debug(f"[Suno Annotation Engine] Built {len(suno_safe_annotations)} safe annotations")
            except (AttributeError, TypeError, KeyError) as e:
                log.warning(f"[Suno Annotation Engine] Failed to build safe annotations: {e}")
            except Exception as e:
                log.error(f"[Suno Annotation Engine] Unexpected error building safe annotations: {e}", exc_info=True)

        # Color Resolution —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã—à–µ
        # –û–±–Ω–æ–≤–ª—è–µ–º intermediate_result —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º style –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        if style and color_resolution:
            try:
                # ‚úÖ –£–ñ–ï –í–´–ü–û–õ–ù–ï–ù–û –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û –í–´–®–ï (—Å—Ç—Ä–æ–∫–∞ 1289, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ color_resolution)
                # color_resolution —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ style_dependent_results (—Å—Ç—Ä–æ–∫–∞ 1307)
                # color_wave —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ color_resolution (—Å—Ç—Ä–æ–∫–∞ 1311-1314)
                # –ï—Å–ª–∏ style –∏–∑–º–µ–Ω–∏–ª—Å—è, –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å, –Ω–æ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if not color_wave or color_wave == ["#FFFFFF", "#B0BEC5"]:
                    # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ color_wave –Ω–µ –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º
                    intermediate_result = {
                        "emotions": emotions,
                        "tlp": tlp,
                        "style": style,
                    }
                    color_resolution = self.color_engine.resolve_color_wave(intermediate_result)
                    if color_resolution and hasattr(color_resolution, 'colors') and color_resolution.colors:
                        color_wave = color_resolution.colors
            except Exception as e:
                log.warning(f"[Color Engine] Error recalculating with style: {e}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                pass
        
        # Task 18.2: Auto-resolve Color-Key conflicts
        resolver = GenreConflictResolver()
        suggested_key, was_resolved = resolver.resolve_color_key_conflict(
            key, color_wave, style
        )
        if was_resolved:
            log.debug(f"Color-Key Konflikt aufgel√∂st: {key} ‚Üí {suggested_key}")
            key = suggested_key
            # Update style dict with new key
            style["key"] = key

        # ============================================================
        # ü•á –ò–ï–†–ê–†–•–ò–Ø –°–û–ó–î–ê–ù–ò–Ø SUNO –ü–†–û–ú–ü–¢–û–í (–ü–û–†–Ø–î–û–ö –í–´–ü–û–õ–ù–ï–ù–ò–Ø)
        # ============================================================
        # –ü–æ—Ä—è–¥–æ–∫ —Å–æ–∑–¥–∞–Ω–∏—è –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è!
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –ø–æ—Ç–æ–º –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç.
        #
        # 1. ü•à –í–´–°–û–ö–ò–ô: build_suno_prompt (Legacy Bridge) - —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º
        #    - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä —Å Matrix Architecture –¥–∞–Ω–Ω—ã–º–∏
        #    - –†–µ–∑—É–ª—å—Ç–∞—Ç: style["suno_ready_prompt"]
        # 2. üìä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô: SunoPromptEngine.get_style_prompt() - —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ _enhance_suno_annotations
        #    - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        #    - –†–µ–∑—É–ª—å—Ç–∞—Ç: style["suno_style_prompt_alt"]
        # 3. üó∫Ô∏è –ú–ê–ü–ü–ò–ù–ì: GenreRoutingEngine.SUNO_STYLE - —Å–æ–∑–¥–∞–µ—Ç—Å—è –≤ Fusion Engine —Å–µ–∫—Ü–∏–∏
        #    - Suno —Å—Ç–∏–ª—å –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞ –∂–∞–Ω—Ä–æ–≤
        #    - –†–µ–∑—É–ª—å—Ç–∞—Ç: style["suno_style_from_routing"]
        # 4. ü•á –ó–û–õ–û–¢–û–ô –°–¢–ê–ù–î–ê–†–¢: FusionEngine - —Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–º (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ)
        #    - –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (emotion, bpm, tonality, color, instrumentation, vocal)
        #    - –†–µ–∑—É–ª—å—Ç–∞—Ç: fusion_summary["suno_style_prompt"] –∏ fusion_summary["suno_lyrics_prompt"]
        #    - –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤: style["suno_style_prompt_fusion"] –∏ style["suno_lyrics_prompt_fusion"]
        # ============================================================
        
        # --- Legacy Bridge: Build Suno Prompt using legacy formatter ---
        # ü•à –í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢: –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä (—Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º)
        if LEGACY_SUNO_AVAILABLE and style:
            try:
                # Prepare data for the legacy formatter
                genre = style.get("genre", "Unknown")
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–ª—è instruments_list
                instruments_list = style.get("instruments", [])
                if isinstance(instruments_list, list) and len(instruments_list) > 0:
                    instruments_str = instruments_list
                else:
                    instruments_str = []
                    log.debug("[Type Check] instruments_list invalid, using empty list")
                
                # Get mood from emotions or style
                primary_mood = (
                    max(emotions, key=emotions.get) if emotions else "neutral"
                ) if isinstance(emotions, dict) and emotions else (
                    style.get("mood") or style.get("atmosphere") or "neutral"
                )
                
                # Get vocals from vocal_result
                # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–´–í–ê –¶–ï–ü–ò: VOCAL_RESULT ‚Üí VOCALS_LIST
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ vocals –∏–∑ vocal_result
                vocals_list = []
                if vocal_result and isinstance(vocal_result, dict) and len(vocal_result) > 0:
                    vocal_form = vocal_result.get("vocal_form", "solo")
                    gender = vocal_result.get("gender", "auto")
                    if gender and gender != "auto":
                        vocals_list.append(gender)
                    if vocal_form and vocal_form != "solo":
                        vocals_list.append(vocal_form)
                else:
                    log.debug("[Chain Fix] vocal_result invalid, using empty vocals_list")
                
                # Get BPM (ensure it's an int)
                bpm_val = bpm if isinstance(bpm, int) else (int(bpm) if isinstance(bpm, (float, str)) and str(bpm).isdigit() else 120)
                
                # Get key
                key_val = style.get("key") or key or "auto"
                
                # Prepare style_data dict for build_suno_prompt
                style_data_for_prompt = {
                    "genre": genre,
                    "style": style.get("style", genre),
                    "key": key_val,
                    "atmosphere": primary_mood,
                    "visual": style.get("visual", DEFAULT_CONFIG.FALLBACK_VISUAL),
                    "vocal_form": vocal_result.get("vocal_form", "solo") if isinstance(vocal_result, dict) else "solo",
                    "techniques": [],
                    # –î–æ–±–∞–≤–ª—è–µ–º emotions, tlp –∏ vocal_result –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è vocal
                    "emotions": emotions if emotions and isinstance(emotions, dict) else {},
                    "tlp": tlp if tlp and isinstance(tlp, dict) else {},
                    "vocal_result": vocal_result if isinstance(vocal_result, dict) else {},
                }
                
                # Call the legacy builder to get a professional Suno string
                suno_prompt_advanced = build_suno_prompt(
                    style_data=style_data_for_prompt,
                    vocals=vocals_list,
                    instruments=instruments_str,
                    bpm=bpm_val,
                    philosophy="Matrix Architecture + Legacy Bridge",
                    version=STUDIOCORE_VERSION,
                    prompt_variant="suno_style"
                )
                
                # Save into style result
                style["suno_ready_prompt"] = suno_prompt_advanced
                log.debug("[Legacy Bridge] Suno prompt generated successfully")
            except (AttributeError, TypeError, KeyError, ValueError) as e:
                log.warning(f"[Legacy Bridge] Prompt Builder failed: {e}")
            except Exception as e:
                log.error(f"[Legacy Bridge] Unexpected error in prompt builder: {e}", exc_info=True)
                # Fallback to simple format
                genre = style.get("genre", "Unknown")
                instruments_list = style.get("instruments", [])
                instruments_str = ", ".join(str(instr) for instr in instruments_list) if isinstance(instruments_list, list) and instruments_list else "None"
                primary_mood = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
                style["suno_ready_prompt"] = f"{genre} | {instruments_str} | {primary_mood} | {bpm} BPM | {key}"
        else:
            # Fallback if legacy adapter not available
            if style:
                genre = style.get("genre", "Unknown")
                instruments_list = style.get("instruments", [])
                instruments_str = ", ".join(str(instr) for instr in instruments_list) if isinstance(instruments_list, list) and instruments_list else "None"
                primary_mood = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
                style["suno_ready_prompt"] = f"{genre} | {instruments_str} | {primary_mood} | {bpm} BPM | {key}"
        
        # Ensure suno_ready_prompt is always set (final safety check)
        if style and "suno_ready_prompt" not in style:
            # Ultimate fallback
            genre = style.get("genre", "Unknown")
            instruments_str = "None"
            primary_mood = "neutral"
            style["suno_ready_prompt"] = f"{genre} | {instruments_str} | {primary_mood} | {bpm} BPM | {key}"

        # RDE Analysis —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã—à–µ (resonance, fracture, entropy)
        # –î–æ–±–∞–≤–ª—è–µ–º emotion_vector –µ—Å–ª–∏ TLP –¥–æ—Å—Ç—É–ø–µ–Ω
        if tlp and "emotion_vector" not in rde_result:
            try:
                rde_emotion_vector = self.rde_engine.export_emotion_vector(raw)
                rde_result["emotion_vector"] = {
                    "valence": rde_emotion_vector.valence,
                    "arousal": rde_emotion_vector.arousal,
                }
            except Exception as e:
                log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å RDE emotion vector: {e}")
        
        # Task 18.1: Auto-resolve Genre-RDE conflicts (part of Phase 4)
        genre = style.get("genre", "") if style else ""
        adjusted_rde, was_resolved = consistency.resolve_genre_rde_conflict(genre, rde_result)
        if was_resolved:
            log.debug(f"Genre-RDE Konflikt aufgel√∂st: {rde_result} ‚Üí {adjusted_rde}")
            rde_result = adjusted_rde

        # ============================================================
        # PHASE 5: FUSION_AND_FINALIZE
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞: suno_prompt_generation, fusion_engine_routing, deduplicate_results, assemble_final_json
        # ============================================================
        log.debug("[Phase 5] –ó–∞–ø—É—Å–∫ FUSION_AND_FINALIZE: suno_prompt_generation, fusion_engine_routing, deduplicate_results, assemble_final_json")
        
        # 5.1: suno_prompt_generation (Legacy Bridge)
        if LEGACY_SUNO_AVAILABLE and style:
            try:
                genre = style.get("genre", "Unknown")
                instruments_list = style.get("instruments", [])
                instruments_str = instruments_list if isinstance(instruments_list, list) and len(instruments_list) > 0 else []
                primary_mood = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
                vocals_list = []
                if vocal_result and isinstance(vocal_result, dict):
                    gender = vocal_result.get("gender", "auto")
                    if gender and gender != "auto":
                        vocals_list.append(gender)
                bpm_val = bpm if isinstance(bpm, int) else (int(bpm) if isinstance(bpm, (float, str)) and str(bpm).isdigit() else 120)
                key_val = style.get("key") or key or "auto"
                style_data_for_prompt = {
                    "genre": genre,
                    "style": style.get("style", genre),
                    "key": key_val,
                    "atmosphere": primary_mood,
                    "visual": style.get("visual", DEFAULT_CONFIG.FALLBACK_VISUAL),
                    "vocal_form": vocal_result.get("vocal_form", "solo") if isinstance(vocal_result, dict) else "solo",
                    "techniques": [],
                    # –î–æ–±–∞–≤–ª—è–µ–º emotions, tlp –∏ vocal_result –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è vocal
                    "emotions": emotions if emotions and isinstance(emotions, dict) else {},
                    "tlp": tlp if tlp and isinstance(tlp, dict) else {},
                    "vocal_result": vocal_result if isinstance(vocal_result, dict) else {},
                }
                suno_prompt_advanced = build_suno_prompt(
                    style_data=style_data_for_prompt,
                    vocals=vocals_list,
                    instruments=instruments_str,
                    bpm=bpm_val,
                    philosophy="Matrix Architecture + Legacy Bridge",
                    version=STUDIOCORE_VERSION,
                    prompt_variant="suno_style"
                )
                style["suno_ready_prompt"] = suno_prompt_advanced
                log.debug("[Phase 5.1] Suno prompt generated (Legacy Bridge)")
            except Exception as e:
                log.warning(f"[Phase 5.1] Suno prompt generation failed: {e}")
                if style:
                    genre = style.get("genre", "Unknown")
                    style["suno_ready_prompt"] = f"{genre} | {bpm} BPM | {key}"
        
        # 5.2: fusion_engine_routing
        fusion_summary = None
        if self.fusion_engine and self.genre_routing_engine:
            try:
                # Get dominant emotion for genre routing
                dominant_emotion = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
                
                # Get genre route from GenreRoutingEngineV64
                genre_route = self.genre_routing_engine.route(emotions or {}, dominant_emotion)
                log.debug(f"[Fusion Engine] Genre route: {genre_route}")
                
                # üó∫Ô∏è –ú–ê–ü–ü–ò–ù–ì: Also use SUNO_STYLE mapping directly to enhance style
                # –≠—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è style, –Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–π
                if genre_route.get("suno_style") and style:
                    style["suno_style_from_routing"] = genre_route["suno_style"]
                    log.debug(f"[Genre Routing] Suno style from routing: {genre_route['suno_style']}")
                
                # Prepare payload in format expected by FusionEngine
                # FusionEngine expects: legacy, emotion, bpm, tonality, color, instrumentation, vocal, tlp
                fusion_payload = {
                    "legacy": {
                        "style": style,
                        "bpm": bpm,
                        "instruments": style.get("instruments", []),
                        "vocals": vocal_result.get("vocals", []) if isinstance(vocal_result, dict) else [],
                        "vocal_form": vocal_result.get("vocal_form", "solo") if isinstance(vocal_result, dict) else "solo",
                        "tlp": tlp,
                    },
                    "emotion": {
                        "profile": emotions or {},
                        "dominant": dominant_emotion,
                    },
                    "bpm": {
                        "estimate": bpm if isinstance(bpm, (int, float)) else 120,
                        "target_bpm": bpm if isinstance(bpm, (int, float)) else 120,
                    },
                    "tonality": {
                        "section_keys": [key] if key else [],
                        "fallback_key": key or "C (C minor)",
                    },
                    "color": {
                        "profile": {
                            "primary_color": color_wave[0] if color_wave else "soft light",
                            "accent_color": color_wave[-1] if len(color_wave) > 1 else "shadows",
                        },
                        "wave": color_wave,
                    },
                    "instrumentation": {
                        "selection": {
                            "selected": style.get("instruments", []),
                        },
                        "palette": style.get("instruments", []),
                    },
                    "vocal": {
                        "tone": vocal_result.get("tone", "neutral") if isinstance(vocal_result, dict) else "neutral",
                        "style": vocal_result.get("style", "standard") if isinstance(vocal_result, dict) else "standard",
                        "gender": vocal_result.get("gender", "auto") if isinstance(vocal_result, dict) else "auto",
                    },
                    "tlp": tlp,
                }
                
                # Call FusionEngine.fuse()
                fusion_summary = self.fusion_engine.fuse(fusion_payload, genre_route=genre_route)
                log.debug(f"[Fusion Engine] Fusion summary generated: {fusion_summary.get('final_genre', 'N/A')}")
                
                # ü•á –ó–û–õ–û–¢–û–ô –°–¢–ê–ù–î–ê–†–¢: Update style with fusion results if available
                # Fusion Engine –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–æ–º–ø—Ç—ã
                if fusion_summary:
                    # Merge fusion results into style
                    if fusion_summary.get("final_genre"):
                        style["genre"] = fusion_summary["final_genre"]
                    if fusion_summary.get("final_subgenre"):
                        style["subgenre"] = fusion_summary["final_subgenre"]
                    if fusion_summary.get("mood"):
                        style["mood"] = fusion_summary["mood"]
                    if fusion_summary.get("suno_style_prompt"):
                        # Store fusion suno prompt (—ç—Ç–æ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç –±—É–¥–µ—Ç –≤ fusion_summary["suno_style_prompt"] –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ app.py —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º 1
                        style["suno_style_prompt_fusion"] = fusion_summary["suno_style_prompt"]
                    if fusion_summary.get("suno_lyrics_prompt"):
                        # Store fusion lyrics prompt (—ç—Ç–æ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è lyrics)
                        style["suno_lyrics_prompt_fusion"] = fusion_summary["suno_lyrics_prompt"]
                    
                    log.debug("[Phase 5.2] Fusion Engine: Style updated with fusion results")
            except Exception as e:
                log.warning(f"[Phase 5.2] Fusion Engine failed: {e}")
                fusion_summary = None

        # Task 10.2: Calculate runtime
        runtime_ms = int((time.time() - start_time) * 1000)

        # 5.3: deduplicate_results
        breathing_map = {}
        all_module_results: List[Tuple[str, Dict[str, Any]]] = []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
        all_module_results.append(("emotion_engine", {"emotions": emotions} if emotions else {}))
        all_module_results.append(("tone_engine", {"key": key, "tone_hint": tone_hint} if tone_hint else {"key": key}))
        all_module_results.append(("tlp_engine", {"tlp": tlp} if tlp else {}))
        all_module_results.append(("rhythm_engine", {"bpm": bpm, "rhythm_analysis": rhythm_analysis}))
        all_module_results.append(("rde_engine", {"rde": rde_result} if rde_result else {}))
        all_module_results.append(("semantic_layers", {"semantic_layers": semantic_layers} if semantic_layers else {}))
        all_module_results.append(("vocal_allocator", {"vocal": vocal_result} if vocal_result else {}))
        all_module_results.append(("integrity_engine", {"integrity": integrity_result} if integrity_result else {}))
        all_module_results.append(("text_annotation", {"annotated_text_ui": annotated_text_ui, "annotated_text_suno": annotated_text_suno}))
        all_module_results.append(("color_engine", {"color_wave": color_wave}))
        
        if style:
            style_source = "matrix_style" if style.get("matrix_mode") else "legacy_style"
            all_module_results.append((style_source, {"style": style}))
        
        if emotion_profile_7axis:
            all_module_results.append(("dynamic_emotion_engine", {"emotion_profile_7axis": emotion_profile_7axis}))
        
        if self.suno_prompt_engine and style and style.get("suno_style_prompt_alt"):
            all_module_results.append(("suno_prompt_engine", {"suno_style_prompt_alt": style.get("suno_style_prompt_alt")}))
        
        if emotion_driven_annotations:
            all_module_results.append(("emotion_suno_adapter", {"emotion_driven_annotations": emotion_driven_annotations}))
        
        if suno_safe_annotations:
            all_module_results.append(("suno_annotation_engine", {"suno_safe_annotations": suno_safe_annotations}))
        
        if fusion_summary:
            all_module_results.append(("fusion_engine", {"fusion": fusion_summary}))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
        deduplicated_result = deduplicator.deduplicate_results(all_module_results)
        log.debug(f"[Phase 5.3] –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(all_module_results)} –º–æ–¥—É–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")

        # 5.4: assemble_final_json
        log.debug("[Phase 5.4] –°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
        result = {
            "emotions": deduplicated_result.get("emotions", emotions if emotions and isinstance(emotions, dict) else {}),
            "tlp": deduplicated_result.get("tlp", tlp if tlp and isinstance(tlp, dict) else {}),
            "bpm": deduplicated_result.get("bpm", bpm if isinstance(bpm, (int, float)) else DEFAULT_CONFIG.FALLBACK_BPM),
            "key": deduplicated_result.get("key", key if key and isinstance(key, str) else DEFAULT_CONFIG.FALLBACK_KEY),
            "structure": structure if structure and isinstance(structure, dict) else {},
            "style": deduplicated_result.get("style", style if style and isinstance(style, dict) else {}),
            "vocal": deduplicated_result.get("vocal", vocal_result if vocal_result and isinstance(vocal_result, dict) else {}),
            "semantic_layers": deduplicated_result.get("semantic_layers", semantic_layers if semantic_layers and isinstance(semantic_layers, dict) else {}),
            "integrity": deduplicated_result.get("integrity", integrity_result if integrity_result and isinstance(integrity_result, dict) else {}),
            "annotated_text_ui": deduplicated_result.get("annotated_text_ui", annotated_text_ui if annotated_text_ui and isinstance(annotated_text_ui, str) else ""),
            "annotated_text_suno": deduplicated_result.get("annotated_text_suno", annotated_text_suno if annotated_text_suno and isinstance(annotated_text_suno, str) else ""),
            "color_wave": deduplicated_result.get("color_wave", color_wave if isinstance(color_wave, list) else ["#FFFFFF", "#B0BEC5"]),
            "rde": deduplicated_result.get("rde", rde_result if rde_result and isinstance(rde_result, dict) else {}),
            "breathing_map": breathing_map if isinstance(breathing_map, dict) else {},
            "section_profiles": section_profiles if isinstance(section_profiles, list) else [],
            # Task 10.2: Add runtime metrics for diagnostics
            "runtime_ms": runtime_ms if isinstance(runtime_ms, (int, float)) else 0,
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        if "_deduplication_metadata" in deduplicated_result:
            result["_deduplication_metadata"] = deduplicated_result["_deduplication_metadata"]
        
        # Add fusion summary if available (enhances final result)
        if fusion_summary and isinstance(fusion_summary, dict):
            result["fusion"] = fusion_summary
            # Also add fusion prompts to style for easy access (if not already set)
            if fusion_summary.get("suno_style_prompt") and not style.get("suno_style_prompt_fusion"):
                style["suno_style_prompt_fusion"] = fusion_summary["suno_style_prompt"]
            if fusion_summary.get("suno_lyrics_prompt") and not style.get("suno_lyrics_prompt_fusion"):
                style["suno_lyrics_prompt_fusion"] = fusion_summary["suno_lyrics_prompt"]
        
        # Add emotion-driven annotations if available (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤)
        if emotion_driven_annotations and isinstance(emotion_driven_annotations, dict):
            result["emotion_driven_annotations"] = emotion_driven_annotations
            # Also merge into style for easy access
            if emotion_driven_annotations.get("style"):
                style["emotion_driven_style"] = emotion_driven_annotations["style"]
            if emotion_driven_annotations.get("vocal_profile"):
                style["emotion_driven_vocal"] = emotion_driven_annotations["vocal_profile"]
            if emotion_driven_annotations.get("instrumentation"):
                style["emotion_driven_instruments"] = emotion_driven_annotations["instrumentation"]
        
        # Add safe annotations if available (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤)
        if suno_safe_annotations and isinstance(suno_safe_annotations, list):
            result["suno_safe_annotations"] = suno_safe_annotations
        
        # Add 7-axis emotion profile if available (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ç–∏–ø–æ–≤)
        if emotion_profile_7axis and isinstance(emotion_profile_7axis, dict):
            result["emotion_profile_7axis"] = emotion_profile_7axis
        
        # Add Genre Selection Process data (clusters, genre_scores)
        genre_selection_data = {}
        # Try to get clusters and genre_scores from EmotionEngine
        # Note: self.emotion is AutoEmotionalAnalyzer, not EmotionEngine
        # We need to create EmotionEngine instance to get clusters and genre_scores
        try:
            from .emotion import EmotionEngine
            
            # Create EmotionEngine instance for getting clusters and genre_scores
            emotion_engine = EmotionEngine()
            emotion_profile = emotion_engine.build_emotion_profile(raw)
            
            if isinstance(emotion_profile, dict):
                clusters = emotion_profile.get("clusters", {})
                genre_scores = emotion_profile.get("genre_scores", {})
                
                if clusters or genre_scores:
                    genre_selection_data["clusters"] = clusters
                    genre_selection_data["genre_scores"] = genre_scores
                    log.debug(f"[Genre Selection] Got clusters: {len(clusters)}, genre_scores: {len(genre_scores)}")
                
                # Add top_genres to style if not already set (for Legacy Mode)
                if genre_scores and isinstance(genre_scores, dict) and not style.get("top_genres"):
                    sorted_genres = sorted(genre_scores.items(), key=lambda x: x[1], reverse=True)[:5]
                    top_genres_list = [(genre, score) for genre, score in sorted_genres if score > 0]
                    if top_genres_list:
                        style["top_genres"] = top_genres_list
                        log.debug(f"[Genre Selection] Added top_genres to style: {len(top_genres_list)} genres")
        except (ImportError, AttributeError, Exception) as e:
            log.debug(f"[Genre Selection] Could not get emotion profile from EmotionEngine: {e}")
            # Fallback: try to compute clusters and genre_scores manually if possible
            try:
                # If we have emotions, we can try to compute clusters manually
                if emotions and isinstance(emotions, dict) and len(emotions) > 0:
                    # This is a simplified fallback - not as accurate as EmotionEngine
                    log.debug("[Genre Selection] Using fallback method for clusters/genre_scores")
            except Exception as e2:
                log.debug(f"[Genre Selection] Fallback also failed: {e2}")
        
        if genre_selection_data:
            result["genre_selection"] = genre_selection_data
            log.debug(f"[Genre Selection] Saved genre_selection data: {bool(genre_selection_data.get('clusters'))}, {bool(genre_selection_data.get('genre_scores'))}")
        else:
            log.debug("[Genre Selection] No genre_selection data available")
        
        # If emotion_profile_7axis is available, try to compute genre_bias
        if emotion_profile_7axis and isinstance(emotion_profile_7axis, dict):
            try:
                from .emotion_genre_matrix import compute_genre_bias
                genre_bias = compute_genre_bias(emotion_profile_7axis)
                if genre_bias:
                    result["genre_bias"] = genre_bias
            except Exception as e:
                log.debug(f"[Genre Bias] Could not compute genre bias: {e}")
        
        # Add Genre Routing data
        if self.genre_routing_engine and emotions:
            try:
                dominant_emotion = max(emotions, key=emotions.get) if emotions and isinstance(emotions, dict) else "neutral"
                genre_route = self.genre_routing_engine.route(emotions or {}, dominant_emotion)
                if genre_route and isinstance(genre_route, dict):
                    result["genre_routing"] = genre_route
            except Exception as e:
                log.debug(f"[Genre Routing] Could not get genre route: {e}")
        
        # Add Matrix Mode specific data (Quantum Jitter, Serendipity, Fibonacci)
        if quantum_jitter_data:
            result["quantum_jitter"] = quantum_jitter_data
        if serendipity_data:
            result["serendipity"] = serendipity_data
        if fibonacci_data:
            result["fibonacci_rotation"] = fibonacci_data
        
        # Add Matrix Architecture metadata to result
        if self.matrix_enabled:
            result["matrix_architecture"] = {
                "enabled": True,
                "engines": {
                    "genre": self.matrix_genre_engine is not None,
                    "instruments": self.matrix_instrument_engine is not None,
                    "serendipity": self.matrix_serendipity is not None,
                    "breathing": self.matrix_breathing_engine is not None,
                }
            }
        
        # Add integration metadata (shows what engines are active) - —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
        result["integrations"] = {
            "fusion_engine": bool(self.fusion_engine),
            "hybrid_genre_engine": bool(self.hybrid_genre_engine),
            "suno_prompt_engine": bool(self.suno_prompt_engine),
            "emotion_suno_adapter": bool(self.emotion_suno_adapter_available),
            "suno_annotation_engine": bool(self.suno_annotation_engine),
            "dynamic_emotion_engine": bool(self.dynamic_emotion_engine),
            "legacy_suno_bridge": bool(LEGACY_SUNO_AVAILABLE),
            "matrix_architecture": bool(self.matrix_enabled),
            "genre_database_loader": bool(getattr(self, 'genre_database', None)),
        }
        
        # Enrich result with smart defaults for missing fields
        result = self._enrich_result_with_smart_defaults(result, text, preferred_gender)
        
        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±–Ω–æ–≤–ª—è–µ–º breathing_map –≤ result –ø–æ—Å–ª–µ _enrich_result_with_smart_defaults
        # _enrich_result_with_smart_defaults —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç result["breathing"], 
        # –ø–æ—ç—Ç–æ–º—É –º—ã –æ–±–Ω–æ–≤–ª—è–µ–º result["breathing_map"] –∏–∑ result["breathing"]
        if isinstance(result, dict):
            breathing_data = result.get("breathing")
            if isinstance(breathing_data, dict):
                result["breathing_map"] = breathing_data
            elif not result.get("breathing_map"):
                # –ï—Å–ª–∏ breathing_map –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
                result["breathing_map"] = {}
        
        return result


class StudioCoreV5:
    """Compatibility wrapper exposing the v5 API expected by legacy tools."""

    def __init__(self, *args, **kwargs):
        self._core = StudioCore(*args, **kwargs)

    def analyze(self, *args, **kwargs):
        return self._core.analyze(*args, **kwargs)

    def emotion(self, text: str):
        return self._core.emotion.analyze(text)

    def style(
        self,
        emo: Dict[str, float],
        tlp: Dict[str, float],
        text: str,
        bpm: int,
        semantic_hints: Optional[Dict[str, Any]] = None,
        voice_hint: Optional[str] = None,
    ) -> Dict[str, Any]:
        if not self._core.style:
            raise RuntimeError("Style subsystem is unavailable.")
        return self._core.style.build(emo, tlp, text, bpm, semantic_hints, voice_hint)

    def tone(
        self,
        emo: Dict[str, float],
        tlp: Dict[str, float],
        key_hint: Optional[str] = None,
    ):
        return self._core.tone.colors_for_primary(emo, tlp, key_hint or "auto")

    def rhythm(
        self,
        text: str,
        *,
        emotions: Optional[Dict[str, float]] = None,
        tlp: Optional[Dict[str, float]] = None,
        cf: Optional[float] = None,
        header_bpm: Optional[float] = None,
    ):
        return self._core.rhythm.analyze(
            text,
            emotions=emotions,
            tlp=tlp,
            cf=cf,
            header_bpm=header_bpm,
        )

    def __getattr__(self, item: str):
        return getattr(self._core, item)


# ==========================================================
# Task 6.2: Version is now imported from config.py
log.info(
    f"üîπ [StudioCore {MONOLITH_VERSION}] Monolith loaded (Section - Aware Duet Mode v2)."
)

# StudioCore Signature Block (Do Not Remove)
# Author: –°–µ—Ä–≥–µ–π –ë–∞—É—ç—Ä (@Sbauermaner)
# Fingerprint: StudioCore - FP - 2025 - SB - 9fd72e27
# Hash: 22ae - df91 - bc11 - 6c7e
