# -*- coding: utf-8 -*-
"""
StudioCore v5.2 ‚Äî VocalProfileRegistry (Extended Adaptive Integration)
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å AdaptiveVocalAllocator –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–µ–≤—Ü–æ–≤ –∏ —Ñ–æ—Ä–º—ã.
"""

from typing import List, Dict, Any, Tuple
from .emotion import AutoEmotionalAnalyzer

VALID_VOICES = [
    "male","female","duet","trio","quartet","quintet","choir",
    "tenor","soprano","alto","baritone","bass",
    "raspy","breathy","powerful","soft","emotional","angelic",
    "deep","whispered","warm","clear"
]

VALID_INSTRUMENTS = [
    "guitar","piano","synth","bass","drums","strings","violin","cello",
    "trumpet","saxophone","organ","harp","choir","vocals","pad","flute",
    "horns","percussion","tagelharpa"
]

DEFAULT_VOCAL_MAP = {
    "rock":       {"female":["female","emotional","alto"], "male":["male","raspy","tenor"], "inst":["guitar","drums","bass","piano"]},
    "pop":        {"female":["female","clear","soprano"],  "male":["male","soft","tenor"],   "inst":["piano","synth","bass","drums"]},
    "folk":       {"female":["female","warm","alto"],      "male":["male","emotional","baritone"], "inst":["guitar","strings","flute"]},
    "cinematic":  {"female":["female","angelic"],          "male":["male","deep"],          "inst":["strings","piano","choir","drums"]},
    "electronic": {"female":["female","breathy"],          "male":["male","soft"],          "inst":["synth","pad","bass","drums"]},
    "ambient":    {"female":["female","whispered"],        "male":["male","soft"],          "inst":["pad","piano","strings"]},
    "orchestral": {"female":["female","angelic"],          "male":["male","deep"],          "inst":["strings","choir","horns","percussion"]},
}


class VocalProfileRegistry:
    """Adaptive engine: suggests suitable vocal & instrumental settings."""

    def __init__(self):
        self.map = DEFAULT_VOCAL_MAP

    # --------------------------------------------------------
    def _detect_ensemble_hints(self, text: str, sections: List[Dict[str, Any]]) -> Dict[str, bool]:
        s = (text + " " + " ".join(s.get("tag", "") for s in sections)).lower()
        return {
            "wants_choir": any(k in s for k in ["choir","—Ö–æ—Ä","chorus","anthem","mass"]),
            "wants_duet":  any(k in s for k in ["duet","–¥—É—ç—Ç","duo","–≤–º–µ—Å—Ç–µ","pair"]),
            "wants_trio":  any(k in s for k in ["trio","—Ç—Ä–∏–æ"]),
            "wants_quartet": any(k in s for k in ["quartet","–∫–≤–∞—Ä—Ç–µ—Ç"]),
            "wants_quintet": any(k in s for k in ["quintet","–∫–≤–∏–Ω—Ç–µ—Ç"]),
            "dialogue": any(k in s for k in ["(male)","(female)","he said","she said","–æ–Ω","–æ–Ω–∞"]),
            "call_response": any(k in s for k in ["–≤ –æ—Ç–≤–µ—Ç","–æ—Ç–∫–ª–∏–∫","–æ—Ç–≤–µ—Ç–∏–ª","respond","reply"]),
        }

    # --------------------------------------------------------
    def auto_vocal_form(self, emo: Dict[str,float], tlp: Dict[str,float], text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–æ–∫–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        - CF (Conscious Frequency)
        - Truth/Love/Pain
        - —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è —Ç–µ–∫—Å—Ç–∞
        - –¥–ª–∏–Ω—ã (—Å–ª–æ–≤)
        """
        wc = len(text.split())
        cf = tlp.get("conscious_frequency", 0.0)
        love, pain, truth = tlp.get("love", 0.0), tlp.get("pain", 0.0), tlp.get("truth", 0.0)

        # ‚ú¥Ô∏è –≠–Ω–µ—Ä–≥–∏—è –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–æ—Ü–∏–π (–ø–æ —Ñ–∏–ª–æ—Å–æ—Ñ–∏–∏ StudioCore)
        base_energy = (truth * 0.4 + pain * 0.6 + cf * 0.8) - (love * 0.3)
        emo_energy = max(emo.values()) if emo else 0.25

        ensemble_intensity = round(min(1.0, max(0.0, (base_energy + emo_energy) / 1.5)), 3)

        # –±–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –∞–Ω—Å–∞–º–±–ª—è
        if wc < 40 and ensemble_intensity < 0.3:
            form = "solo"
        elif 40 <= wc < 80 or 0.3 <= ensemble_intensity < 0.45:
            form = "duet"
        elif 80 <= wc < 150 or 0.45 <= ensemble_intensity < 0.6:
            form = "trio"
        elif 150 <= wc < 250 or 0.6 <= ensemble_intensity < 0.75:
            form = "quartet"
        elif wc >= 250 or ensemble_intensity >= 0.75:
            form = "choir"
        else:
            form = "solo"

        # üí° –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è —Ñ–æ—Ä–º—ã –ø–æ CF –∏ TLP
        if cf > 0.9 and form != "choir":
            form = "choir"
        elif cf > 0.85 and pain > 0.05 and form in ["solo", "duet"]:
            form = "trio"
        elif cf < 0.6 and love > 0.25 and pain < 0.04:
            form = "solo"

        return form

    # --------------------------------------------------------
    def get(self, genre: str, preferred_gender: str, text: str, sections: List[Dict[str,Any]],
            override: Dict[str, Any] | None = None) -> Tuple[List[str], List[str], str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (voices, instruments, vocal_form)
        override ‚Äî —Å–ª–æ–≤–∞—Ä—å –æ—Ç AdaptiveVocalAllocator —Å –∫–ª—é—á–∞–º–∏:
        { "vocal_form": str, "gender": str, "vocal_count": int }
        """
        g = genre if genre in self.map else "rock"
        hints = self._detect_ensemble_hints(text, sections)
        emo = AutoEmotionalAnalyzer().analyze(text)

        # ‚öôÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π TLP-stub (–¥–æ –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–∑ —è–¥—Ä–∞)
        tlp_stub = {
            "conscious_frequency": emo.get("intensity", 0.5),
            "love": emo.get("joy", 0.3),
            "pain": emo.get("sadness", 0.2),
            "truth": emo.get("peace", 0.4)
        }

        form = self.auto_vocal_form(emo, tlp_stub, text)

        # üî∏ override (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
        if override:
            form = override.get("vocal_form", form)
            preferred_gender = override.get("gender", preferred_gender)

        # üî∏ –í—ã–±–æ—Ä –ø–æ –ø–æ–ª—É
        if preferred_gender == "female":
            vox = self.map[g]["female"]
        elif preferred_gender == "male":
            vox = self.map[g]["male"]
        elif preferred_gender == "auto":
            vox = self.map[g]["female"] if emo.get("joy",0) > emo.get("anger",0) else self.map[g]["male"]
        else:
            vox = self.map[g]["female"]

        # üî∏ –•–∏–Ω—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
        if hints["wants_choir"]:
            form = "choir"
        elif hints["wants_quintet"]:
            form = "quintet"
        elif hints["wants_quartet"]:
            form = "quartet"
        elif hints["wants_trio"]:
            form = "trio"
        elif hints["wants_duet"] or hints["dialogue"] or hints["call_response"]:
            form = "duet"

        vox = [form] + vox
        inst = self.map[g]["inst"]

        # üéô –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Ñ–æ—Ä–º—É
        if "choir" in vox:
            if "male" in vox and "female" in vox:
                vocal_form = "choir_mixed"
            elif "female" in vox:
                vocal_form = "choir_female"
            elif "male" in vox:
                vocal_form = "choir_male"
            else:
                vocal_form = "choir_auto"
        elif "quartet" in vox:
            vocal_form = "quartet_mixed"
        elif "trio" in vox:
            vocal_form = "trio_mixed"
        elif "duet" in vox:
            if "male" in vox and "female" in vox:
                vocal_form = "duet_mf"
            elif "male" in vox:
                vocal_form = "duet_mm"
            elif "female" in vox:
                vocal_form = "duet_ff"
            else:
                vocal_form = "duet_auto"
        elif "female" in vox and "male" not in vox:
            vocal_form = "solo_female"
        elif "male" in vox and "female" not in vox:
            vocal_form = "solo_male"
        else:
            vocal_form = "solo_auto"

        vox = [v for v in vox if v in VALID_VOICES][:6]
        inst = [i for i in inst if i in VALID_INSTRUMENTS][:6]

        return vox, inst, vocal_form


# --------------------------------------------------------
def map_emotion_to_english(mood: str, tone: str = "mid") -> str:
    """Converts emotion/tone info into English phrasing hints for Suno inline annotation."""
    mood = (mood or "neutral").lower()
    tone = (tone or "mid").lower()
    emotion_map = {
        "calm": "soft whisper, warm tone",
        "peaceful": "gentle flow, smooth phrasing",
        "hopeful": "light rise, airy resonance",
        "joyful": "open tone, bright timbre, smiling delivery",
        "sad": "slow breath, trembling vibrato",
        "melancholy": "emotional depth, low register warmth",
        "dramatic": "belted rasp, strong dynamic contrast",
        "angry": "harsh tone, powerful projection",
        "prayerful": "vibrato with tender breath",
        "romantic": "soft dynamics, emotional phrasing",
        "intense": "raspy tone, controlled tension",
        "neutral": "balanced tone, natural phrasing"
    }
    tone_map = {
        "low": "deep resonance",
        "mid": "mid-range timbre",
        "high": "bright tone",
        "whisper": "breathy delivery",
        "belt": "strong projection",
    }
    phrase = emotion_map.get(mood, "neutral phrasing, natural tone")
    tone_descr = tone_map.get(tone, "")
    return f"{phrase}, {tone_descr}".strip(", ")
