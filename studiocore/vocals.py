# -*- coding: utf-8 -*-
"""
StudioCore v5.1 â€” VocalProfileRegistry
ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ð±Ð¾Ñ€ Ð²Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÐµÐ¹ Ð¿Ð¾ Ð¶Ð°Ð½Ñ€Ñƒ, ÑÐ¼Ð¾Ñ†Ð¸ÑÐ¼ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ.
Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ map_emotion_to_english() Ð´Ð»Ñ inline-Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ð¸ (Suno adaptive mode)
"""

from typing import List, Dict, Any, Tuple
from .emotion import AutoEmotionalAnalyzer

VALID_VOICES = [
    "male","female","duet","trio","quartet","quintet","choir",
    "tenor","soprano","alto","baritone","bass",
    "raspy","breathy","powerful","soft","emotional","angelic","deep","whispered","warm","clear"
]

VALID_INSTRUMENTS = [
    "guitar","piano","synth","bass","drums","strings","violin","cello","trumpet",
    "saxophone","organ","harp","choir","vocals","pad","flute","horns","percussion","tagelharpa"
]

DEFAULT_VOCAL_MAP = {
    "rock":       {"female":["female","emotional","alto"], "male":["male","raspy","tenor"], "inst":["guitar","drums","bass","piano"]},
    "pop":        {"female":["female","clear","soprano"],  "male":["male","soft","tenor"],   "inst":["piano","synth","bass","drums"]},
    "folk":       {"female":["female","warm","alto"],      "male":["male","emotional","baritone"], "inst":["guitar","strings","flute"]},
    "cinematic":  {"female":["female","angelic"],          "male":["male","deep"],          "inst":["strings","piano","choir","drums"]},
    "electronic": {"female":["female","breathy"],          "male":["male","soft"],          "inst":["synth","pad","bass","drums"]},
    "ambient":    {"female":["female","whispered"],        "male":["male","soft"],          "inst":["pad","piano","strings"]},
    "orchestral": {"female":["female","angelic"],          "male":["male","deep"],          "inst":["strings","choir","horns","percussion"]},
}


class VocalProfileRegistry:
    """Adaptive engine: suggests suitable vocal & instrumental settings."""

    def __init__(self):
        self.map = DEFAULT_VOCAL_MAP

    # --------------------------------------------------------
    # ðŸ” ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° Ð¸ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¾Ð² Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ ÑÐ¾ÑÑ‚Ð°Ð²Ð°
    # --------------------------------------------------------
    def _detect_ensemble_hints(self, text: str, sections: List[Dict[str, Any]]) -> Dict[str, bool]:
        s = (text + " " + " ".join(s.get("tag", "") for s in sections)).lower()
        return {
            "wants_choir": any(k in s for k in ["choir","Ñ…Ð¾Ñ€","chorus","anthem","mass"]),
            "wants_duet":  any(k in s for k in ["duet","Ð´ÑƒÑÑ‚","duo","Ð²Ð¼ÐµÑÑ‚Ðµ","pair"]),
            "wants_trio":  any(k in s for k in ["trio","Ñ‚Ñ€Ð¸Ð¾"]),
            "wants_quartet": any(k in s for k in ["quartet","ÐºÐ²Ð°Ñ€Ñ‚ÐµÑ‚"]),
            "wants_quintet": any(k in s for k in ["quintet","ÐºÐ²Ð¸Ð½Ñ‚ÐµÑ‚"]),
            "dialogue": any(k in s for k in ["(male)","(female)","he said","she said","Ð¾Ð½","Ð¾Ð½Ð°"]),
            "call_response": any(k in s for k in ["Ð² Ð¾Ñ‚Ð²ÐµÑ‚","Ð¾Ñ‚ÐºÐ»Ð¸Ðº","Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»","respond","reply"]),
        }

    # --------------------------------------------------------
    # ðŸŽ™ ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð²Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ñ„Ð¾Ñ€Ð¼Ñ‹
    # --------------------------------------------------------
    def auto_vocal_form(self, emo: Dict[str,float], tlp: Dict[str,float], text: str) -> str:
        wc = len(text.split())
        cf = tlp.get("conscious_frequency", 0)
        love, pain, truth = tlp.get("love",0), tlp.get("pain",0), tlp.get("truth",0)
        energy = (love + pain + truth) / 3
        emo_energy = max(emo.values()) if emo else 0.3

        ensemble_intensity = round(min(1.0, (energy + emo_energy + cf) / 3), 3)

        if wc < 40 and ensemble_intensity < 0.3:
            return "solo"
        elif 40 <= wc < 80 or ensemble_intensity > 0.4:
            return "duet"
        elif 80 <= wc < 150 or ensemble_intensity > 0.5:
            return "trio"
        elif 150 <= wc < 250 or ensemble_intensity > 0.6:
            return "quartet"
        elif wc >= 250 or ensemble_intensity > 0.75:
            return "choir"
        return "solo"

    # --------------------------------------------------------
    # ðŸ§© ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð²Ð¾ÐºÐ°Ð»Ð° Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
    # --------------------------------------------------------
    def get(self, genre: str, preferred_gender: str, text: str, sections: List[Dict[str,Any]]) -> Tuple[List[str], List[str], str]:
        g = genre if genre in self.map else "rock"
        hints = self._detect_ensemble_hints(text, sections)

        emo = AutoEmotionalAnalyzer().analyze(text)
        tlp_stub = {"conscious_frequency": 0.5, "love": emo.get("joy", 0.3), "pain": emo.get("sadness", 0.2), "truth": emo.get("peace", 0.4)}
        form = self.auto_vocal_form(emo, tlp_stub, text)

        # ÐŸÐ¾Ð»/Ð²Ð¾ÐºÐ°Ð»
        if preferred_gender == "female":
            vox = self.map[g]["female"]
        elif preferred_gender == "male":
            vox = self.map[g]["male"]
        elif preferred_gender == "auto":
            vox = self.map[g]["female"] if emo.get("joy",0) > emo.get("anger",0) else self.map[g]["male"]
        else:
            vox = self.map[g]["female"]

        # ÐŸÐ¾Ð´ÑÐºÐ°Ð·ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð°
        if hints["wants_choir"]:
            form = "choir"
        elif hints["wants_quintet"]:
            form = "quintet"
        elif hints["wants_quartet"]:
            form = "quartet"
        elif hints["wants_trio"]:
            form = "trio"
        elif hints["wants_duet"] or hints["dialogue"] or hints["call_response"]:
            form = "duet"

        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¾ÑÑ‚Ð°Ð²Ð°
        vox = [form] + vox
        inst = self.map[g]["inst"]

        # --- ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ ---
        if "choir" in vox:
            if "male" in vox and "female" in vox:
                vocal_form = "choir_mixed"
            elif "female" in vox:
                vocal_form = "choir_female"
            elif "male" in vox:
                vocal_form = "choir_male"
            else:
                vocal_form = "choir_auto"
        elif "quintet" in vox:
            vocal_form = "quintet_mixed"
        elif "quartet" in vox:
            vocal_form = "quartet_mixed"
        elif "trio" in vox:
            vocal_form = "trio_mixed"
        elif "duet" in vox:
            if "male" in vox and "female" in vox:
                vocal_form = "duet_mf"
            elif "male" in vox:
                vocal_form = "duet_mm"
            elif "female" in vox:
                vocal_form = "duet_ff"
            else:
                vocal_form = "duet_auto"
        elif "female" in vox and "male" not in vox:
            vocal_form = "solo_female"
        elif "male" in vox and "female" not in vox:
            vocal_form = "solo_male"
        else:
            vocal_form = "solo_auto"

        vox = [v for v in vox if v in VALID_VOICES][:6]
        inst = [i for i in inst if i in VALID_INSTRUMENTS][:6]

        return vox, inst, vocal_form


# --------------------------------------------------------
# ðŸ—£ï¸ ÐÐ´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð°Ð½Ð½Ð¾Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾ ÑÐ¼Ð¾Ñ†Ð¸ÑÐ¼ (Ð´Ð»Ñ inline prompt)
# --------------------------------------------------------
def map_emotion_to_english(mood: str, tone: str = "mid") -> str:
    """
    Converts emotion/tone info into English phrasing hints for Suno inline annotation.
    Lyrics stay in original language; only annotations are English.
    """
    mood = (mood or "neutral").lower()
    tone = (tone or "mid").lower()

    emotion_map = {
        "calm": "soft whisper, warm tone",
        "peaceful": "gentle flow, smooth phrasing",
        "hopeful": "light rise, airy resonance",
        "joyful": "open tone, bright timbre, smiling delivery",
        "sad": "slow breath, trembling vibrato",
        "melancholy": "emotional depth, low register warmth",
        "dramatic": "belted rasp, strong dynamic contrast",
        "angry": "harsh tone, powerful projection",
        "prayerful": "vibrato with tender breath",
        "romantic": "soft dynamics, emotional phrasing",
        "intense": "raspy tone, controlled tension",
        "neutral": "balanced tone, natural phrasing"
    }

    tone_map = {
        "low": "deep resonance",
        "mid": "mid-range timbre",
        "high": "bright tone",
        "whisper": "breathy delivery",
        "belt": "strong projection",
    }

    phrase = emotion_map.get(mood, "neutral phrasing, natural tone")
    tone_descr = tone_map.get(tone, "")
    return f"{phrase}, {tone_descr}".strip(", ")
